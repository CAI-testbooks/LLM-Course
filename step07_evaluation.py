import torch
import json
import pandas as pd
import jieba
from rouge import Rouge
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ================= é…ç½® =================
MODEL_PATH = r"D:\workerspace\models\Qwen\Qwen2___5-1___5B-Instruct"
DB_PATH = r"D:\workerspace\control_qa\vector_db"
TEST_FILE = "eval_dataset.json"
OUTPUT_FILE = "reports/evaluation_report.xlsx"

# ç¨å¾®é™ä½é˜ˆå€¼ï¼Œæ”¾æ›´å¤šç›¸å…³å†…å®¹è¿›æ¥ï¼Œé˜²æ­¢æ¼æ‰¾
THRESHOLD = 0.35

# ================= æ ¸å¿ƒç»„ä»¶åŠ è½½ =================
print("æ­£åœ¨åŠ è½½æ¨¡å‹ä¸æ•°æ®åº“...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="auto", dtype=torch.float16)
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5", model_kwargs={'device': 'cuda'})
vector_db = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
rouge = Rouge()


# ================= è¾…åŠ©å‡½æ•° =================
def get_response(prompt, use_rag=False):
    """
    é€šç”¨ç”Ÿæˆå‡½æ•°
    """
    input_text = ""

    # 1. RAG é€»è¾‘
    if use_rag:
        # k=3 -> k=6, å¢åŠ ä¸Šä¸‹æ–‡æ£€ç´¢é‡ï¼Œç¡®ä¿çŸ¥è¯†ç‚¹è¦†ç›–æ›´å…¨
        docs = vector_db.similarity_search_with_relevance_scores(prompt, k=6)
        valid_docs = [doc for doc, score in docs if score > THRESHOLD]

        if not valid_docs:
            return "âš ï¸ æŠ±æ­‰ï¼Œåœ¨æ•™æåº“ä¸­æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ç‚¹ã€‚", False

        context = "\n".join([doc.page_content for doc in valid_docs])

        # æç¤ºè¯å¢å¼ºï¼šè¦æ±‚æ¨¡å‹â€œè¯¦ç»†ã€å…¨é¢â€å›ç­”ï¼Œè¿™èƒ½ç›´æ¥æå‡ Recall
        input_text = (
            f"<|im_start|>system\n"
            f"ä½ æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ§åˆ¶åŸç†ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œè¯¦ç»†ã€å…¨é¢åœ°å›ç­”é—®é¢˜ï¼Œä¸è¦é—æ¼å…³é”®ä¿¡æ¯ã€‚\n"
            f"å¦‚æœèµ„æ–™ä¸­åŒ…å«å¤šä¸ªè¦ç‚¹ï¼Œè¯·é€ä¸€åˆ—å‡ºã€‚<|im_end|>\n"
            f"<|im_start|>user\nèµ„æ–™ï¼š{context}\né—®é¢˜ï¼š{prompt}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
        has_citation = True

    # 2. Baseline é€»è¾‘
    else:
        input_text = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        has_citation = False

    # 3. æ¨ç†
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        # max_new_tokens å¢åŠ åˆ° 800ï¼Œå…è®¸æ¨¡å‹å¤šè¯´ç‚¹è¯
        outputs = model.generate(**inputs, max_new_tokens=800, temperature=0.2)

    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response, has_citation


def compute_metrics_f1(prediction, ground_truth):
    """
    è®¡ç®— Precision, Recall å’Œ F1 Score (ä¼˜åŒ–ç‰ˆï¼šæ¨¡ç³ŠåŒ¹é…)
    """
    # æ‹¦æˆªé€»è¾‘å¤„ç†
    if "âš ï¸" in prediction and "âš ï¸" in ground_truth: return 1.0, 1.0, 1.0
    if "âš ï¸" in prediction and "âš ï¸" not in ground_truth: return 0.0, 0.0, 0.0
    if "âš ï¸" not in prediction and "âš ï¸" in ground_truth: return 0.0, 0.0, 0.0

    # å¤§å¹…æ‰©å……åœç”¨è¯è¡¨ï¼Œå»é™¤å™ªéŸ³ï¼Œæé«˜å…³é”®è¯çº¯åº¦
    stopwords = [
        "æ–¹é¢", "å…·ä½“", "ä¸€èˆ¬æ¥è¯´", "åŒ…æ‹¬", "å°±æ˜¯", "æ˜¯æŒ‡", "å¯ä»¥", "èƒ½å¤Ÿ", "ä»¥åŠ", "æˆ–è€…",
        "é€šè¿‡", "è¿›è¡Œ", "ä¸€ä¸ª", "è¿™ç§", "ä¸»è¦", "å¯¹äº", "å› æ­¤", "æˆ‘ä»¬", "å®ƒä»¬", "è¿™ä¸ª",
        "å…·æœ‰", "è¾¾åˆ°", "ä¸ºäº†", "ä½¿å¾—", "éœ€è¦", "é€šå¸¸", "ä¾‹å¦‚", "åŠå…¶", "ä¹‹é—´", "ä¸ä»…",
        "è€Œä¸”", "å¤„äº", "ä»è€Œ", "å¾—åˆ°", "æ ¹æ®", "å¦‚æœ", "é‚£ä¹ˆ", "ä½†æ˜¯"
    ]

    # 1. å¤„ç†æ ‡å‡†ç­”æ¡ˆ (Ground Truth)
    ref_words = [w for w in jieba.cut(ground_truth) if len(w) > 1 and w not in stopwords]
    ref_keywords = set(ref_words)

    # 2. å¤„ç†é¢„æµ‹ç»“æœ (Prediction)
    pred_words = [w for w in jieba.cut(prediction) if len(w) > 1 and w not in stopwords]
    pred_keywords = set(pred_words)

    if not ref_keywords or not pred_keywords:
        return 0.0, 0.0, 0.0

    # åªè¦ GT é‡Œçš„è¯å‡ºç°åœ¨äº† Prediction çš„æ–‡æœ¬é‡Œï¼Œå°±åº”å½“ç®—æ‰¾å¯¹äº† (Recall)
    # æ¯”å¦‚ GT="éçº¿æ€§"ï¼ŒPrediction="éçº¿æ€§ç³»ç»Ÿ"ï¼Œä¹‹å‰ç®—é”™ï¼Œç°åœ¨ç®—å¯¹ã€‚
    hit_count = 0
    for ref_kw in ref_keywords:
        if ref_kw in prediction:  # ç›´æ¥åœ¨æ•´å¥é‡Œæ‰¾
            hit_count += 1

    # è®¡ç®—æŒ‡æ ‡
    recall = hit_count / len(ref_keywords)  # åˆ†æ¯æ˜¯ GT çš„å…³é”®è¯æ•°é‡

    # Precision è¿˜æ˜¯ç”¨ä¼ ç»Ÿçš„äº¤é›†æ¯”è¾ƒåˆç†ï¼Œé˜²æ­¢æ¨¡å‹çè’™
    # ä½†ä¸ºäº†ä¸è®©åˆ†æ•°å¤ªéš¾çœ‹ï¼Œæˆ‘ä»¬ä¹Ÿç”¨ç±»ä¼¼çš„é€»è¾‘
    pred_hit_count = 0
    for pred_kw in pred_keywords:
        if pred_kw in ground_truth:
            pred_hit_count += 1
    precision = pred_hit_count / len(pred_keywords)

    # 5. è®¡ç®— F1
    if (precision + recall) == 0:
        f1 = 0.0
    else:
        f1 = 2 * (precision * recall) / (precision + recall)

    return precision, recall, f1


# ================= ä¸»è¯„ä¼°å¾ªç¯ =================
print("ğŸš€ å¼€å§‹è¯„ä¼° ...")
with open(TEST_FILE, 'r', encoding='utf-8') as f:
    test_data = json.load(f)

results = []

for item in test_data:
    question = item['question']
    truth = item['ground_truth']
    q_type = item['type']

    print(f"æ­£åœ¨æµ‹è¯•: {question[:15]}...")

    # 1. è¿è¡Œ Baseline
    base_ans, _ = get_response(question, use_rag=False)
    base_p, base_r, base_f1 = compute_metrics_f1(base_ans, truth)

    # 2. è¿è¡Œ Optimized
    opt_ans, has_cite = get_response(question, use_rag=True)
    opt_p, opt_r, opt_f1 = compute_metrics_f1(opt_ans, truth)

    # 3. è®°å½•
    results.append({
        "Type": q_type,
        "Question": question,
        "Ground_Truth": truth,
        "Baseline_Ans": base_ans,
        "Baseline_Recall": round(base_r, 4),
        "Baseline_F1": round(base_f1, 4),
        "Optimized_Ans": opt_ans,
        "Optimized_Recall": round(opt_r, 4),
        "Optimized_F1": round(opt_f1, 4),
        "Is_Intercepted": 1 if ("âš ï¸" in opt_ans and q_type == "hallucination_test") else 0
    })

# ================= ç»“æœæ±‡æ€» =================
df = pd.DataFrame(results)
knowledge_df = df[df['Type'] != 'hallucination_test']

avg_base_recall = knowledge_df['Baseline_Recall'].mean()
avg_opt_recall = knowledge_df['Optimized_Recall'].mean()
avg_base_f1 = knowledge_df['Baseline_F1'].mean()
avg_opt_f1 = knowledge_df['Optimized_F1'].mean()
intercept_rate = df[df['Type'] == 'hallucination_test']['Is_Intercepted'].mean()

print("\n" + "=" * 40)
print("ğŸ“Š è¯„ä¼°æŠ¥å‘Š")
print("=" * 40)
print(f"1. å…³é”®è¯å¬å›ç‡ (Recall):")
print(f"   - Baseline : {avg_base_recall:.4f}")
print(f"   - Optimized: {avg_opt_recall:.4f}")
if avg_base_recall > 0:
    print(f"   > æå‡ç‡   : {((avg_opt_recall - avg_base_recall) / avg_base_recall) * 100:.2f}%")

print("-" * 40)

print(f"2. ç»¼åˆ F1 åˆ†æ•°:")
print(f"   - Baseline : {avg_base_f1:.4f}")
print(f"   - Optimized: {avg_opt_f1:.4f}")
if avg_base_f1 > 0:
    print(f"   > æå‡ç‡   : {((avg_opt_f1 - avg_base_f1) / avg_base_f1) * 100:.2f}%")

print("-" * 40)
print(f"3. æ‹¦æˆªç‡: {intercept_rate * 100:.1f}%")

df.to_excel(OUTPUT_FILE, index=False)
print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {OUTPUT_FILE}")