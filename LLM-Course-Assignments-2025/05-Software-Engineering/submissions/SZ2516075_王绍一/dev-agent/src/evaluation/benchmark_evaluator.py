# src/evaluation/benchmark_evaluator.py
import time
import json
import statistics
from typing import Dict, List, Any
from datetime import datetime

class BenchmarkEvaluator:
    """åŸºå‡†æµ‹è¯•è¯„ä¼°å™¨"""
    
    def __init__(self, model_factory, agent_class):
        self.model_factory = model_factory
        self.agent_class = agent_class
        self.results = {}
    
    def evaluate_on_dataset(self, dataset_name: str, model_id: str, 
                          num_samples: int = 10) -> Dict[str, Any]:
        """åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡åž‹"""
        
        print(f"ðŸ“Š å¼€å§‹è¯„ä¼°: {model_id} åœ¨ {dataset_name} ä¸Š")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®é›†
        from src.datasets.dataset_manager import DatasetManager
        dataset_manager = DatasetManager()
        
        try:
            dataset = dataset_manager.load_dataset(dataset_name)
        except:
            print(f"âŒ æ— æ³•åŠ è½½æ•°æ®é›†: {dataset_name}")
            return {"error": f"æ— æ³•åŠ è½½æ•°æ®é›†: {dataset_name}"}
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        samples = dataset[:num_samples] if len(dataset) > num_samples else dataset
        
        # åˆ›å»ºæ¨¡åž‹å’ŒAgent
        model_info = self.model_factory.create_model(model_id)
        agent = self.agent_class(model_info)
        
        evaluation_results = []
        
        for i, sample in enumerate(samples):
            print(f"\nðŸ” è¯„ä¼°æ ·æœ¬ {i+1}/{len(samples)}")
            
            # å‡†å¤‡é—®é¢˜
            if dataset_name == "humaneval":
                problem = sample["prompt"]
                test = sample.get("test", "")
            elif dataset_name == "mbpp":
                problem = sample["text"]
                test = "\n".join(sample.get("test_list", []))
            else:
                problem = sample.get("problem_statement", "")
                test = sample.get("test_patch", "")
            
            # è¿è¡ŒAgent
            start_time = time.time()
            result = agent.process_requirement(problem)
            end_time = time.time()
            
            # è¯„ä¼°ç»“æžœ
            evaluation = self._evaluate_result(result, test, problem)
            evaluation["time_used"] = end_time - start_time
            evaluation["sample_index"] = i
            
            evaluation_results.append(evaluation)
            
            print(f"   ç»“æžœ: {'âœ… é€šè¿‡' if evaluation['passed'] else 'âŒ å¤±è´¥'}")
            print(f"   ç”¨æ—¶: {evaluation['time_used']:.2f}s")
            print(f"   åˆ†æ•°: {evaluation['score']:.2f}")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        stats = self._calculate_statistics(evaluation_results)
        
        # ä¿å­˜ç»“æžœ
        result_id = f"{model_id}_{dataset_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.results[result_id] = {
            "model": model_id,
            "dataset": dataset_name,
            "timestamp": datetime.now().isoformat(),
            "stats": stats,
            "details": evaluation_results
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        self._save_results(result_id)
        
        print(f"\nðŸŽ‰ è¯„ä¼°å®Œæˆ!")
        print(f"   é€šè¿‡çŽ‡: {stats['pass_rate']:.2%}")
        print(f"   å¹³å‡ç”¨æ—¶: {stats['avg_time']:.2f}s")
        print(f"   å¹³å‡åˆ†æ•°: {stats['avg_score']:.2f}")
        
        return self.results[result_id]
    
    def _evaluate_result(self, result: Dict, test: str, problem: str) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªç»“æžœ"""
        
        score = 0.0
        passed = False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆä»£ç 
        final_code = result.get("final_code") or result.get("code")
        if not final_code:
            return {
                "passed": False,
                "score": 0.0,
                "reason": "æ— ä»£ç ç”Ÿæˆ"
            }
        
        # æµ‹è¯•æ˜¯å¦é€šè¿‡
        if result.get("success", False):
            score += 50
            passed = True
        
        # ä»£ç è´¨é‡è¯„åˆ†
        code_quality = self._evaluate_code_quality(final_code)
        score += code_quality * 20  # æœ€å¤š20åˆ†
        
        # æµ‹è¯•è¦†ç›–çŽ‡
        if result.get("test_result", {}).get("coverage", 0):
            score += result["test_result"]["coverage"] * 20  # æœ€å¤š20åˆ†
        
        # è¿­ä»£æ¬¡æ•°å°‘åŠ åˆ†
        iterations = len(result.get("bugs", [])) + 1
        score += max(0, 10 - iterations)  # æœ€å¤š10åˆ†
        
        return {
            "passed": passed,
            "score": min(100, score),
            "code_quality": code_quality,
            "iterations": iterations,
            "bugs_found": len(result.get("bugs", [])),
            "tests_passed": result.get("test_result", {}).get("tests_passed", 0)
        }
    
    def _evaluate_code_quality(self, code: str) -> float:
        """è¯„ä¼°ä»£ç è´¨é‡"""
        try:
            import ast
            
            tree = ast.parse(code)
            
            score = 0.0
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ³¨é‡Š
            has_comments = False
            for node in ast.walk(tree):
                if isinstance(node, ast.Expr) and isinstance(node.value, ast.Str):
                    has_comments = True
                    break
            
            if has_comments:
                score += 0.3
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°
            has_functions = any(isinstance(node, ast.FunctionDef) for node in ast.walk(tree))
            if has_functions:
                score += 0.3
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯å¤„ç†
            has_try_except = any(isinstance(node, ast.Try) for node in ast.walk(tree))
            if has_try_except:
                score += 0.4
            
            return score
            
        except:
            return 0.0
    
    def _calculate_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            return {}
        
        passed = [r["passed"] for r in results]
        scores = [r["score"] for r in results]
        times = [r["time_used"] for r in results]
        
        pass_rate = sum(passed) / len(passed) if passed else 0
        
        return {
            "pass_rate": pass_rate,
            "avg_score": statistics.mean(scores) if scores else 0,
            "avg_time": statistics.mean(times) if times else 0,
            "median_score": statistics.median(scores) if scores else 0,
            "total_samples": len(results),
            "passed_samples": sum(passed),
            "score_distribution": self._create_distribution(scores)
        }
    
    def _create_distribution(self, scores: List[float]) -> Dict[str, int]:
        """åˆ›å»ºåˆ†æ•°åˆ†å¸ƒ"""
        distribution = {
            "0-20": 0, "21-40": 0, "41-60": 0,
            "61-80": 0, "81-100": 0
        }
        
        for score in scores:
            if score <= 20:
                distribution["0-20"] += 1
            elif score <= 40:
                distribution["21-40"] += 1
            elif score <= 60:
                distribution["41-60"] += 1
            elif score <= 80:
                distribution["61-80"] += 1
            else:
                distribution["81-100"] += 1
        
        return distribution
    
    def _save_results(self, result_id: str):
        """ä¿å­˜è¯„ä¼°ç»“æžœ"""
        if result_id not in self.results:
            return
        
        filename = f"evaluation_results/{result_id}.json"
        os.makedirs("evaluation_results", exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results[result_id], f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ è¯„ä¼°ç»“æžœå·²ä¿å­˜åˆ°: {filename}")
    
    def compare_models(self, model_ids: List[str], dataset_name: str, 
                      num_samples: int = 5) -> Dict[str, Any]:
        """æ¯”è¾ƒå¤šä¸ªæ¨¡åž‹"""
        
        comparison_results = {}
        
        for model_id in model_ids:
            print(f"\nðŸ“Š è¯„ä¼°æ¨¡åž‹: {model_id}")
            
            result = self.evaluate_on_dataset(
                dataset_name=dataset_name,
                model_id=model_id,
                num_samples=num_samples
            )
            
            comparison_results[model_id] = result.get("stats", {})
        
        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        comparison_report = self._generate_comparison_report(comparison_results)
        
        return {
            "comparison": comparison_results,
            "report": comparison_report,
            "best_model": self._select_best_model(comparison_results)
        }
    
    def _generate_comparison_report(self, results: Dict[str, Dict]) -> str:
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("ðŸ“ˆ æ¨¡åž‹æ¯”è¾ƒæŠ¥å‘Š")
        report_lines.append("=" * 60)
        
        for model_id, stats in results.items():
            report_lines.append(f"\nðŸ”¹ {model_id}:")
            report_lines.append(f"   é€šè¿‡çŽ‡: {stats.get('pass_rate', 0):.2%}")
            report_lines.append(f"   å¹³å‡åˆ†æ•°: {stats.get('avg_score', 0):.2f}")
            report_lines.append(f"   å¹³å‡ç”¨æ—¶: {stats.get('avg_time', 0):.2f}s")
        
        return "\n".join(report_lines)
    
    def _select_best_model(self, results: Dict[str, Dict]) -> str:
        """é€‰æ‹©æœ€ä½³æ¨¡åž‹"""
        if not results:
            return "æ— æ•°æ®"
        
        # æ ¹æ®é€šè¿‡çŽ‡å’Œåˆ†æ•°é€‰æ‹©
        best_model = None
        best_score = -1
        
        for model_id, stats in results.items():
            pass_rate = stats.get("pass_rate", 0)
            avg_score = stats.get("avg_score", 0)
            avg_time = stats.get("avg_time", 0)
            
            # ç»¼åˆè¯„åˆ†å…¬å¼
            score = pass_rate * 100 + avg_score - avg_time / 10
            
            if score > best_score:
                best_score = score
                best_model = model_id
        
        return best_model