# src/models/model_factory.py
import os
from typing import Dict, Any, Optional

class ModelFactory:
    """æ¨¡å‹å·¥å‚ï¼Œæ”¯æŒå¤šç§ä»£ç LLM"""
    
    def __init__(self, cache_dir: str = "D:/huggingface_cache"):
        self.cache_dir = cache_dir
        os.environ['HF_HOME'] = cache_dir
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        
        # æ”¯æŒçš„æ¨¡å‹é…ç½®
        self.model_configs = {
            # è¶…å°æ¨¡å‹ - 200MB
            "tiny_starcoder": {
                "name": "bigcode/tiny_starcoder_py",
                "description": "ä¸“ä¸ºPythonçš„å°æ¨¡å‹ï¼Œ200MB",
                "params": "164M",
                "size_gb": 0.2
            },
            # DeepSeekç³»åˆ—
            "deepseek-coder-1.3b": {
                "name": "deepseek-ai/deepseek-coder-1.3b-instruct",
                "description": "DeepSeek 1.3Bå‚æ•°ä»£ç æ¨¡å‹",
                "params": "1.3B",
                "size_gb": 2.7
            },
            "deepseek-coder-6.7b": {
                "name": "deepseek-ai/deepseek-coder-6.7b-instruct",
                "description": "DeepSeek 6.7Bå‚æ•°ä»£ç æ¨¡å‹",
                "params": "6.7B",
                "size_gb": 14
            },
            # Qwenç³»åˆ—
            "qwen-coder-1.5b": {
                "name": "Qwen/Qwen2.5-Coder-1.5B-Instruct",
                "description": "Qwen 1.5Bå‚æ•°ä»£ç æ¨¡å‹",
                "params": "1.5B",
                "size_gb": 3
            },
            # CodeLlama
            "codellama-7b": {
                "name": "codellama/CodeLlama-7b-Instruct-hf",
                "description": "CodeLlama 7Bå‚æ•°æ¨¡å‹",
                "params": "7B",
                "size_gb": 14
            }
        }
    
    def create_model(self, model_id: str, use_quantization: bool = True) -> Dict[str, Any]:
        """åˆ›å»ºæŒ‡å®šæ¨¡å‹"""
        if model_id not in self.model_configs:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_id}")
        
        config = self.model_configs[model_id]
        print(f"ğŸš€ åˆ›å»ºæ¨¡å‹: {model_id}")
        print(f"ğŸ“Š å‚æ•°: {config['params']}, å¤§å°: {config['size_gb']}GB")
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        if not self._check_disk_space(config['size_gb']):
            print(f"âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨æ›´å°æ¨¡å‹")
            # è‡ªåŠ¨é™çº§
            return self._auto_downgrade(config['size_gb'])
        
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                config["name"],
                trust_remote_code=True,
                padding_side="left"
            )
            
            # é‡åŒ–é…ç½®
            quantization_config = None
            if use_quantization and torch.cuda.is_available():
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
            
            # åŠ è½½æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                config["name"],
                quantization_config=quantization_config,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                use_safetensors=True
            )
            
            # è®¾ç½®pad_token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            print(f"âœ… æ¨¡å‹ {model_id} åˆ›å»ºæˆåŠŸ")
            
            return {
                "model": model,
                "tokenizer": tokenizer,
                "config": config,
                "model_id": model_id
            }
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹...")
            return self._create_simulated_model(model_id)
    
    def _check_disk_space(self, required_gb: float) -> bool:
        """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
        import shutil
        
        try:
            total, used, free = shutil.disk_usage(self.cache_dir[:2])
            free_gb = free / (1024**3)
            print(f"ğŸ“Š å¯ç”¨ç©ºé—´: {free_gb:.1f}GB, éœ€è¦: {required_gb}GB")
            return free_gb >= required_gb * 1.5  # 1.5å€å®‰å…¨ç³»æ•°
        except:
            return True  # å¦‚æœæ— æ³•æ£€æŸ¥ï¼Œå‡è®¾ç©ºé—´è¶³å¤Ÿ
    
    def _auto_downgrade(self, required_gb: float) -> Dict[str, Any]:
        """è‡ªåŠ¨é™çº§åˆ°åˆé€‚çš„æ¨¡å‹"""
        # æŒ‰å¤§å°æ’åº
        sorted_models = sorted(
            self.model_configs.items(),
            key=lambda x: x[1]["size_gb"]
        )
        
        for model_id, config in sorted_models:
            if config["size_gb"] < required_gb:
                print(f"ğŸ”„ è‡ªåŠ¨é™çº§åˆ°: {model_id}")
                return self.create_model(model_id, use_quantization=True)
        
        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤ªå¤§ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
        print("âš ï¸ æ‰€æœ‰çœŸå®æ¨¡å‹éƒ½å¤ªå¤§ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹")
        return self._create_simulated_model("simulated")
    
    def _create_simulated_model(self, model_id: str) -> Dict[str, Any]:
        """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹"""
        print("ğŸ­ åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰")
        
        return {
            "model": None,
            "tokenizer": None,
            "config": {
                "name": "simulated",
                "description": "æ¨¡æ‹Ÿæ¨¡å‹ï¼Œæ— éœ€ä¸‹è½½",
                "params": "0",
                "size_gb": 0
            },
            "model_id": "simulated",
            "simulated": True
        }
    
    def list_available_models(self) -> Dict[str, Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹ä¿¡æ¯"""
        return self.model_configs