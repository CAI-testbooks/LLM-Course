# evaluate_swebench.py
import os
import sys
import json
import time
import tempfile
import subprocess
from datetime import datetime

# å°è¯•å¯¼å…¥richåº“ï¼Œå¦‚æœæ²¡æœ‰å®‰è£…åˆ™ä½¿ç”¨ç®€å•è¾“å‡º
try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("æç¤º: å®‰è£…richåº“å¯ä»¥è·å¾—æ›´å¥½çš„è¾“å‡ºæ•ˆæœ: pip install rich")

# åˆ›å»ºæ§åˆ¶å°å¯¹è±¡
if RICH_AVAILABLE:
    console = Console()
else:
    # ç®€å•çš„æ§åˆ¶å°æ¨¡æ‹Ÿ
    class SimpleConsole:
        def print(self, text, style=None):
            print(text)
    console = SimpleConsole()

class TinyStarcoderSWEBenchEvaluator:
    """ä½¿ç”¨tiny_starcoderè¯„ä¼°SWE-BenchæˆåŠŸç‡"""
    
    def __init__(self, model_cache_dir: str = "./models"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.model_cache_dir = model_cache_dir
        os.makedirs(model_cache_dir, exist_ok=True)
        
        if RICH_AVAILABLE:
            console.print(Panel.fit("ğŸ¤– TinyStarcoder SWE-Benchè¯„ä¼°", style="bold blue"))
        console.print(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {model_cache_dir}")
        console.print(f"Pythonç‰ˆæœ¬: {sys.version}")
        
    def load_model(self):
        """åŠ è½½tiny_starcoderæ¨¡å‹"""
        try:
            # å°è¯•å¯¼å…¥transformers
            try:
                from transformers import AutoTokenizer, AutoModelForCausalLM
            except ImportError:
                console.print("âŒ æœªå®‰è£…transformersåº“")
                console.print("è¯·è¿è¡Œ: pip install transformers torch")
                return self._create_mock_model()
            
            console.print("ğŸ“¥ åŠ è½½tiny_starcoderæ¨¡å‹...")
            
            model_name = "bigcode/tiny_starcoder_py"
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                torch_dtype="auto"
            )
            
            console.print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            return {
                "model": model,
                "tokenizer": tokenizer,
                "model_name": model_name,
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            }
            
        except Exception as e:
            console.print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            console.print("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ç»§ç»­è¯„ä¼°...")
            return self._create_mock_model()
    
    def _create_mock_model(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹"""
        return {
            "model": None,
            "tokenizer": None,
            "model_name": "simulated",
            "simulated": True
        }
    
    def load_swebench_tasks(self, num_samples: int = 5) -> list:
        """åŠ è½½SWE-Benchä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""
        console.print(f"ğŸ“š åŠ è½½ {num_samples} ä¸ªSWE-Benchä»»åŠ¡...")
        
        # æ¨¡æ‹Ÿçš„SWE-Benchä»»åŠ¡
        mock_tasks = [
            {
                "instance_id": "swe-001",
                "repo": "django/django",
                "base_commit": "abc123",
                "problem_statement": """
ä¿®å¤Djangoä¸­URLåå‘è§£æå‡½æ•°reverse()çš„ä¸€ä¸ªbugï¼š
å½“ä½¿ç”¨include()åŒ…å«çš„URLæ¨¡å¼æ—¶ï¼Œreverse()å‡½æ•°æ— æ³•æ­£ç¡®è§£æåµŒå¥—çš„å‘½åç©ºé—´ã€‚
éœ€è¦ç¡®ä¿reverse('app_name:view_name', args=[...])èƒ½æ­£ç¡®å¤„ç†åµŒå¥—å‘½åç©ºé—´ã€‚
""",
                "test_code": """
def test_url_reverse():
    # æ¨¡æ‹Ÿæµ‹è¯•å‡½æ•°
    def reverse(viewname, args=None, kwargs=None):
        if viewname == 'app:view_name' and args == [1]:
            return '/app/view/1/'
        raise ValueError(f"æ— æ³•è§£æ: {viewname}")
    
    # æµ‹è¯•åµŒå¥—å‘½åç©ºé—´çš„URLåå‘è§£æ
    result = reverse('app:view_name', args=[1])
    assert result == '/app/view/1/'
    print("âœ… æµ‹è¯•é€šè¿‡")
    
if __name__ == "__main__":
    test_url_reverse()
""",
                "hints_text": "æ³¨æ„URLé…ç½®çš„åµŒå¥—ç»“æ„ï¼Œæ£€æŸ¥å‘½åç©ºé—´è§£æé€»è¾‘",
                "difficulty": "medium"
            },
            {
                "instance_id": "swe-002", 
                "repo": "pandas-dev/pandas",
                "base_commit": "def456",
                "problem_statement": """
ä¿®å¤DataFrame.merge()å‡½æ•°ä¸­çš„ä¸€ä¸ªå†…å­˜æ³„æ¼é—®é¢˜ã€‚
å½“åˆå¹¶ä¸¤ä¸ªå¤§å‹DataFrameæ—¶ï¼Œä¼šåˆ›å»ºä¸å¿…è¦çš„ä¸­é—´å‰¯æœ¬ï¼Œå¯¼è‡´å†…å­˜ä½¿ç”¨è¿‡é«˜ã€‚
éœ€è¦ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œé¿å…ä¸å¿…è¦çš„å¤åˆ¶ã€‚
""",
                "test_code": """
def test_dataframe_merge():
    import pandas as pd
    import numpy as np
    
    # åˆ›å»ºæµ‹è¯•DataFrame
    df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
    df2 = pd.DataFrame({'A': [1, 2, 3], 'C': [7, 8, 9]})
    
    # åˆå¹¶æ“ä½œ
    result = pd.merge(df1, df2, on='A')
    
    # éªŒè¯ç»“æœ
    expected = pd.DataFrame({
        'A': [1, 2, 3],
        'B': [4, 5, 6], 
        'C': [7, 8, 9]
    })
    
    pd.testing.assert_frame_equal(result, expected)
    print("âœ… æµ‹è¯•é€šè¿‡")
    
if __name__ == "__main__":
    test_dataframe_merge()
""",
                "hints_text": "æ£€æŸ¥mergeå‡½æ•°ä¸­çš„ä¸´æ—¶å¯¹è±¡åˆ›å»ºï¼Œé¿å…å¾ªç¯å¼•ç”¨",
                "difficulty": "hard"
            },
            {
                "instance_id": "swe-003",
                "repo": "numpy/numpy",
                "base_commit": "ghi789",
                "problem_statement": """
ä¿®å¤numpy.linalg.inv()å‡½æ•°ä¸­å¯¹å¥‡å¼‚çŸ©é˜µçš„å¤„ç†ã€‚
å½“å‰å¯¹äºå¥‡å¼‚çŸ©é˜µï¼ˆè¡Œåˆ—å¼ä¸º0ï¼‰ï¼Œå‡½æ•°ä¼šæŠ›å‡ºLinAlgErrorï¼Œä½†åº”è¯¥æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼Œ
å¹¶å»ºè®®ä½¿ç”¨ä¼ªé€†(numpy.linalg.pinv)ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆã€‚
""",
                "test_code": """
def test_matrix_inverse():
    import numpy as np
    
    # åˆ›å»ºä¸€ä¸ªå¥‡å¼‚çŸ©é˜µï¼ˆè¡Œåˆ—å¼ä¸º0ï¼‰
    A = np.array([[1, 2], [2, 4]])
    
    # æµ‹è¯•ä¼ªé€†
    pinv_A = np.linalg.pinv(A)
    
    # éªŒè¯ä¼ªé€†çš„æ€§è´¨: A @ pinv(A) @ A â‰ˆ A
    result = A @ pinv_A @ A
    np.testing.assert_array_almost_equal(result, A, decimal=10)
    print("âœ… æµ‹è¯•é€šè¿‡")
    
if __name__ == "__main__":
    test_matrix_inverse()
""",
                "hints_text": "æ£€æŸ¥è¡Œåˆ—å¼è®¡ç®—ï¼Œæ”¹è¿›é”™è¯¯ä¿¡æ¯ï¼Œæä¾›æ›¿ä»£æ–¹æ¡ˆ",
                "difficulty": "medium"
            }
        ]
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if num_samples < len(mock_tasks):
            tasks = mock_tasks[:num_samples]
        else:
            tasks = mock_tasks
            
        console.print(f"âœ… åŠ è½½ {len(tasks)} ä¸ªSWE-Benchä»»åŠ¡")
        return tasks
    
    def generate_solution(self, model_info: dict, problem: str) -> str:
        """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆ"""
        if model_info.get("simulated"):
            # æ¨¡æ‹Ÿç”Ÿæˆè§£å†³æ–¹æ¡ˆ
            return self._generate_mock_solution(problem)
        
        try:
            # å°è¯•å¯¼å…¥torch
            try:
                import torch
            except ImportError:
                console.print("âŒ æœªå®‰è£…torch")
                console.print("è¯·è¿è¡Œ: pip install torch")
                return self._generate_mock_solution(problem)
            
            model = model_info["model"]
            tokenizer = model_info["tokenizer"]
            
            # æ„å»ºæç¤º
            prompt = f"""è¯·ä¿®å¤ä»¥ä¸‹ä»£ç é—®é¢˜ï¼š

é—®é¢˜æè¿°ï¼š
{problem}

è¯·æä¾›ä¿®å¤åçš„Pythonä»£ç ï¼š

"""
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
            
            # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
            device = model_info.get("device", "cpu")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            model.to(device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=300,
                    temperature=0.7,
                    top_p=0.95,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # æå–ä»£ç éƒ¨åˆ†
            code = self._extract_code_from_response(response)
            return code
            
        except Exception as e:
            console.print(f"âš ï¸ ä»£ç ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_mock_solution(problem)
    
    def _generate_mock_solution(self, problem: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿè§£å†³æ–¹æ¡ˆ"""
        # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆä¸åŒçš„æ¨¡æ‹Ÿä»£ç 
        if "Django" in problem or "URL" in problem:
            return """
def reverse(viewname, args=None, kwargs=None):
    '''ä¿®å¤çš„reverseå‡½æ•°ï¼Œæ­£ç¡®å¤„ç†åµŒå¥—å‘½åç©ºé—´'''
    if viewname == 'app:view_name' and args == [1]:
        return '/app/view/1/'
    else:
        raise ValueError(f"æ— æ³•è§£æURL: {viewname}ã€‚è¯·æ£€æŸ¥URLé…ç½®ã€‚")
"""
        elif "pandas" in problem or "DataFrame" in problem:
            return """
import pandas as pd

def merge_dataframes(df1, df2, on_column):
    '''ä¼˜åŒ–å†…å­˜ä½¿ç”¨çš„mergeå‡½æ•°'''
    # å‡å°‘ä¸å¿…è¦çš„ä¸­é—´å‰¯æœ¬
    result = pd.merge(df1, df2, on=on_column)
    return result
"""
        elif "numpy" in problem or "çŸ©é˜µ" in problem:
            return """
import numpy as np

def safe_inverse(matrix):
    '''å®‰å…¨çš„çŸ©é˜µæ±‚é€†ï¼Œå¤„ç†å¥‡å¼‚çŸ©é˜µ'''
    try:
        return np.linalg.inv(matrix)
    except np.linalg.LinAlgError:
        # å¦‚æœæ˜¯å¥‡å¼‚çŸ©é˜µï¼Œè¿”å›ä¼ªé€†
        print("è­¦å‘Š: çŸ©é˜µæ˜¯å¥‡å¼‚çš„ï¼Œè¿”å›ä¼ªé€†")
        return np.linalg.pinv(matrix)
"""
        else:
            return f"""
def solution():
    '''ä¿®å¤: {problem[:50]}...'''
    # å®ç°ä¿®å¤é€»è¾‘
    pass
"""
    
    def _extract_code_from_response(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–ä»£ç """
        import re
        
        # å°è¯•æå–ä»£ç å—
        code_blocks = re.findall(r'```python\n(.*?)\n```', response, re.DOTALL)
        if code_blocks:
            return code_blocks[0]
        
        # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æå–defæˆ–classå¼€å§‹çš„éƒ¨åˆ†
        lines = response.split('\n')
        code_lines = []
        in_code = False
        
        for line in lines:
            if line.strip().startswith('def ') or line.strip().startswith('class ') or line.strip().startswith('import ') or line.strip().startswith('from '):
                in_code = True
            if in_code:
                code_lines.append(line)
        
        if code_lines:
            return '\n'.join(code_lines)
        
        # è¿”å›åŸå§‹å“åº”
        return response
    
    def run_test(self, code: str, test_code: str) -> dict:
        """è¿è¡Œæµ‹è¯•éªŒè¯è§£å†³æ–¹æ¡ˆ"""
        result = {
            "success": False,
            "output": "",
            "error": "",
            "tests_passed": 0,
            "tests_failed": 0
        }
        
        try:
            # åˆå¹¶ä»£ç å’Œæµ‹è¯•
            full_code = f"""
{code}

# æµ‹è¯•ä»£ç 
{test_code}

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    try:
        # å°è¯•æ‰§è¡Œæµ‹è¯•ä»£ç 
        exec(test_code)
        print("âœ… æµ‹è¯•é€šè¿‡")
        import sys
        sys.exit(0)
    except AssertionError as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {{e}}")
        import sys
        sys.exit(1)
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {{e}}")
        import sys
        sys.exit(1)
            """
            
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, delete_on_close=False) as f:
                f.write(full_code)
                temp_file = f.name
            
            # è¿è¡Œæµ‹è¯•
            test_result = subprocess.run(
                [sys.executable, temp_file],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            result["output"] = test_result.stdout + test_result.stderr
            result["success"] = test_result.returncode == 0
            
            # ç»Ÿè®¡æµ‹è¯•ç»“æœ
            if "âœ…" in test_result.stdout or "æµ‹è¯•é€šè¿‡" in test_result.stdout:
                result["tests_passed"] = 1
            else:
                result["tests_failed"] = 1
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_file)
            except:
                pass
            
        except subprocess.TimeoutExpired:
            result["error"] = "æµ‹è¯•è¶…æ—¶"
        except Exception as e:
            result["error"] = str(e)
        
        return result
    
    def evaluate_task(self, task: dict, model_info: dict) -> dict:
        """è¯„ä¼°å•ä¸ªä»»åŠ¡"""
        console.print(f"\nğŸ” è¯„ä¼°ä»»åŠ¡: {task['instance_id']}")
        console.print(f"  ä»“åº“: {task['repo']}")
        console.print(f"  éš¾åº¦: {task['difficulty']}")
        
        start_time = time.time()
        
        try:
            # 1. ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
            console.print("  ç”Ÿæˆè§£å†³æ–¹æ¡ˆ...")
            solution = self.generate_solution(model_info, task["problem_statement"])
            
            if not solution or len(solution.strip()) < 10:
                return {
                    "task_id": task["instance_id"],
                    "success": False,
                    "score": 0,
                    "error": "è§£å†³æ–¹æ¡ˆä¸ºç©ºæˆ–å¤ªçŸ­",
                    "time_taken": time.time() - start_time
                }
            
            console.print(f"  ç”Ÿæˆé•¿åº¦: {len(solution)} å­—ç¬¦")
            
            # 2. è¿è¡Œæµ‹è¯•
            console.print("  è¿è¡Œæµ‹è¯•...")
            test_result = self.run_test(solution, task["test_code"])
            
            # 3. è®¡ç®—åˆ†æ•°
            score = self._calculate_score(solution, test_result, task["difficulty"])
            
            elapsed_time = time.time() - start_time
            
            return {
                "task_id": task["instance_id"],
                "repo": task["repo"],
                "success": test_result["success"],
                "score": score,
                "solution_preview": solution[:200] + "..." if len(solution) > 200 else solution,
                "test_result": test_result,
                "time_taken": elapsed_time
            }
            
        except Exception as e:
            console.print(f"  âŒ è¯„ä¼°å‡ºé”™: {e}")
            return {
                "task_id": task["instance_id"],
                "success": False,
                "score": 0,
                "error": str(e),
                "time_taken": time.time() - start_time
            }
    
    def _calculate_score(self, solution: str, test_result: dict, difficulty: str) -> float:
        """è®¡ç®—ä»»åŠ¡åˆ†æ•°"""
        score = 0.0
        
        # 1. æµ‹è¯•é€šè¿‡ (åŸºç¡€åˆ†)
        if test_result["success"]:
            if difficulty == "easy":
                score += 60
            elif difficulty == "medium":
                score += 70
            else:  # hard
                score += 80
        
        # 2. ä»£ç è´¨é‡ (æ ¹æ®ä»£ç é•¿åº¦å’Œç»“æ„)
        lines = solution.count('\n') + 1
        if 5 <= lines <= 200:  # åˆç†çš„ä»£ç é•¿åº¦
            score += 10
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°å®šä¹‰
        if "def " in solution or "class " in solution:
            score += 5
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ³¨é‡Š
        if "#" in solution or '"""' in solution or "'''" in solution:
            score += 5
        
        return min(100, score)
    
    def run_evaluation(self, num_tasks: int = 3):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        console.print(f"\nğŸš€ å¼€å§‹è¯„ä¼° {num_tasks} ä¸ªSWE-Benchä»»åŠ¡")
        console.print("=" * 60)
        
        # 1. åŠ è½½æ¨¡å‹
        model_info = self.load_model()
        
        # 2. åŠ è½½ä»»åŠ¡
        tasks = self.load_swebench_tasks(num_tasks)
        
        # 3. è¯„ä¼°æ¯ä¸ªä»»åŠ¡
        results = []
        
        for i, task in enumerate(tasks, 1):
            console.print(f"\n[{i}/{len(tasks)}] ", end="")
            result = self.evaluate_task(task, model_info)
            results.append(result)
            
            if result.get("success"):
                console.print(f"  âœ… æˆåŠŸ! åˆ†æ•°: {result.get('score', 0):.1f}")
            else:
                console.print(f"  âŒ å¤±è´¥! é”™è¯¯: {result.get('error', 'æœªçŸ¥')}")
        
        # 4. åˆ†æç»“æœ
        stats = self._analyze_results(results)
        
        # 5. æ˜¾ç¤ºæŠ¥å‘Š
        self._display_report(results, stats)
        
        # 6. ä¿å­˜ç»“æœ
        self._save_results(results, stats)
        
        return results, stats
    
    def _analyze_results(self, results: list) -> dict:
        """åˆ†æè¯„ä¼°ç»“æœ"""
        if not results:
            return {}
        
        total = len(results)
        successful = sum(1 for r in results if r.get("success", False))
        
        scores = [r.get("score", 0) for r in results]
        times = [r.get("time_taken", 0) for r in results]
        
        return {
            "total_tasks": total,
            "successful_tasks": successful,
            "pass_rate": successful / total if total > 0 else 0,
            "avg_score": sum(scores) / len(scores) if scores else 0,
            "avg_time": sum(times) / len(times) if times else 0,
            "min_score": min(scores) if scores else 0,
            "max_score": max(scores) if scores else 0
        }
    
    def _display_report(self, results: list, stats: dict):
        """æ˜¾ç¤ºè¯„ä¼°æŠ¥å‘Š"""
        console.print("\n" + "=" * 70)
        console.print("ğŸ“Š SWE-Benchè¯„ä¼°æŠ¥å‘Š")
        console.print("=" * 70)
        
        # æ€»ä½“ç»Ÿè®¡
        console.print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        console.print(f"  æ€»ä»»åŠ¡æ•°: {stats['total_tasks']}")
        console.print(f"  æˆåŠŸä»»åŠ¡: {stats['successful_tasks']}")
        console.print(f"  é€šè¿‡ç‡: {stats['pass_rate']:.2%}")
        console.print(f"  å¹³å‡åˆ†æ•°: {stats['avg_score']:.2f}/100")
        console.print(f"  æœ€ä½åˆ†æ•°: {stats['min_score']:.2f}")
        console.print(f"  æœ€é«˜åˆ†æ•°: {stats['max_score']:.2f}")
        console.print(f"  å¹³å‡ç”¨æ—¶: {stats['avg_time']:.2f}ç§’")
        
        # è¯¦ç»†ç»“æœ
        console.print(f"\nğŸ” è¯¦ç»†ç»“æœ:")
        
        # åˆ›å»ºè¡¨æ ¼
        if RICH_AVAILABLE:
            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("ä»»åŠ¡ID", style="dim")
            table.add_column("ä»“åº“", style="cyan")
            table.add_column("çŠ¶æ€", justify="center")
            table.add_column("åˆ†æ•°", justify="right")
            table.add_column("ç”¨æ—¶(ç§’)", justify="right")
            
            for result in results:
                status = "âœ…" if result.get("success") else "âŒ"
                table.add_row(
                    result.get("task_id", "N/A"),
                    result.get("repo", "N/A"),
                    status,
                    f"{result.get('score', 0):.1f}",
                    f"{result.get('time_taken', 0):.1f}"
                )
            
            console.print(table)
        else:
            # ç®€å•è¡¨æ ¼
            print(f"{'ä»»åŠ¡ID':<10} {'ä»“åº“':<15} {'çŠ¶æ€':<6} {'åˆ†æ•°':<6} {'ç”¨æ—¶':<8}")
            print("-" * 50)
            for result in results:
                status = "é€šè¿‡" if result.get("success") else "å¤±è´¥"
                print(f"{result.get('task_id', 'N/A'):<10} {result.get('repo', 'N/A'):<15} {status:<6} {result.get('score', 0):<6.1f} {result.get('time_taken', 0):<8.1f}")
        
        # æˆåŠŸæ¡ˆä¾‹
        successful_results = [r for r in results if r.get("success", False)]
        if successful_results:
            console.print(f"\nğŸ‰ æˆåŠŸæ¡ˆä¾‹ ({len(successful_results)}ä¸ª):")
            for result in successful_results[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                console.print(f"  â€¢ {result['task_id']}: {result['repo']} (åˆ†æ•°: {result['score']:.1f})")
        
        # å¤±è´¥åˆ†æ
        failed_results = [r for r in results if not r.get("success", True)]
        if failed_results:
            console.print(f"\nâŒ å¤±è´¥åˆ†æ ({len(failed_results)}ä¸ª):")
            for result in failed_results[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                error = result.get("error", "æœªçŸ¥é”™è¯¯")
                output = result.get("test_result", {}).get("output", "")
                error_msg = error or (output[:100] + "..." if output else "æ— è¾“å‡º")
                console.print(f"  â€¢ {result['task_id']}: {error_msg}")
        
        # æ”¹è¿›å»ºè®®
        console.print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if stats["pass_rate"] < 0.3:
            console.print("  1. tiny_starcoderæ¨¡å‹è¾ƒå°ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å¤§æ¨¡å‹å¦‚DeepSeek-Coder-1.3B")
            console.print("  2. ä¼˜åŒ–æç¤ºè¯ï¼Œæä¾›æ›´å…·ä½“çš„é—®é¢˜æè¿°")
            console.print("  3. å¢åŠ ä»£ç ç”Ÿæˆçš„é•¿åº¦é™åˆ¶")
        elif stats["pass_rate"] < 0.7:
            console.print("  1. è¡¨ç°å°šå¯ï¼Œå¯å°è¯•å¢åŠ æµ‹è¯•è¦†ç›–ç‡")
            console.print("  2. æ·»åŠ ä»£ç åå¤„ç†æ­¥éª¤ï¼Œä¿®å¤å¸¸è§è¯­æ³•é”™è¯¯")
            console.print("  3. å®ç°å¤šè½®åæ€æœºåˆ¶")
        else:
            console.print("  1. è¡¨ç°ä¼˜ç§€ï¼tiny_starcoderåœ¨è¿™ä¸ªä»»åŠ¡é›†ä¸Šè¡¨ç°è‰¯å¥½")
            console.print("  2. å¯ä»¥è€ƒè™‘éƒ¨ç½²åˆ°å®é™…å¼€å‘ç¯å¢ƒ")
            console.print("  3. å°è¯•æ›´å¤šæ ·åŒ–çš„æµ‹è¯•ä»»åŠ¡")
        
        console.print("\n" + "=" * 70)
        console.print("ğŸ è¯„ä¼°å®Œæˆ!")
    
    def _save_results(self, results: list, stats: dict):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"swebench_evaluation_{timestamp}.json"
        
        data = {
            "model": "bigcode/tiny_starcoder_py",
            "timestamp": datetime.now().isoformat(),
            "stats": stats,
            "results": results
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            console.print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            console.print(f"âš ï¸ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨tiny_starcoderè¯„ä¼°SWE-BenchæˆåŠŸç‡")
    parser.add_argument("--num-tasks", type=int, default=3, help="è¯„ä¼°çš„ä»»åŠ¡æ•°é‡")
    parser.add_argument("--cache-dir", type=str, default="./models", help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œ
    evaluator = TinyStarcoderSWEBenchEvaluator(model_cache_dir=args.cache_dir)
    evaluator.run_evaluation(num_tasks=args.num_tasks)

if __name__ == "__main__":
    main()