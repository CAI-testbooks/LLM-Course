<img width="83" height="24" alt="image" src="https://github.com/user-attachments/assets/faa0ae30-0483-4ac7-93b4-51dfa3db8f10" /> # **AutoDev-Agent——基于大模型的自动化软件开发代理系统**  

---

## 一、实验题目  
基于大模型的自动化软件开发代理系统系统设计与原型实现  

> 代码仓库链接：[LLM-Course课设仓库：AutoDev-Agent](https://github.com/lcx4411/AutoDev-Agent)

---

## 二、实验目的  

1. 理解并掌握大模型在软件开发自动化中的应用场景与实现方法。  
2. 设计并实现一个基于Agent架构的AI辅助编程系统，支持需求理解、代码生成、测试生成、错误修复等功能。  
3. 通过实际数据集（HumanEval、MBPP、SWE-Bench Lite）验证系统各模块的有效性。  
4. 探索并实践Self-Reflection机制在AI辅助开发中的迭代优化作用。  

---

## 三、实验内容与系统架构  

### 3.1 系统目标  
构建一个**多Agent协同**的AI开发助手，具备以下能力：  
- 自然语言需求理解与任务拆解  
- 函数/文件级代码生成  
- 单元测试自动生成  
- 基于错误信息的Bug定位与修复  
- 自我反思与迭代优化  

### 3.2 系统架构  
系统采用分层Agent协作架构，具体流程如下：  

```
        ┌──────────────┐
        │   用户需求   │
        └──────┬───────┘
               ↓
        ┌──────────────┐
        │ Task Planner │ ← 需求理解 / 任务拆解
        └──────┬───────┘
               ↓
     ┌─────────┴─────────┐
     ↓                   ↓
Code Generator     Test Generator
     ↓                   ↓
┌──────────────┐   ┌──────────────┐
│ 代码执行环境  │←→ │ 单元测试运行 │
└──────┬───────┘   └──────┬───────┘
       ↓                  ↓
   Error Trace      Test Failure
        └──────┬───────┘
               ↓
        ┌──────────────┐
        │ Bug Fix Agent│ ← 反思 + 修复
        └──────────────┘
```

### 3.2 项目结构

```text
code-assistant-agent/
├── README.md                    # 项目说明文档
├── .gitignore                   # Git忽略文件
│
├── config/                     # 配置文件目录
│   ├── __init__.py
│   ├── config.yaml            # 主配置文件
│   ├── model_config.yaml      # 模型配置
│   └── prompt_templates.yaml  # 提示词模板
│
├── src/                        # 源代码目录
│   ├── __init__.py
│   ├── main.py                # 主入口文件
│   │
│   ├── core/                  # 核心逻辑
│   │   ├── __init__.py
│   │   ├── agent.py          # 主Agent类
│   │   ├── bugfix_agent.py   # 针对缺陷修复的Agent类
│   │   ├── plan_schema.py    # 任务结构定义（可拓展）
│   │   ├── planner.py        # 任务规划器
│   │   ├── code_generator.py # 代码生成器
│   │   ├── tester.py         # 测试生成器
│   │   ├── fixer.py          # Bug修复器
│   │   └── reflection.py     # 反思引擎
│   │ 
│   ├── evaluation/           # 评估模块
│   │   ├── __init__.py
│   │   ├── humaneval_eval.py     # humaneval代码生成评估
│   │   ├── mbpp_testgen_eval.py  # mbpp测试生成评估
│   │   └── swebench_fix_eval.py  # swebench代码修评估
│   │ 
│   ├── models/                # 模型相关
│   │   ├── __init__.py
│   │   ├── base_model.py     # 模型基类
│   │   └── qwen_model.py     # Qwen适配器
│   │
│   ├── tools/                 # 工具模块
│   │   ├── __init__.py
│   │   ├── base_tool.py      # 工具基类
│   │   └── python_repl.py    # Python执行器
│   │
│   ├── utils/                 # 工具函数
│   │   ├── __init__.py
│   │   ├── llm_utils.py      # LLM调用
│   │   └── config_utils.py   # 配置加载
│     
```

---

## 四、实验步骤与实现方案  

### 4.1 数据准备  
使用开源代码数据集进行模块验证：  
- **HumanEval**：用于代码生成任务评估  
- **MBPP**：用于测试生成任务评估  
- **SWE-Bench Lite**：用于Bug修复任务评估  

> 数据集通过HuggingFace直接加载，便于复现与实验。

### 4.2 模型选择  
- **基础模型**：Qwen3-Coder-7B/14B（代码生成与修复）  
- **备用模型**：DeepSeek-Coder-6.7B（轻量快速场景）  
- **推理方式**：Prompt Engineering + Tool-Calling  

### 4.3 核心Agent设计  

#### 4.3.1 Task Planner  
将自然语言需求拆解为结构化开发任务，输出JSON格式任务描述。  

#### 4.3.2 Code Generator Agent  
基于任务描述生成符合规范的代码，支持多语言（优先Python）。  

#### 4.3.3 Test Generator Agent  
针对生成的代码自动生成单元测试用例，覆盖正常、边界与异常情况。  

#### 4.3.4 Bug Fix Agent  
结合错误信息与测试失败信息，分析原因并生成修复后的代码。  

### 4.4 Reflection Agent
引入Self-Reflection Prompt，使系统能在失败后分析原因、调整假设，并进行多轮迭代优化。  

---

## 五、实验进展与当前版本说明  

### 5.0 历史版本（第一版）  
- 已完成系统整体架构设计与各Agent功能定义  
- 完成Prompt模板设计与数据集选型  
- 实现模拟数据流，验证系统逻辑可行性

### 5.1 当前版本（第二版）
- 参考标准项目开发流程，构建规范的项目结构，后续将基于该版本进行功能完善。已将第一版内容完成迁移。

### 5.2 已实现功能  
- Task Planner 结构化输出  
- Code / Test / Bug Fix Agent 的Prompt模板  
- Self-Reflection 机制设计  

### 5.3 待实现与优化  
1. 完善各Agent的具体实现与模型调用  
2. 实现评估指标计算（Pass@1、测试覆盖率、修复成功率等）  
3. 模块解耦，支持各Agent独立运行与测试  
4. 开发CLI工具，提供用户交互界面  

---

## 六、实验评估方案与结果分析

为全面评估所提出自动化开发与修复系统在不同软件工程任务中的有效性，本文选取了 HumanEval、MBPP 以及 SWE-Bench Lite 三个具有代表性的数据集，分别对应 代码生成、测试生成以及真实缺陷修复 三类任务，覆盖从函数级实现到真实工程 Bug 修复的不同难度层级。

### 6.1 HumanEval：代码生成功能正确性评估

HumanEval 数据集主要用于评估模型在函数级代码生成任务中的语义理解与实现能力。该评测采用 Pass@1 作为指标，即模型在首次生成代码且不进行任何修正的情况下，通过官方单元测试的比例。

实验结果如下：

```yaml
Total Samples : 164
Passed        : 163
Pass@1        : 0.9939
```

结果表明，所提出的系统在 HumanEval 上取得了 99.39% 的 Pass@1，几乎覆盖全部测试样本。这说明：

- 系统在自然语言描述到函数级实现的映射上具有极强的准确性；

- 生成代码在语法、边界条件及核心逻辑上表现稳定；

- 规划（Planner）与代码生成（Generator）模块之间的协同机制能够有效减少生成错误。

该结果验证了系统在基础代码生成任务上的可靠性，为后续更复杂任务提供了坚实基础。

### 6.2 MBPP：自动测试生成有效性评估

MBPP 数据集用于评估模型在给定问题描述的情况下，生成**高质量断言测试（assert-based tests）**的能力。该任务重点考察测试是否：

1. 可执行（Executable）

2. 能覆盖合理的输入空间

3. 能有效揭示潜在实现错误

评估结果如下：

```yaml
Total Samples        : 500
Executable Tests     : 498
AssertionError       : 185
Timeout Cases        : 1
Runtime Error Cases  : 1
Test Validity Rate   : 0.9960
```

从结果可以看出：

- 99.6% 的测试样本可以成功执行，说明测试生成模块在语法与运行时安全性方面具有很高稳定性；

- 185 个 AssertionError 表明生成的测试在执行过程中成功触发了不满足断言的情况，这恰恰体现了测试对实现逻辑的判别能力；

- Timeout 与 Runtime Error 样本极少，说明系统在测试规模与复杂度控制方面较为合理。

总体来看，系统生成的测试不仅具备良好的可执行性，同时在发现潜在逻辑缺陷方面具有较强实用价值，验证了测试生成模块在自动化测试场景下的有效性。

### 6.3 SWE-Bench Lite：真实工程缺陷修复评估

SWE-Bench Lite 是一个真实世界软件缺陷修复数据集，包含来自多个成熟开源项目的真实 Bug 报告与对应修复补丁。该任务是对系统 综合理解能力、代码定位能力与修复能力 的全面检验。

本实验采用基于补丁文本相似度的自动评估方式，并设定相似度阈值来判断是否修复成功。实验结果如下：

```yaml
Total Samples      : 300
Average Similarity : 0.17963
Matched Fixes      : 105
Fix Rate           : 0.3500
```

分析结果表明：

- 系统在 SWE-Bench Lite 上达到了 35.0% 的修复成功率；

- 在未引入项目级上下文、完整测试环境及多轮交互的前提下，该结果已具备较强竞争力；

- 平均相似度为 0.17963，说明即使在未完全匹配官方补丁的情况下，模型生成的修复方案在结构和逻辑上仍具有一定合理性。

考虑到 SWE-Bench 任务涉及 跨文件依赖、复杂工程上下文及隐含设计约束，该实验结果验证了系统在真实工程 Bug 修复场景下的可行性与扩展潜力。

### 6.4 综合分析与讨论

综合三组实验结果可以得出以下结论：

1. **从简单到复杂任务的能力递进明显**
系统在 HumanEval 上表现接近完美，在 MBPP 测试生成任务中保持高度稳定，在 SWE-Bench Lite 上仍能实现有效修复，体现出良好的泛化能力。

2. **模块化设计提高了系统整体鲁棒性**
代码生成、测试生成与缺陷修复模块之间相互独立又协同工作，使系统能够适配不同类型的软件工程任务。

3. **真实工程场景仍存在提升空间**
SWE-Bench 的结果表明，未来可通过引入多轮修复、项目级上下文建模或测试驱动修复机制进一步提升修复成功率。

总体而言，实验结果充分验证了本文提出系统在自动代码生成、测试生成及缺陷修复等多项软件工程任务中的有效性与实用价值。

--- 

> 说明：本报告为第二版实验报告，后续将随系统迭代持续更新完善。
