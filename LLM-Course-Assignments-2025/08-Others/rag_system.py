"""
æ°”è±¡æ™ºèƒ½RAGç³»ç»Ÿ - ç®€åŒ–ç‰ˆæœ¬
ä¿®å¤transformersç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional, Any
import json
import pickle
from pathlib import Path
import warnings
import logging
import re
from datetime import datetime
import sys

warnings.filterwarnings('ignore')

from sentence_transformers import SentenceTransformer, util
from config import config


class KnowledgeBaseRetriever:
    """çŸ¥è¯†åº“æ£€ç´¢å™¨"""

    def __init__(self, knowledge_path: str = None, model_name: str = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“æ£€ç´¢å™¨

        Args:
            knowledge_path: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„
            model_name: æ£€ç´¢æ¨¡å‹åç§°
        """
        self.knowledge_path = knowledge_path or config.paths.get("knowledge_json", "/home/Liyang/agent/knowledge_base.json")
        self.model_name = model_name or config.knowledge_config.get('base_model', 'paraphrase-multilingual-MiniLM-L12-v2')

        # åŠ è½½çŸ¥è¯†åº“
        self.knowledge_base = self.load_knowledge_base()

        # åˆå§‹åŒ–æ£€ç´¢æ¨¡å‹
        self.retrieval_model = SentenceTransformer(self.model_name)

        # æ„å»ºæ–‡æ¡£å’Œå‘é‡
        self.documents, self.document_embeddings = self.prepare_documents()

        print(f"çŸ¥è¯†åº“æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ: {len(self.documents)} æ–‡æ¡£")

    def load_knowledge_base(self) -> Dict:
        """åŠ è½½çŸ¥è¯†åº“"""
        knowledge_path = Path(self.knowledge_path)

        if not knowledge_path.exists():
            print(f"è­¦å‘Š: çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_path}")
            return {"items": []}

        try:
            with open(knowledge_path, 'r', encoding='utf-8') as f:
                knowledge_base = json.load(f)
            print(f"æˆåŠŸåŠ è½½çŸ¥è¯†åº“: {len(knowledge_base.get('items', []))} ä¸ªæ¡ç›®")
            return knowledge_base
        except Exception as e:
            print(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
            return {"items": []}

    def format_document(self, item: Dict) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†åº“æ¡ç›®ä¸ºæ–‡æ¡£æ–‡æœ¬"""
        parts = []

        # æ ‡é¢˜
        if 'title' in item:
            parts.append(f"æ ‡é¢˜: {item['title']}")

        # ç±»åˆ«
        if 'category' in item:
            parts.append(f"ç±»åˆ«: {item['category']}")

        # ç§‘å­¦ä¾æ®
        if 'scientific_basis' in item:
            parts.append(f"ç§‘å­¦ä¾æ®: {item['scientific_basis']}")

        # é¢„è­¦æŒ‡æ ‡
        if 'warning_indicators' in item:
            parts.append(f"é¢„è­¦æŒ‡æ ‡: {item['warning_indicators']}")

        # å½±å“ä¸åº”å¯¹
        if 'impact_response' in item:
            parts.append(f"å½±å“ä¸åº”å¯¹: {item['impact_response']}")

        return "\n".join(parts)

    def prepare_documents(self) -> Tuple[List[str], np.ndarray]:
        """å‡†å¤‡æ–‡æ¡£å’ŒåµŒå…¥å‘é‡"""
        documents = []
        items = self.knowledge_base.get('items', [])

        for i, item in enumerate(items):
            doc_text = self.format_document(item)
            documents.append(doc_text)

        if not documents:
            # åˆ›å»ºä¸€äº›ç¤ºä¾‹æ–‡æ¡£
            documents = [
                "é«˜æ¸©å¤©æ°”å®¹æ˜“å¯¼è‡´ä¸­æš‘ï¼Œå»ºè®®å‡å°‘æˆ·å¤–æ´»åŠ¨ï¼Œå¤šå–æ°´ï¼Œé¿å…ä¸­åˆæ—¶æ®µå¤–å‡ºã€‚",
                "æš´é›¨å¤©æ°”å¯èƒ½å¯¼è‡´åŸå¸‚å†…æ¶ï¼Œè¯·æ³¨æ„äº¤é€šå®‰å…¨ï¼Œé¿å…æ¶‰æ°´è¡Œè½¦ã€‚",
                "å¹²æ—±å¤©æ°”éœ€è¦èŠ‚çº¦ç”¨æ°´ï¼Œæ³¨æ„é˜²ç«ï¼Œå‡å°‘æˆ·å¤–æ´»åŠ¨ã€‚",
                "å°é£å¤©æ°”é£åŠ›å¼ºåŠ²ï¼Œè¯·å›ºå®šå¥½é—¨çª—ï¼Œé¿å…å¤–å‡ºã€‚",
                "å¯’æ½®å¤©æ°”æ°”æ¸©éª¤é™ï¼Œè¯·æ³¨æ„ä¿æš–ï¼Œé¢„é˜²æ„Ÿå†’ã€‚"
            ]
            print("ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£ï¼Œå› ä¸ºæ²¡æœ‰åŠ è½½åˆ°å®é™…çŸ¥è¯†åº“")

        # è®¡ç®—æ–‡æ¡£åµŒå…¥å‘é‡
        print(f"è®¡ç®—æ–‡æ¡£åµŒå…¥å‘é‡...")
        document_embeddings = self.retrieval_model.encode(
            documents,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        return documents, document_embeddings

    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if len(self.documents) == 0:
            return []

        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.retrieval_model.encode(
            query,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = util.cos_sim(query_embedding, self.document_embeddings)[0]

        # è·å–top_kç»“æœ
        top_indices = torch.topk(similarities, k=min(top_k, len(self.documents))).indices.tolist()

        # æ„å»ºç»“æœ
        results = []
        for idx in top_indices:
            similarity = similarities[idx].item()
            results.append({
                'document': self.documents[idx],
                'similarity': similarity,
                'rank': len(results) + 1
            })

        return results


class WeatherFeatureExtractor:
    """å¤©æ°”ç‰¹å¾æå–å™¨"""

    def __init__(self):
        self.features = {}

    def extract(self, query: str) -> Dict:
        """ä»æŸ¥è¯¢ä¸­æå–å¤©æ°”ç‰¹å¾"""
        features = {}

        # æ¸©åº¦æå–
        temp_patterns = [
            r'æ¸©åº¦\s*([0-9]+\.?[0-9]*)\s*â„ƒ',
            r'([0-9]+\.?[0-9]*)\s*â„ƒ',
            r'æ°”æ¸©\s*([0-9]+\.?[0-9]*)'
        ]

        for pattern in temp_patterns:
            match = re.search(pattern, query)
            if match:
                try:
                    temp = float(match.group(1))
                    features['temperature'] = temp

                    if temp >= 35:
                        features['heat_level'] = 'é…·çƒ­'
                    elif temp >= 30:
                        features['heat_level'] = 'ç‚çƒ­'
                    elif temp >= 25:
                        features['heat_level'] = 'æ¸©æš–'
                    elif temp >= 15:
                        features['heat_level'] = 'èˆ’é€‚'
                    else:
                        features['heat_level'] = 'å¯’å†·'

                    break
                except ValueError:
                    continue

        # æ¹¿åº¦æå–
        humidity_patterns = [
            r'æ¹¿åº¦\s*([0-9]+\.?[0-9]*)\s*%',
            r'([0-9]+\.?[0-9]*)\s*%æ¹¿åº¦'
        ]

        for pattern in humidity_patterns:
            match = re.search(pattern, query)
            if match:
                try:
                    humidity = float(match.group(1))
                    features['humidity'] = humidity

                    if humidity >= 80:
                        features['humidity_level'] = 'é«˜æ¹¿'
                    elif humidity >= 60:
                        features['humidity_level'] = 'ä¸­ç­‰'
                    elif humidity >= 40:
                        features['humidity_level'] = 'èˆ’é€‚'
                    else:
                        features['humidity_level'] = 'å¹²ç‡¥'

                    break
                except ValueError:
                    continue

        # å¤©æ°”ç°è±¡å…³é”®è¯
        weather_keywords = {
            'é™é›¨': 'rain',
            'é™æ°´': 'precipitation',
            'ä¸‹é›¨': 'rain',
            'æš´é›¨': 'heavy_rain',
            'å¤§é£': 'wind',
            'å°é£': 'typhoon',
            'çƒ­æµª': 'heatwave',
            'é«˜æ¸©': 'high_temperature',
            'å¹²æ—±': 'drought',
            'å¯’æ½®': 'cold_wave',
            'éœœå†»': 'frost',
            'é›¾éœ¾': 'haze',
            'æ²™å°˜': 'sandstorm'
        }

        for keyword, feature_key in weather_keywords.items():
            if keyword in query:
                features[feature_key] = True

        # æ—¶é—´å…³é”®è¯
        time_keywords = {
            'ä»Šå¤©': 'today',
            'æ˜å¤©': 'tomorrow',
            'åå¤©': 'day_after_tomorrow',
            'æœ¬å‘¨': 'this_week',
            'å‘¨æœ«': 'weekend',
            'æœªæ¥ä¸‰å¤©': 'next_three_days',
            'ä¸‹å‘¨': 'next_week'
        }

        for keyword, time_key in time_keywords.items():
            if keyword in query:
                features['time_period'] = time_key
                break

        return features


class ResponseGenerator:
    """å“åº”ç”Ÿæˆå™¨"""

    def __init__(self):
        # å“åº”æ¨¡æ¿
        self.templates = {
            'high_temperature': [
                "æ ¹æ®æŸ¥è¯¢ï¼Œå½“å‰å¤©æ°”æ¸©åº¦è¾ƒé«˜ï¼Œéœ€è¦æ³¨æ„é˜²æš‘é™æ¸©ã€‚",
                "é«˜æ¸©å¤©æ°”å®¹æ˜“å¼•å‘ä¸­æš‘ï¼Œå»ºè®®å‡å°‘æˆ·å¤–æ´»åŠ¨ã€‚",
                "è¯·åšå¥½é˜²æ™’æªæ–½ï¼Œé¿å…åœ¨é«˜æ¸©æ—¶æ®µè¿›è¡Œå‰§çƒˆè¿åŠ¨ã€‚"
            ],
            'rain': [
                "æŸ¥è¯¢æ¶‰åŠé™é›¨å¤©æ°”ï¼Œè¯·æ³¨æ„æºå¸¦é›¨å…·ã€‚",
                "é™é›¨å¯èƒ½å½±å“å‡ºè¡Œï¼Œå»ºè®®æå‰è§„åˆ’è·¯çº¿ã€‚",
                "é›¨å¤©è·¯æ»‘ï¼Œè¯·æ³¨æ„äº¤é€šå®‰å…¨ã€‚"
            ],
            'drought': [
                "å¹²æ—±å¤©æ°”éœ€è¦ç‰¹åˆ«æ³¨æ„èŠ‚çº¦ç”¨æ°´ã€‚",
                "é«˜æ¸©å¹²æ—±å¤©æ°”å®¹æ˜“å¼•å‘ç«ç¾ï¼Œè¯·æ³¨æ„é˜²ç«ã€‚",
                "å»ºè®®å‡å°‘æˆ·å¤–æ´»åŠ¨ï¼Œé¿å…é•¿æ—¶é—´æš´éœ²åœ¨å¹²ç‡¥ç¯å¢ƒä¸­ã€‚"
            ],
            'wind': [
                "å¤§é£å¤©æ°”è¯·æ³¨æ„å®‰å…¨ï¼Œé¿å…åœ¨å¹¿å‘Šç‰Œã€ä¸´æ—¶æ­å»ºç‰©ä¸‹åœç•™ã€‚",
                "å»ºè®®å›ºå®šå¥½é—¨çª—å’Œå®¤å¤–ç‰©å“ï¼Œé˜²æ­¢è¢«é£å¹è½ã€‚"
            ],
            'general': [
                "æ ¹æ®çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼Œä¸ºæ‚¨æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š",
                "ç»“åˆæ°”è±¡ç‰¹å¾åˆ†æï¼Œå»ºè®®æ‚¨ï¼š",
                "ç»¼åˆæ¥çœ‹ï¼Œéœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š"
            ]
        }

    def generate(self, query: str, retrieved_docs: List[Dict],
                 weather_features: Dict) -> Dict:
        """ç”Ÿæˆå“åº”"""

        # åˆ†æå¤©æ°”ç‰¹å¾
        analysis_parts = []

        if 'temperature' in weather_features:
            temp = weather_features['temperature']
            if temp >= 35:
                analysis_parts.append(f"æ¸©åº¦é«˜è¾¾{temp}â„ƒï¼Œå±äºé…·çƒ­å¤©æ°”")
            elif temp >= 30:
                analysis_parts.append(f"æ¸©åº¦{temp}â„ƒï¼Œå±äºç‚çƒ­å¤©æ°”")
            elif temp >= 25:
                analysis_parts.append(f"æ¸©åº¦{temp}â„ƒï¼Œè¾ƒä¸ºæ¸©æš–")
            else:
                analysis_parts.append(f"æ¸©åº¦{temp}â„ƒï¼Œè¾ƒä¸ºå‡‰çˆ½")

        if 'humidity' in weather_features:
            humidity = weather_features['humidity']
            if humidity >= 80:
                analysis_parts.append(f"æ¹¿åº¦{humidity}%ï¼Œè¾ƒä¸ºæ½®æ¹¿")
            elif humidity <= 40:
                analysis_parts.append(f"æ¹¿åº¦{humidity}%ï¼Œè¾ƒä¸ºå¹²ç‡¥")
            else:
                analysis_parts.append(f"æ¹¿åº¦{humidity}%ï¼Œè¾ƒä¸ºèˆ’é€‚")

        # æå–æ£€ç´¢æ–‡æ¡£çš„å…³é”®ä¿¡æ¯
        doc_summaries = []
        for doc in retrieved_docs[:3]:  # å–å‰3ä¸ªæ–‡æ¡£
            doc_text = doc['document']
            similarity = doc['similarity']

            # æå–æ‘˜è¦ï¼ˆå–å‰100ä¸ªå­—ç¬¦ï¼‰
            summary = doc_text[:100] + "..." if len(doc_text) > 100 else doc_text
            doc_summaries.append({
                'summary': summary,
                'similarity': similarity
            })

        # ç”Ÿæˆå»ºè®®
        recommendations = []

        if weather_features.get('high_temperature'):
            recommendations.extend([
                "é¿å…åœ¨10:00-16:00é«˜æ¸©æ—¶æ®µè¿›è¡Œæˆ·å¤–æ´»åŠ¨",
                "ç©¿æˆ´å®½æ¾ã€é€æ°”çš„è¡£ç‰©ï¼Œä½©æˆ´å¤ªé˜³é•œå’Œé®é˜³å¸½",
                "åŠæ—¶è¡¥å……æ°´åˆ†ï¼Œä¸è¦ç­‰åˆ°å£æ¸´æ‰å–æ°´",
                "å¦‚å‡ºç°å¤´æ™•ã€æ¶å¿ƒç­‰ä¸­æš‘ç—‡çŠ¶ï¼Œç«‹å³åˆ°é˜´å‡‰å¤„ä¼‘æ¯"
            ])

        if weather_features.get('rain'):
            recommendations.extend([
                "å‡ºé—¨å‰æŸ¥çœ‹å¤©æ°”é¢„æŠ¥ï¼Œæºå¸¦é›¨å…·",
                "è¡Œè½¦æ—¶æ³¨æ„å‡é€Ÿæ…¢è¡Œï¼Œä¿æŒå®‰å…¨è½¦è·",
                "é¿å…åœ¨æ ‘ä¸‹ã€ç”µçº¿æ†ä¸‹é¿é›¨",
                "æ³¨æ„é˜²èŒƒé›·ç”µå¤©æ°”"
            ])

        if weather_features.get('drought'):
            recommendations.extend([
                "èŠ‚çº¦ç”¨æ°´ï¼Œå‡å°‘ä¸å¿…è¦çš„ç”¨æ°´",
                "æ³¨æ„é˜²ç«ï¼Œä¸è¦ä¹±æ‰”çƒŸå¤´",
                "é¿å…åœ¨æˆ·å¤–ä½¿ç”¨æ˜ç«",
                "åšå¥½çš®è‚¤ä¿æ¹¿ï¼Œé˜²æ­¢çš®è‚¤å¹²ç‡¥"
            ])

        # å¦‚æœæ²¡æœ‰ç‰¹å®šå»ºè®®ï¼Œæ·»åŠ é€šç”¨å»ºè®®
        if not recommendations:
            recommendations = [
                "å…³æ³¨å½“åœ°æ°”è±¡éƒ¨é—¨çš„æœ€æ–°é¢„æŠ¥å’Œé¢„è­¦",
                "æ ¹æ®å¤©æ°”å˜åŒ–åŠæ—¶è°ƒæ•´å‡ºè¡Œè®¡åˆ’",
                "åšå¥½ä¸ªäººé˜²æŠ¤ï¼Œä¿æŒå¥åº·ç”Ÿæ´»æ–¹å¼"
            ]

        # æ„å»ºå“åº”
        response = {
            'query': query,
            'timestamp': datetime.now().isoformat(),
            'weather_analysis': analysis_parts,
            'retrieved_docs_count': len(retrieved_docs),
            'doc_summaries': doc_summaries,
            'recommendations': recommendations,
            'confidence': min(0.9, retrieved_docs[0]['similarity'] if retrieved_docs else 0.5)
        }

        # ç”Ÿæˆè‡ªç„¶è¯­è¨€å“åº”
        response['natural_response'] = self._generate_natural_response(response)

        return response

    def _generate_natural_response(self, response: Dict) -> str:
        """ç”Ÿæˆè‡ªç„¶è¯­è¨€å“åº”"""
        lines = []

        lines.append(f"ğŸ“Š é’ˆå¯¹æ‚¨çš„æŸ¥è¯¢ã€Œ{response['query']}ã€ï¼Œåˆ†æå¦‚ä¸‹ï¼š")
        lines.append("")

        # å¤©æ°”åˆ†æ
        if response['weather_analysis']:
            lines.append("ğŸŒ¤ï¸ **å¤©æ°”ç‰¹å¾åˆ†æ**")
            for analysis in response['weather_analysis']:
                lines.append(f"â€¢ {analysis}")
            lines.append("")

        # æ£€ç´¢ç»“æœ
        lines.append(f"ğŸ” **çŸ¥è¯†åº“æ£€ç´¢ç»“æœ**ï¼ˆå…±{response['retrieved_docs_count']}æ¡ç›¸å…³æ–‡æ¡£ï¼‰")
        for i, doc in enumerate(response['doc_summaries'][:3], 1):
            lines.append(f"{i}. {doc['summary']} (ç›¸å…³åº¦: {doc['similarity']:.2f})")
        lines.append("")

        # å»ºè®®
        lines.append("ğŸ’¡ **å»ºè®®æªæ–½**")
        for i, rec in enumerate(response['recommendations'][:5], 1):
            lines.append(f"{i}. {rec}")

        lines.append("")
        lines.append("ğŸ“… åˆ†ææ—¶é—´ï¼š" + datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M"))

        return "\n".join(lines)


class SimpleRAGSystem:
    """ç®€åŒ–çš„RAGç³»ç»Ÿ"""

    def __init__(self, output_dir: str = None):
        # è¾“å‡ºç›®å½•
        if output_dir is None:
            base_dir = config.paths.get("finetune_output", "/home/Liyang/agent/finetune_output")
            self.output_dir = Path(base_dir) / "rag_system"
        else:
            self.output_dir = Path(output_dir)

        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()

        # åˆå§‹åŒ–ç»„ä»¶
        self.logger.info("åˆå§‹åŒ–RAGç³»ç»Ÿç»„ä»¶...")
        self.retriever = KnowledgeBaseRetriever()
        self.feature_extractor = WeatherFeatureExtractor()
        self.response_generator = ResponseGenerator()

        self.logger.info("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.output_dir / "rag_system.log"

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

        self.logger = logging.getLogger(__name__)

    def process_query(self, query: str, top_k: int = 5) -> Dict:
        """å¤„ç†æŸ¥è¯¢"""
        self.logger.info(f"å¤„ç†æŸ¥è¯¢: {query}")

        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            self.logger.info("æ­¥éª¤1: æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
            retrieved_docs = self.retriever.retrieve(query, top_k=top_k)
            self.logger.info(f"æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")

            # 2. æå–å¤©æ°”ç‰¹å¾
            self.logger.info("æ­¥éª¤2: æå–å¤©æ°”ç‰¹å¾...")
            weather_features = self.feature_extractor.extract(query)
            self.logger.info(f"æå–åˆ°å¤©æ°”ç‰¹å¾: {weather_features}")

            # 3. ç”Ÿæˆå“åº”
            self.logger.info("æ­¥éª¤3: ç”Ÿæˆå“åº”...")
            response = self.response_generator.generate(query, retrieved_docs, weather_features)

            # 4. ä¿å­˜ç»“æœ
            self.logger.info("æ­¥éª¤4: ä¿å­˜ç»“æœ...")
            self._save_result(response)

            return response

        except Exception as e:
            self.logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

            # è¿”å›é”™è¯¯å“åº”
            return {
                'query': query,
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
                'natural_response': f"æŠ±æ­‰ï¼Œå¤„ç†æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            }

    def _save_result(self, response: Dict):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = self.output_dir / f"result_{timestamp}.json"

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(response, f, indent=2, ensure_ascii=False)

        self.logger.info(f"ç»“æœä¿å­˜åˆ°: {result_file}")

    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("=" * 60)
        print("ğŸ¤– æ°”è±¡æ™ºèƒ½RAGç³»ç»Ÿ - äº¤äº’æ¨¡å¼")
        print("=" * 60)
        print("è¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        print()

        while True:
            try:
                query = input("è¯·è¾“å…¥æ‚¨çš„æ°”è±¡æŸ¥è¯¢: ").strip()

                if query.lower() == 'quit':
                    print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break

                if query.lower() == 'help':
                    self._show_help()
                    continue

                if not query:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„æŸ¥è¯¢")
                    continue

                # å¤„ç†æŸ¥è¯¢
                print("\n" + "=" * 40)
                print("ğŸ” æ­£åœ¨å¤„ç†æ‚¨çš„æŸ¥è¯¢...")
                response = self.process_query(query)

                # æ˜¾ç¤ºç»“æœ
                print("\n" + "=" * 60)
                print("ğŸ“‹ æŸ¥è¯¢ç»“æœ:")
                print("=" * 60)
                print(response['natural_response'])
                print("=" * 60)
                print()

            except KeyboardInterrupt:
                print("\n\nç¨‹åºè¢«ä¸­æ–­ï¼Œé€€å‡º...")
                break
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {e}")
                continue

    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©"""
        help_text = """
        æ°”è±¡æ™ºèƒ½RAGç³»ç»Ÿ - å¸®åŠ©
        
        æ‚¨å¯ä»¥è¯¢é—®ä»¥ä¸‹ç±»å‹çš„é—®é¢˜ï¼š
        
        1. æ¸©åº¦ç›¸å…³ï¼š
           - "ä»Šå¤©æ¸©åº¦35â„ƒä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ"
           - "é«˜æ¸©å¤©æ°”éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"
        
        2. é™æ°´ç›¸å…³ï¼š
           - "æ˜å¤©é™é›¨50mmçš„é¢„æµ‹"
           - "æš´é›¨å¤©æ°”å¦‚ä½•é˜²æŠ¤ï¼Ÿ"
        
        3. ç‰¹æ®Šå¤©æ°”ï¼š
           - "å°é£æ¥äº†æ€ä¹ˆåŠï¼Ÿ"
           - "å¹²æ—±å¤©æ°”å¦‚ä½•åº”å¯¹ï¼Ÿ"
           - "å¯’æ½®å¤©æ°”é˜²æŠ¤æªæ–½"
        
        4. ç»¼åˆæŸ¥è¯¢ï¼š
           - "æ¸©åº¦30â„ƒæ¹¿åº¦70%çš„å¤©æ°”æƒ…å†µ"
           - "æœªæ¥ä¸‰å¤©é«˜æ¸©å¹²æ—±é¢„è­¦"
        
        ç¤ºä¾‹æŸ¥è¯¢ï¼š
           - æ¸©åº¦35â„ƒæ¹¿åº¦40%çš„å¤©æ°”æƒ…å†µ
           - æ˜å¤©é™é›¨é¢„æµ‹
           - é«˜æ¸©çƒ­æµªé˜²æŠ¤æªæ–½
           - å°é£å¤©æ°”æ³¨æ„äº‹é¡¹
        
        å‘½ä»¤ï¼š
           - help: æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
           - quit: é€€å‡ºç³»ç»Ÿ
        
        ç³»ç»Ÿä¼šä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶ç»“åˆæ°”è±¡ç‰¹å¾ç»™å‡ºå»ºè®®ã€‚
        """
        print(help_text)

    def batch_process(self, queries_file: str):
        """æ‰¹é‡å¤„ç†æŸ¥è¯¢"""
        queries_path = Path(queries_file)

        if not queries_path.exists():
            self.logger.error(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {queries_file}")
            return

        # è¯»å–æŸ¥è¯¢
        with open(queries_path, 'r', encoding='utf-8') as f:
            queries = [line.strip() for line in f if line.strip()]

        self.logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(queries)} ä¸ªæŸ¥è¯¢")

        results = []
        for i, query in enumerate(queries, 1):
            self.logger.info(f"å¤„ç†æŸ¥è¯¢ {i}/{len(queries)}: {query}")

            try:
                response = self.process_query(query)
                results.append(response)
            except Exception as e:
                self.logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥: {query}, é”™è¯¯: {e}")
                results.append({
                    'query': query,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })

        # ä¿å­˜æ‰¹é‡ç»“æœ
        batch_file = self.output_dir / f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {batch_file}")

        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if 'error' not in r)
        failed = len(queries) - successful

        print(f"\næ‰¹é‡å¤„ç†ç»Ÿè®¡:")
        print(f"æ€»æŸ¥è¯¢æ•°: {len(queries)}")
        print(f"æˆåŠŸå¤„ç†: {successful}")
        print(f"å¤„ç†å¤±è´¥: {failed}")
        print(f"ç»“æœæ–‡ä»¶: {batch_file}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import argparse

    parser = argparse.ArgumentParser(description="æ°”è±¡æ™ºèƒ½RAGç³»ç»Ÿ")
    parser.add_argument('--query', type=str, help='ç›´æ¥å¤„ç†å•ä¸ªæŸ¥è¯¢')
    parser.add_argument('--batch', type=str, help='æ‰¹é‡å¤„ç†æŸ¥è¯¢æ–‡ä»¶')
    parser.add_argument('--output', type=str, help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = SimpleRAGSystem(output_dir=args.output)

    if args.query:
        # å¤„ç†å•ä¸ªæŸ¥è¯¢
        response = rag_system.process_query(args.query)
        print("\n" + "=" * 60)
        print("ğŸ“‹ æŸ¥è¯¢ç»“æœ:")
        print("=" * 60)
        print(response['natural_response'])
        print("=" * 60)

    elif args.batch:
        # æ‰¹é‡å¤„ç†
        rag_system.batch_process(args.batch)

    else:
        # äº¤äº’æ¨¡å¼
        rag_system.interactive_mode()


if __name__ == "__main__":
    main()