#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæ™ºèƒ½å¯¹è¯ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—
å®ç°åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ™ºèƒ½å¯¹è¯åŠŸèƒ½
"""

import os
import time
import logging
import json  # <--- æ–°å¢ï¼šç”¨äºè§£æJSONæ•°æ®
from typing import List, Dict, Any, Optional
from pathlib import Path

# ç¯å¢ƒå˜é‡åŠ è½½
from dotenv import load_dotenv

load_dotenv()

# LangChainç›¸å…³å¯¼å…¥
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document  # <--- æ–°å¢ï¼šç”¨äºæ„å»ºæ–‡æ¡£å¯¹è±¡
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun

# HTTPè¯·æ±‚
import requests

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DeepSeekLLM(LLM):
    """
    DeepSeekå¤§è¯­è¨€æ¨¡å‹æ¥å£å°è£…
    å®ç°LangChain LLMåŸºç±»ï¼Œæ”¯æŒAPIè°ƒç”¨å’Œé‡è¯•æœºåˆ¶
    """

    api_key: str
    base_url: str = "https://api.deepseek.com"
    model: str = "deepseek-chat"
    temperature: float = 0.7
    max_retries: int = 3

    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com",
                 model: str = "deepseek-chat", temperature: float = 0.7, max_retries: int = 3):
        super().__init__(api_key=api_key, base_url=base_url, model=model,
                         temperature=temperature, max_retries=max_retries)

    @property
    def _llm_type(self) -> str:
        return "deepseek"

    def _call(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> str:
        """
        è°ƒç”¨DeepSeek APIç”Ÿæˆå›ç­”
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": self.temperature,
            "max_tokens": 2000
        }

        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=30
                )

                if response.status_code == 200:
                    result = response.json()
                    return result['choices'][0]['message']['content']
                else:
                    logger.warning(f"APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å°è¯•æ¬¡æ•°: {attempt + 1}")

            except Exception as e:
                logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}, å°è¯•æ¬¡æ•°: {attempt + 1}")

            if attempt < self.max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

        raise Exception("APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")


class RAGSystem:
    """
    RAGæ™ºèƒ½å¯¹è¯ç³»ç»Ÿä¸»ç±»
    æ•´åˆæ–‡æ¡£å¤„ç†ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢å’Œç”ŸæˆåŠŸèƒ½
    """

    def __init__(self, auto_load_medical_data: bool = True):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ

        Args:
            auto_load_medical_data: æ˜¯å¦è‡ªåŠ¨åŠ è½½åŒ»ç–—æ•°æ®é›†ï¼ˆé»˜è®¤Trueï¼‰
        """
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.api_key = os.getenv('DEEPSEEK_API_KEY')
        self.base_url = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
        self.chunk_size = int(os.getenv('CHUNK_SIZE', 512))  # åŒ»ç–—æ–‡æœ¬å»ºè®®æ”¹å°ä¸€ç‚¹ï¼Œè¿™é‡Œè®¾ä¸º512
        self.chunk_overlap = int(os.getenv('CHUNK_OVERLAP', 64))
        self.max_retries = int(os.getenv('MAX_RETRIES', 3))

        if not self.api_key:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEY")

        # åˆå§‹åŒ–ç»„ä»¶
        self.llm = None
        self.embeddings = None
        self.vectorstore = None
        self.memory = None
        self.qa_chain = None
        self.documents = []  # å­˜å‚¨æ‰€æœ‰æ–‡æ¡£

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self._init_embeddings()

        # åˆå§‹åŒ–å¯¹è¯è®°å¿†
        self._init_memory()

        # è‡ªåŠ¨åŠ è½½åŒ»ç–—æ•°æ®é›†
        if auto_load_medical_data:
            self._auto_load_huatuo_dataset()

        logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _init_embeddings(self):
        """
        åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        ä¿®æ”¹ï¼šä½¿ç”¨ä¸­æ–‡æ•ˆæœæ›´å¥½çš„ BAAI/bge-small-zh-v1.5
        """
        try:
            logger.info("å°è¯•åˆå§‹åŒ–æœ¬åœ°HuggingFaceåµŒå…¥æ¨¡å‹ (BGE-Chinese)...")
            # ä½¿ç”¨åŒ—äº¬æ™ºæºçš„ BGE ä¸­æ–‡æ¨¡å‹ï¼Œé€‚åˆä¸­æ–‡åŒ»ç–—è¯­å¢ƒ
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-small-zh-v1.5",
                model_kwargs={
                    'device': 'cpu',  # å¦‚æœæœ‰GPUå¯æ”¹ä¸º 'cuda'
                    'trust_remote_code': False
                },
                cache_folder="./models",
                encode_kwargs={'normalize_embeddings': True}
            )
            logger.info("æœ¬åœ°ä¸­æ–‡åµŒå…¥æ¨¡å‹(BGE)åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"æœ¬åœ°åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            logger.info("åˆ‡æ¢åˆ°ç¦»çº¿TF-IDFåµŒå…¥æ¨¡å¼...")
            try:
                from offline_embeddings import TFIDFEmbeddings
                self.embeddings = TFIDFEmbeddings()
                logger.info("TF-IDFåµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except ImportError:
                logger.warning("ä½¿ç”¨ç®€å•æ–‡æœ¬åŒ¹é…æ¨¡å¼ï¼ˆåŠŸèƒ½å—é™ï¼‰")
                self.embeddings = None

    def _init_memory(self):
        """
        åˆå§‹åŒ–å¯¹è¯è®°å¿†
        """
        self.memory = ConversationBufferWindowMemory(
            k=5,
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        logger.info("å¯¹è¯è®°å¿†åˆå§‹åŒ–æˆåŠŸ")

    def _auto_load_huatuo_dataset(self):
        """
        è‡ªåŠ¨åŠ è½½Huatuo-26MåŒ»ç–—æ•°æ®é›†
        é»˜è®¤ä»data/medical.jsonåŠ è½½
        """
        medical_data_path = os.path.join(os.path.dirname(__file__), "data", "medical.json")

        if not os.path.exists(medical_data_path):
            logger.warning(f"åŒ»ç–—æ•°æ®é›†æœªæ‰¾åˆ°: {medical_data_path}")
            logger.info("å¦‚éœ€ä½¿ç”¨åŒ»ç–—çŸ¥è¯†åº“ï¼Œè¯·å°†æ•°æ®æ–‡ä»¶æ”¾ç½®åœ¨ data/medical.json")
            return

        logger.info("ğŸ¥ æ­£åœ¨åŠ è½½Huatuo-26MåŒ»ç–—æ•°æ®é›†...")

        # åŠ è½½åŒ»ç–—æ•°æ®
        medical_docs = self.load_medical_data(medical_data_path)

        if medical_docs:
            # æ„å»ºå‘é‡æ•°æ®åº“
            success = self.build_vectorstore(medical_docs)

            if success:
                logger.info(f"âœ… Huatuo-26MåŒ»ç–—æ•°æ®é›†åŠ è½½æˆåŠŸï¼åŒ…å« {len(medical_docs)} æ¡åŒ»ç–—çŸ¥è¯†")
                logger.info("ğŸ“š çŸ¥è¯†åº“å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹åŒ»ç–—é—®ç­”")
            else:
                logger.warning("åŒ»ç–—æ•°æ®é›†å‘é‡åŒ–å¤±è´¥")
        else:
            logger.warning("åŒ»ç–—æ•°æ®é›†åŠ è½½ä¸ºç©º")

    def load_medical_data(self, json_path: str) -> List[Document]:
        """
        æ–°å¢ï¼šåŠ è½½åŒ»ç–—JSONæ•°æ®é›†

        Args:
            json_path: JSONæ–‡ä»¶è·¯å¾„

        Returns:
            List[Document]: æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
        """
        if not os.path.exists(json_path):
            logger.warning(f"åŒ»ç–—æ•°æ®é›†ä¸å­˜åœ¨: {json_path}")
            return []

        logger.info(f"æ­£åœ¨åŠ è½½åŒ»ç–—æ•°æ®é›†: {json_path}")
        documents = []

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # å¤„ç†æ•°æ®ï¼Œè½¬æ¢ä¸º Document å¯¹è±¡
            # å‡è®¾æ•°æ®æ ¼å¼ä¸º [{"instruction": "...", "output": "..."}, ...]
            for i, item in enumerate(data):
                # å…¼å®¹ä¸åŒçš„å­—æ®µå
                q = item.get("instruction", "") or item.get("question", "")
                a = item.get("output", "") or item.get("answer", "")

                if q and a:
                    # æ ¸å¿ƒç­–ç•¥ï¼šå°†é—®é¢˜å’Œç­”æ¡ˆæ‹¼åœ¨ä¸€èµ·ä½œä¸ºçŸ¥è¯†å—
                    content = f"é—®é¢˜ï¼š{q}\nç­”æ¡ˆï¼š{a}"

                    # æ·»åŠ å…ƒæ•°æ®ï¼Œç”¨äºå¼•ç”¨å’Œæº¯æº
                    metadata = {
                        "source": "åŒ»ç–—çŸ¥è¯†åº“",
                        "id": i,
                        "original_question": q
                    }

                    documents.append(Document(page_content=content, metadata=metadata))

            logger.info(f"åŒ»ç–—æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(documents)} æ¡è®°å½•")
            return documents

        except Exception as e:
            logger.error(f"åŠ è½½åŒ»ç–—æ•°æ®å¤±è´¥: {str(e)}")
            return []

    def load_pdf_documents(self, pdf_paths: List[str]) -> List[Document]:
        """
        åŠ è½½PDFæ–‡æ¡£å¹¶è¿›è¡Œæ–‡æœ¬åˆ†å‰²
        """
        all_documents = []

        # ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼Œæ›´é€‚åˆé•¿æ–‡æœ¬
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ"]
        )

        for pdf_path in pdf_paths:
            try:
                if not os.path.exists(pdf_path):
                    logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                    continue

                loader = PyPDFLoader(pdf_path)
                documents = loader.load()

                split_docs = text_splitter.split_documents(documents)
                all_documents.extend(split_docs)

                logger.info(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {pdf_path}, åˆ†å‰²ä¸º {len(split_docs)} ä¸ªç‰‡æ®µ")

            except Exception as e:
                logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {pdf_path}: {str(e)}")

        logger.info(f"æ€»å…±åŠ è½½ {len(all_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return all_documents

    def build_vectorstore(self, documents: List[Document], save_path: str = None) -> bool:
        """
        æ„å»ºå‘é‡æ•°æ®åº“
        """
        try:
            if not documents:
                logger.warning("æ²¡æœ‰æ–‡æ¡£å¯ä»¥æ„å»ºå‘é‡æ•°æ®åº“")
                return False

            # æ£€æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            if self.embeddings is None:
                logger.warning("åµŒå…¥æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨TF-IDFæ–‡æ¡£å­˜å‚¨")
                from offline_embeddings import TFIDFEmbeddings
                self.tfidf_embeddings = TFIDFEmbeddings()
                self.documents = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]
                self.tfidf_embeddings.embed_documents(self.documents)
                logger.info(f"TF-IDFæ–‡æ¡£å­˜å‚¨æ„å»ºæˆåŠŸï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")
                return True

            # æ„å»ºFAISSå‘é‡æ•°æ®åº“
            self.vectorstore = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings
            )

            # ä¿å­˜å‘é‡æ•°æ®åº“
            if save_path:
                try:
                    self.vectorstore.save_local(save_path)
                    logger.info(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {save_path}")
                except Exception as save_error:
                    logger.warning(f"ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {str(save_error)}")

            logger.info("å‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {str(e)}")
            # å¤‡é€‰æ–¹æ¡ˆï¼šTF-IDF
            try:
                from offline_embeddings import TFIDFEmbeddings
                self.tfidf_embeddings = TFIDFEmbeddings()
                self.documents = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]
                self.tfidf_embeddings.embed_documents(self.documents)
                logger.info(f"å·²åˆ‡æ¢åˆ°TF-IDFæ–‡æ¡£å­˜å‚¨æ¨¡å¼ï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")
                return True
            except Exception as fallback_error:
                logger.error(f"å¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(fallback_error)}")
                return False

    def load_vectorstore(self, load_path: str) -> bool:
        """
        åŠ è½½å·²ä¿å­˜çš„å‘é‡æ•°æ®åº“
        """
        try:
            self.vectorstore = FAISS.load_local(
                load_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            logger.info(f"å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ: {load_path}")
            return True
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥: {str(e)}")
            return False

    def init_qa_chain(self, temperature: float = 0.7) -> bool:
        """
        åˆå§‹åŒ–é—®ç­”é“¾
        """
        try:
            self.llm = DeepSeekLLM(
                api_key=self.api_key,
                base_url=self.base_url,
                temperature=temperature,
                max_retries=self.max_retries
            )

            if self.vectorstore:
                logger.info("ä½¿ç”¨æ ‡å‡†é—®ç­”æ¨¡å¼ï¼ˆå‘é‡æ£€ç´¢ï¼‰")
                # åˆ›å»ºæ£€ç´¢å™¨ï¼Œkå€¼å¯ä»¥é€‚å½“è°ƒå¤§ä»¥è·å–æ›´å¤šåŒ»ç–—èƒŒæ™¯
                retriever = self.vectorstore.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 3}
                )

                self.qa_chain = ConversationalRetrievalChain.from_llm(
                    llm=self.llm,
                    retriever=retriever,
                    memory=self.memory,
                    return_source_documents=True,
                    verbose=True,
                    output_key="answer"
                )
                logger.info("æ ‡å‡†é—®ç­”é“¾åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.info("ä½¿ç”¨ç®€å•é—®ç­”æ¨¡å¼ï¼ˆæ— å‘é‡åº“ï¼‰")
                self.qa_chain = "simple_mode"

            return True

        except Exception as e:
            logger.error(f"é—®ç­”é“¾åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False

    def ask_question(self, question: str) -> Dict[str, Any]:
        """
        æé—®å¹¶è·å–å›ç­”ï¼ˆåŒ…å«Promptå·¥ç¨‹ä¼˜åŒ–ï¼‰
        """
        if not self.qa_chain:
            return {
                "answer": "ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆå§‹åŒ–é—®ç­”é“¾",
                "source_documents": [],
                "error": "System not initialized",
                "success": False
            }

        try:
            start_time = time.time()

            # å®šä¹‰åŒ»ç–—é¢†åŸŸçš„Promptæ¨¡æ¿
            medical_prompt_template = """ä½ æ˜¯ä¸€åä¸“ä¸šçš„åŒ»ç–—å¥åº·åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å·²çŸ¥çš„åŒ»ç–—çŸ¥è¯†åº“å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å·²çŸ¥åŒ»ç–—çŸ¥è¯†ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. è¯·æ ¹æ®ä¸Šè¿°â€œå·²çŸ¥åŒ»ç–—çŸ¥è¯†â€è¿›è¡Œå›ç­”ï¼Œä¸è¦ç¼–é€ äº‹å®ã€‚
2. è¯­è¨€è¦ä¸“ä¸šã€äº²åˆ‡ã€å®¢è§‚ã€‚
3. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥â€œæˆ‘çš„çŸ¥è¯†åº“ä¸­æš‚æ—¶æ²¡æœ‰å…³äºæ­¤é—®é¢˜çš„è®°å½•ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿâ€ï¼Œä¸è¦éšæ„çç¼–ã€‚
4. åœ¨å›ç­”ç»“å°¾ï¼Œå¦‚æœç¡®å®å¼•ç”¨äº†çŸ¥è¯†åº“ï¼Œè¯·æ ‡æ³¨â€œ[åŸºäºçŸ¥è¯†åº“å›ç­”]â€ã€‚

è¯·å¼€å§‹å›ç­”ï¼š"""

            # ------------------------------------------------------------------
            # åˆ†æ”¯ 1: ç®€å•æ¨¡å¼ (ä½†å¯èƒ½æŒæœ‰ vectorstore æˆ– tfidf)
            # ------------------------------------------------------------------
            if self.qa_chain == "simple_mode":
                relevant_docs = []
                mode_name = "simple"

                if self.vectorstore:
                    # æœ‰å‘é‡æ•°æ®åº“ï¼Œæ‰‹åŠ¨æ£€ç´¢
                    try:
                        retriever = self.vectorstore.as_retriever(
                            search_type="similarity",
                            search_kwargs={"k": 3}
                        )
                        relevant_docs = retriever.get_relevant_documents(question)
                    except Exception as e:
                        logger.warning(f"æ£€ç´¢å¤±è´¥: {str(e)}")

                elif hasattr(self, 'tfidf_embeddings') and self.tfidf_embeddings is not None and hasattr(self,
                                                                                                         'documents') and self.documents:
                    # TF-IDF æ¨¡å¼
                    docs_info = self.tfidf_embeddings.similarity_search(question, self.documents, k=3)
                    # å°† TF-IDF ç»“æœè½¬æ¢ä¸ºä¼ª Document å¯¹è±¡ä»¥ä¾¿ç»Ÿä¸€å¤„ç†
                    for info in docs_info:
                        relevant_docs.append(Document(
                            page_content=info['content'],
                            metadata={"score": info['score'], "index": info['index'], "source": "TF-IDFæ£€ç´¢"}
                        ))

                # æ„å»ºä¸Šä¸‹æ–‡å’ŒPrompt
                if relevant_docs:
                    context = "\n\n".join([doc.page_content for doc in relevant_docs])
                    prompt = medical_prompt_template.format(context=context, question=question)
                else:
                    # æ— ä»»ä½•æ–‡æ¡£æ—¶çš„ä¿åº•å›ç­”
                    prompt = f"ä½ æ˜¯ä¸€ååŒ»ç–—åŠ©æ‰‹ã€‚ç”¨æˆ·é—®ï¼š{question}ã€‚è¯·å›ç­”ï¼Œå¹¶æé†’ç”¨æˆ·ç”±äºç¼ºä¹çŸ¥è¯†åº“æ”¯æŒï¼Œå»ºè®®å’¨è¯¢åŒ»ç”Ÿã€‚"

                # è°ƒç”¨LLM
                answer = self.llm._call(prompt)

                # æ„å»ºè¿”å›ç»“æœ - è¿”å›å®Œæ•´å†…å®¹è€Œä¸æˆªæ–­
                source_info = [{
                    "content": doc.page_content,  # è¿”å›å®Œæ•´å†…å®¹
                    "metadata": doc.metadata
                } for doc in relevant_docs]

                return {
                    "answer": answer,
                    "source_documents": source_info,
                    "response_time": time.time() - start_time,
                    "success": True,
                    "mode": mode_name
                }

            # ------------------------------------------------------------------
            # åˆ†æ”¯ 2: æ ‡å‡†æ¨¡å¼ (ConversationalRetrievalChain)
            # ------------------------------------------------------------------
            else:
                # å³ä½¿æ˜¯ Chain æ¨¡å¼ï¼Œæˆ‘ä»¬ä¹Ÿæ‰‹åŠ¨æ§åˆ¶ Prompt æµç¨‹ä»¥ä¿è¯ Prompt æ•ˆæœ
                # å› ä¸ºé»˜è®¤çš„ Chain å†…éƒ¨ Prompt æ¯”è¾ƒéš¾æ”¹ï¼Œè¿™é‡Œé‡‡ç”¨æ‰‹åŠ¨æ£€ç´¢ + LLM çš„æ–¹å¼è¦†ç›–

                # 1. æ£€ç´¢
                retriever = self.vectorstore.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 3}
                )
                relevant_docs = retriever.get_relevant_documents(question)

                # 2. æ„å»º Prompt
                context = "\n\n".join([doc.page_content for doc in relevant_docs])
                prompt = medical_prompt_template.format(context=context, question=question)

                # 3. ç”Ÿæˆ
                answer = self.llm._call(prompt)

                # 4. æ ¼å¼åŒ–æ¥æº - è¿”å›å®Œæ•´å†…å®¹
                source_info = [{
                    "content": doc.page_content[:200] + "...",  # è¿”å›å®Œæ•´å†…å®¹
                    "metadata": doc.metadata
                } for doc in relevant_docs]

                return {
                    "answer": answer,
                    "source_documents": source_info,
                    "response_time": time.time() - start_time,
                    "success": True,
                    "mode": "standard"
                }

        except Exception as e:
            logger.error(f"é—®ç­”å¤„ç†å¤±è´¥: {str(e)}")
            return {
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "source_documents": [],
                "error": str(e),
                "success": False
            }

    def clear_memory(self):
        """
        æ¸…ç©ºå¯¹è¯è®°å¿†
        """
        if self.memory:
            self.memory.clear()
            logger.info("å¯¹è¯è®°å¿†å·²æ¸…ç©º")

    def get_memory_summary(self) -> str:
        """
        è·å–å¯¹è¯è®°å¿†æ‘˜è¦
        """
        if self.memory and hasattr(self.memory, 'chat_memory'):
            messages = self.memory.chat_memory.messages
            if messages and len(messages) > 0:
                return f"å½“å‰å¯¹è¯å†å²åŒ…å« {len(messages)} æ¡æ¶ˆæ¯"
        return "æš‚æ— å¯¹è¯å†å²"


if __name__ == "__main__":
    # ç®€å•çš„æœ¬åœ°æµ‹è¯•é€»è¾‘
    print("æ­£åœ¨æµ‹è¯• RAGSystem...")
    rag = RAGSystem()
    print("åˆå§‹åŒ–å®Œæˆã€‚è¯·åœ¨ main.py ä¸­è¿è¡Œå®Œæ•´åº”ç”¨ã€‚")
    rag.init_qa_chain()
    print(rag.ask_question("ç³–å°¿ç—…æ˜¯ä»€ä¹ˆ"))