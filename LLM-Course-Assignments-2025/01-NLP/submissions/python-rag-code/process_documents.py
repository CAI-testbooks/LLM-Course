#!/usr/bin/env python3
"""
å¤„ç†çº¯æ–‡æœ¬ç‰ˆPythonå®˜æ–¹æ–‡æ¡£ (python-3.14-docs-text)
å°†æ–‡æ¡£åˆ†å—å¹¶ä¿å­˜ä¸ºJSONæ ¼å¼ï¼Œä¾›RAGç³»ç»Ÿä½¿ç”¨
"""
import os
import json
import re
from langchain_text_splitters import RecursiveCharacterTextSplitter


def load_text_files(doc_path):
    """
    åŠ è½½çº¯æ–‡æœ¬æ–‡ä»¶
    Args:
        doc_path: æ–‡æœ¬æ–‡æ¡£ç›®å½•è·¯å¾„ï¼Œä¾‹å¦‚ "./data/python-3.14-docs-text"
    Returns:
        åŒ…å«æ‰€æœ‰æ–‡æ¡£å†…å®¹çš„åˆ—è¡¨
    """
    raw_texts = []
    file_count = 0

    print(f"å¼€å§‹ä»ç›®å½•åŠ è½½æ–‡æ¡£: {doc_path}")

    # éå†ç›®å½•ä¸‹çš„æ‰€æœ‰.txtæ–‡ä»¶
    for root, dirs, files in os.walk(doc_path):
        for file in files:
            if file.endswith('.txt'):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read().strip()

                        # è¿‡æ»¤æ‰ç©ºæ–‡ä»¶æˆ–æçŸ­çš„æ–‡ä»¶
                        if len(content) > 100:
                            # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
                            rel_path = os.path.relpath(file_path, doc_path)
                            formatted_content = f"ã€æ¥æº: {rel_path}ã€‘\n{content}"
                            raw_texts.append(formatted_content)
                            file_count += 1

                            # æ˜¾ç¤ºè¿›åº¦
                            if file_count % 100 == 0:
                                print(f"  å·²åŠ è½½ {file_count} ä¸ªæ–‡ä»¶...")
                except Exception as e:
                    print(f"  è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
                    continue

    print(f" æˆåŠŸä» {file_count} ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­åŠ è½½äº†å†…å®¹")
    return raw_texts


def clean_text_content(texts):
    """
    æ¸…ç†æ–‡æœ¬å†…å®¹
    Args:
        texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬åˆ—è¡¨
    """
    cleaned_texts = []


    for i, text in enumerate(texts):
        # 1. ç§»é™¤è¿‡é•¿çš„ç©ºç™½è¡Œï¼ˆä¿ç•™æ­£å¸¸çš„æ®µè½åˆ†éš”ï¼‰
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)

        # 2. ç§»é™¤ASCIIè‰ºæœ¯æˆ–è£…é¥°çº¿
        text = re.sub(r'^[-=*_]{10,}$', '', text, flags=re.MULTILINE)

        # 3. ç§»é™¤è¿‡çŸ­çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯ç›®å½•é¡¹æˆ–é¡µçœ‰é¡µè„šï¼‰
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()
            # ä¿ç•™æœ‰æ„ä¹‰çš„é•¿è¡Œæˆ–æ¥æºæ ‡è®°
            if len(line) > 30 or line.startswith('ã€æ¥æº:') or 'Copyright' in line:
                cleaned_lines.append(line)

        cleaned_text = '\n'.join(cleaned_lines)

        # 4. åªä¿ç•™è¶³å¤Ÿé•¿çš„æ–‡æ¡£
        if len(cleaned_text) > 200:
            cleaned_texts.append(cleaned_text)

        # æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 200 == 0:
            print(f"  å·²æ¸…ç† {i + 1}/{len(texts)} ä¸ªæ–‡æ¡£...")

    print(f"æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(cleaned_texts)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
    return cleaned_texts


def split_documents_optimized(texts):
    """
    ä¸ºPythonæ–‡æ¡£ä¼˜åŒ–çš„åˆ†å—ç­–ç•¥
    Args:
        texts: æ¸…ç†åçš„æ–‡æœ¬åˆ—è¡¨
    Returns:
        åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
    """

    # é’ˆå¯¹Pythonæ–‡æ¡£ä¼˜åŒ–çš„åˆ†å—å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,  # ç¨å¤§çš„å—ï¼Œå› ä¸ºPythonæ–‡æ¡£é€šå¸¸ç»“æ„æ¸…æ™°
        chunk_overlap=100,  # é€‚å½“çš„é‡å ä¿æŒä¸Šä¸‹æ–‡
        separators=[  # Pythonæ–‡æ¡£ç‰¹å®šçš„åˆ†éš”ç¬¦
            "\n\n\n",  # ä¸»è¦ç« èŠ‚åˆ†éš”
            "\n\n",  # æ®µè½åˆ†éš”
            "\nâ€¢ ",  # åˆ—è¡¨é¡¹
            "\n",  # æ¢è¡Œç¬¦
            " ",  # ç©ºæ ¼
            ""  # æœ€åçš„æ‰‹æ®µ
        ],
        length_function=len,
        keep_separator=True  # ä¿ç•™åˆ†éš”ç¬¦æœ‰åŠ©äºç†è§£ç»“æ„
    )

    all_chunks = []

    for i, text in enumerate(texts):
        try:
            chunks = text_splitter.split_text(text)
            all_chunks.extend(chunks)

            # æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"  å·²å¤„ç† {i + 1}/{len(texts)} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ {len(all_chunks)} ä¸ªå—...")
        except Exception as e:
            print(f"  è­¦å‘Š: å¤„ç†æ–‡æ¡£ {i} æ—¶åˆ†å—å¤±è´¥: {e}")
            # å¦‚æœåˆ†å—å¤±è´¥ï¼Œå°è¯•ç®€å•åˆ†å‰²
            simple_chunks = [text[j:j + 500] for j in range(0, len(text), 500)]
            all_chunks.extend(simple_chunks)

    print(f" åˆ†å—å®Œæˆï¼å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—")

    # æ˜¾ç¤ºä¸€äº›æ ·æœ¬
    print("\nğŸ“‹ æ–‡æœ¬å—æ ·æœ¬é¢„è§ˆ:")
    for i in range(min(3, len(all_chunks))):
        print(f"\n--- å— {i + 1} (å‰200å­—ç¬¦) ---")
        print(all_chunks[i][:200] + "...")

    return all_chunks


def save_chunks_to_json(chunks, output_path):
    """
    ä¿å­˜åˆ†å—ç»“æœåˆ°JSONæ–‡ä»¶
    Args:
        chunks: æ–‡æœ¬å—åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # ä¿å­˜ä¸ºJSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)

    print(f" æ–‡æœ¬å—å·²ä¿å­˜è‡³: {output_path}")

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_chars = sum(len(chunk) for chunk in chunks)
    avg_chunk_size = total_chars / len(chunks) if chunks else 0

    print(f" ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - æ–‡æœ¬å—æ•°é‡: {len(chunks)}")
    print(f"  - æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"  - å¹³å‡å—å¤§å°: {avg_chunk_size:.1f} å­—ç¬¦")
    print(f"  - æœ€å°å—: {min(len(c) for c in chunks) if chunks else 0} å­—ç¬¦")
    print(f"  - æœ€å¤§å—: {max(len(c) for c in chunks) if chunks else 0} å­—ç¬¦")


def main():
    """ä¸»å¤„ç†å‡½æ•°"""
    print("=" * 60)
    print("Python 3.14 æ–‡æ¡£å¤„ç†å·¥å…·")
    print("=" * 60)

    # 1. é…ç½®è·¯å¾„
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„æ–‡ä»¶å¤¹åä¸º python-3.14-docs-text
    # å¦‚æœæ–‡ä»¶å¤¹åä¸åŒï¼Œè¯·ä¿®æ”¹è¿™é‡Œ
    doc_path = "./data/python-3.14-docs-text"
    output_path = "./data/document_chunks.json"

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(doc_path):
        print(f" é”™è¯¯: æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {doc_path}")
        return

    print(f"æ–‡æ¡£ç›®å½•: {doc_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print("-" * 60)

    # 2. åŠ è½½æ–‡æ¡£
    raw_texts = load_text_files(doc_path)

    if not raw_texts:
        print(" é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶")
        return

    # 3. æ¸…ç†æ–‡æ¡£
    cleaned_texts = clean_text_content(raw_texts)

    # 4. åˆ†å—å¤„ç†
    chunks = split_documents_optimized(cleaned_texts)

    if not chunks:
        print(" é”™è¯¯: åˆ†å—åæ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æœ¬å—")
        return

    # 5. ä¿å­˜ç»“æœ
    save_chunks_to_json(chunks, output_path)


if __name__ == "__main__":
    main()