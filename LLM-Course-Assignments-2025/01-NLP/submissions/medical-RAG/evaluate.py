import os
import json
import time
import torch
import jieba
from rouge_chinese import Rouge
from datasets import load_dataset
from tqdm import tqdm
from openai import OpenAI

from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import PromptTemplate
import chromadb

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

# 1. DeepSeek API é…ç½®
DEEPSEEK_API_KEY = "sk-xxx"
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

# 2. å¾…è¯„ä¼°çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = "./Qwen/Qwen2.5-7B-Instruct"  # ç¬¬ä¸€æ¬¡è¿è¡ŒåŸºçº¿æ¨¡å‹
# MODEL_PATH = "./Qwen/Qwen-Medical-Merged"       # ç¬¬äºŒæ¬¡è¿è¡Œå¾®è°ƒåæ¨¡å‹

# 3. å…¶ä»–è·¯å¾„é…ç½®
CHROMA_PATH = "./chroma_db"
COLLECTION_NAME = "medical_rag"
EMBED_PATH = "./BAAI/bge-m3"

# 4. æµ‹è¯•é…ç½®
TEST_SIZE = 100  # æµ‹è¯•é›†æ ·æœ¬æ•°é‡

# ================= ğŸ› ï¸ å·¥å…·å‡½æ•° =================

def compute_rouge(pred, label):
    # è®¡ç®— ROUGE-L åˆ†æ•° (è¿‘ä¼¼å¼•ç”¨ F1)
    if not pred or not label:
        return 0.0
    rouge = Rouge()
    # ä¸­æ–‡åˆ†è¯
    pred_seg = ' '.join(jieba.cut(pred))
    label_seg = ' '.join(jieba.cut(label))
    try:
        scores = rouge.get_scores(pred_seg, label_seg)
        return scores[0]['rouge-l']['f']
    except Exception:
        return 0.0

def call_deepseek_judge(prompt, model="deepseek-chat"):
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æœºå™¨äººã€‚è¯·ä¸¥æ ¼éµå¾ªç”¨æˆ·çš„æŒ‡ä»¤è¿›è¡Œåˆ¤æ–­ã€‚æœ€ç»ˆåªè¾“å‡º 'YES' æˆ– 'NO'ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ ‡ç‚¹ç¬¦å·ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
            max_tokens=5
        )
        return response.choices[0].message.content.strip().upper()
    except Exception as e:
        print(f"âš ï¸ DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
        return "ERROR"

def evaluate_single_sample(query_engine, question, reference):
    # è¯„ä¼°å•æ¡æ•°æ®ï¼šRAGç”Ÿæˆ -> Rougeè®¡ç®— -> DeepSeekæ‰“åˆ†
    # 1. RAG ç”Ÿæˆå›ç­”
    response_obj = query_engine.query(question)
    pred_response = response_obj.response
    
    # è·å–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    retrieved_contexts = [n.get_content() for n in response_obj.source_nodes]
    context_text = "\n\n".join(retrieved_contexts)

    # 2. è®¡ç®— Rouge-L
    rouge_score = compute_rouge(pred_response, reference)

    # 3. DeepSeek è¯„ä¼°å‡†ç¡®ç‡ (Accuracy)
    # ç›®æ ‡ï¼šåˆ¤æ–­æ¨¡å‹å›ç­”æ˜¯å¦ç¬¦åˆæ ‡å‡†ç­”æ¡ˆçš„åŒ»å­¦äº‹å®
    acc_prompt = (
        f"ã€ä»»åŠ¡ã€‘ï¼šä½œä¸ºä¸€ååŒ»å­¦ä¸“å®¶ï¼Œè¯·åˆ¤æ–­ã€æ¨¡å‹å›ç­”ã€‘æ˜¯å¦ä¸ã€æ ‡å‡†ç­”æ¡ˆã€‘åœ¨åŒ»å­¦äº‹å®å’Œå»ºè®®ä¸Šä¸€è‡´ã€‚\n\n"
        f"ã€æ ‡å‡†ç­”æ¡ˆã€‘ï¼š\n{reference}\n\n"
        f"ã€æ¨¡å‹å›ç­”ã€‘ï¼š\n{pred_response}\n\n"
        f"ã€è¯„åˆ¤æ ‡å‡†ã€‘ï¼š\n"
        f"1. å¿½ç•¥è¯­æ°”ã€æ ¼å¼æˆ–è¯¦ç»†ç¨‹åº¦çš„å·®å¼‚ã€‚\n"
        f"2. å…³æ³¨æ ¸å¿ƒåŒ»å­¦äº‹å®ï¼ˆå¦‚ç—…å› ã€è¯ç‰©ã€æ²»ç–—å»ºè®®ï¼‰æ˜¯å¦ä¸€è‡´ã€‚\n"
        f"3. å¦‚æœæ¨¡å‹å›ç­”åŒ…å«æ ‡å‡†ç­”æ¡ˆä¸­æ²¡æœ‰çš„é¢å¤–æ­£ç¡®ä¿¡æ¯ï¼Œè§†ä¸ºæ­£ç¡®ã€‚\n"
        f"4. å¦‚æœæ¨¡å‹å›ç­”ä¸æ ‡å‡†ç­”æ¡ˆçš„å…³é”®äº‹å®å†²çªï¼Œè§†ä¸ºé”™è¯¯ã€‚\n\n"
        f"è¯·è¾“å‡ºåˆ¤æ–­ç»“æœï¼ˆåªè¾“å‡º 'YES' ä»£è¡¨æ­£ç¡®ï¼Œ'NO' ä»£è¡¨é”™è¯¯ï¼‰ï¼š"
    )
    acc_res = call_deepseek_judge(acc_prompt)
    is_accurate = 1 if "YES" in acc_res else 0

    # 4. DeepSeek è¯„ä¼°å¹»è§‰ (Hallucination / Faithfulness)
    # ç›®æ ‡ï¼šåˆ¤æ–­æ¨¡å‹æ˜¯å¦ç¼–é€ äº†å‚è€ƒæ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯
    hal_prompt = (
        f"ã€ä»»åŠ¡ã€‘ï¼šä½ æ˜¯ä¸€ä¸ªæå…¶ä¸¥æ ¼çš„äº‹å®æ ¸æŸ¥å‘˜ã€‚è¯·åˆ¤æ–­ã€æ¨¡å‹å›ç­”ã€‘ä¸­çš„ä¿¡æ¯æ˜¯å¦**å®Œå…¨æ”¯æŒ**äºæä¾›çš„ã€å‚è€ƒæ–‡æ¡£ã€‘ã€‚\n\n"
        f"ã€å‚è€ƒæ–‡æ¡£ç‰‡æ®µã€‘ï¼š\n{context_text}\n\n"
        f"ã€æ¨¡å‹å›ç­”ã€‘ï¼š\n{pred_response}\n\n"
        f"ã€è¯„åˆ¤æ ‡å‡†ã€‘ï¼š\n"
        f"1. è¿™æ˜¯ä¸€åœºå¼€å·è€ƒè¯•ï¼Œ**ç¦æ­¢**åˆ©ç”¨ä½ è‡ªå·±çš„åŒ»å­¦çŸ¥è¯†ã€‚\n"
        f"2. å¦‚æœæ¨¡å‹å›ç­”äº†æ–‡æ¡£ä¸­æ²¡æœ‰æåˆ°çš„ä¿¡æ¯ï¼ˆå³ä½¿è¯¥ä¿¡æ¯åœ¨ç°å®ä¸–ç•Œä¸­æ˜¯æ­£ç¡®çš„ï¼‰ï¼Œä¹Ÿè¢«è§†ä¸ºå¹»è§‰ï¼ˆHallucinationï¼‰ã€‚\n"
        f"3. å¦‚æœæ¨¡å‹å›ç­”è¯´â€œæ–‡æ¡£ä¸­æœªæåŠâ€æˆ–â€œä¸çŸ¥é“â€ï¼Œè¿™ä¸å±äºå¹»è§‰ï¼Œå±äºè¯šå®å›ç­”ã€‚\n\n"
        f"è¯·åˆ¤æ–­ï¼šæ¨¡å‹å›ç­”æ˜¯å¦åŒ…å«å‚è€ƒæ–‡æ¡£ä¸æ”¯æŒçš„ä¿¡æ¯ï¼Ÿ\n"
        f"å¦‚æœæœ‰ä¸æ”¯æŒçš„ä¿¡æ¯ï¼ˆå³å­˜åœ¨å¹»è§‰ï¼‰ï¼Œè¾“å‡º 'YES'ï¼›\n"
        f"å¦‚æœæ‰€æœ‰ä¿¡æ¯éƒ½èƒ½åœ¨æ–‡æ¡£æ‰¾åˆ°ä¾æ®ï¼ˆå³æ²¡æœ‰å¹»è§‰ï¼‰ï¼Œè¾“å‡º 'NO'ã€‚"
    )
    hal_res = call_deepseek_judge(hal_prompt)
    is_hallucinated = 1 if "YES" in hal_res else 0

    return {
        "question": question,
        "reference": reference,
        "prediction": pred_response,
        "rouge_l": rouge_score,
        "accurate": is_accurate,
        "hallucinated": is_hallucinated,
        "contexts": context_text[:200] + "..."
    }

# ================= ğŸš€ ä¸»ç¨‹åº =================

def main():
    print(f"ğŸš€ å¼€å§‹è¯„ä¼°æµç¨‹...")
    print(f"ğŸ“Š å½“å‰è¯„ä¼°æ¨¡å‹: {MODEL_PATH}")
    print(f"âš–ï¸  è£åˆ¤æ¨¡å‹: DeepSeek-V3 (API)")

    # 1. åŠ è½½æœ¬åœ° Embedding
    print("Loading Embedding Model...")
    embed_model = HuggingFaceEmbedding(model_name=EMBED_PATH, device="cuda", trust_remote_code=True)
    Settings.embedding_model = embed_model
    
    # 2. åŠ è½½æœ¬åœ° LLM (å¾…è¯„ä¼°çš„å¯¹è±¡)
    print("Loading Local LLM...")
    llm = HuggingFaceLLM(
        context_window=4096,
        max_new_tokens=512,
        tokenizer_name=MODEL_PATH,
        model_name=MODEL_PATH,
        device_map="auto",
        model_kwargs={"torch_dtype": torch.float16, "load_in_4bit": True},
        generate_kwargs={"temperature": 0.1, "do_sample": False}
    )
    Settings.llm = llm

    # 3. è¿æ¥å‘é‡åº“
    print("Connecting to ChromaDB...")
    db = chromadb.PersistentClient(path=CHROMA_PATH)
    chroma_collection = db.get_or_create_collection(COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)
    
    # æ„å»ºæŸ¥è¯¢å¼•æ“
    text_qa_template_str = (
        "ä»¥ä¸‹æ˜¯å‚è€ƒæ–‡æ¡£ä¿¡æ¯ï¼š\n"
        "---------------------\n"
        "{context_str}\n"
        "---------------------\n"
        "è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ã€‚ç­”æ¡ˆåªèƒ½æ ¹æ®å‚è€ƒæ–‡æ¡£å†…å®¹ç”Ÿæˆï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´æ˜â€œæ–‡æ¡£ä¸­æœªæåŠâ€ï¼Œä¸è¦ç¼–é€ \n"
        "é—®é¢˜ï¼š{query_str}\n"
        "å›ç­”ï¼š"
    )
    text_qa_template = PromptTemplate(text_qa_template_str)

    # åº”ç”¨åˆ° query_engine
    query_engine = index.as_query_engine(
        text_qa_template=text_qa_template,
        similarity_top_k=3
    )

    # 4. å‡†å¤‡æµ‹è¯•é›†
    print("Loading Dataset...")
    dataset = load_dataset("./Huatuo26M-Lite", split="train")
    # éšæœºæ‰“ä¹±å¹¶å–å‰ N ä¸ª
    test_set = dataset.shuffle(seed=2024).select(range(TEST_SIZE))
    
    results = []
    
    # 5. å¾ªç¯è¯„ä¼°
    print(f"ğŸ å¼€å§‹è¯„æµ‹ {TEST_SIZE} æ¡æ ·æœ¬ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    for i, item in enumerate(tqdm(test_set)):
        try:
            res = evaluate_single_sample(query_engine, item['question'], item['answer'])
            results.append(res)
            time.sleep(0.5) 
        except Exception as e:
            print(f"âŒ æ ·æœ¬ {i} è¯„ä¼°å‡ºé”™: {e}")
            continue

    # 6. ç»Ÿè®¡ç»“æœ
    if not results:
        print("âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆç»“æœã€‚")
        return

    avg_rouge = sum(r['rouge_l'] for r in results) / len(results)
    avg_acc = sum(r['accurate'] for r in results) / len(results)
    avg_hal = sum(r['hallucinated'] for r in results) / len(results)
    
    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*40)
    print(f"ğŸ“ æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š - {os.path.basename(MODEL_PATH)}")
    print("="*40)
    print(f"âœ… å‡†ç¡®ç‡ (Accuracy):        {avg_acc:.2%}")
    print(f"ğŸ“š å¼•ç”¨ä¸€è‡´æ€§ (Rouge-L):    {avg_rouge:.4f}")
    print(f"ğŸ‘» å¹»è§‰ç‡ (Hallucination):  {avg_hal:.2%}")
    print("="*40)
    
    # ä¿å­˜ JSON æ–‡ä»¶
    output_filename = f"eval_report_{os.path.basename(MODEL_PATH)}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜è‡³: {output_filename}")

if __name__ == "__main__":
    main()