import os
import json
import time
import torch
import jieba
from rouge_chinese import Rouge
from datasets import load_dataset
from tqdm import tqdm
from openai import OpenAI

from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
import chromadb

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

# 1. DeepSeek API é…ç½®
DEEPSEEK_API_KEY = "sk-79bbee98a7bf4215a5e27a993f1f0a23"
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

# 2. å¾…è¯„ä¼°çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
# MODEL_PATH = "./Qwen/Qwen2.5-7B-Instruct"  # ç¬¬ä¸€æ¬¡è¿è¡ŒåŸºçº¿æ¨¡å‹
MODEL_PATH = "./Qwen/Qwen-Medical-Merged"       # ç¬¬äºŒæ¬¡è¿è¡Œå¾®è°ƒåæ¨¡å‹

# 3. å…¶ä»–è·¯å¾„é…ç½®
CHROMA_PATH = "./chroma_db"
COLLECTION_NAME = "medical_rag"
EMBED_PATH = "./BAAI/bge-m3"

# 4. æµ‹è¯•é…ç½®
TEST_SIZE = 100  # æµ‹è¯•é›†æ ·æœ¬æ•°é‡

# ================= ğŸ› ï¸ å·¥å…·å‡½æ•° =================

def compute_rouge(pred, label):
    # è®¡ç®— ROUGE-L åˆ†æ•° (è¿‘ä¼¼å¼•ç”¨ F1)
    if not pred or not label:
        return 0.0
    rouge = Rouge()
    # ä¸­æ–‡åˆ†è¯
    pred_seg = ' '.join(jieba.cut(pred))
    label_seg = ' '.join(jieba.cut(label))
    try:
        scores = rouge.get_scores(pred_seg, label_seg)
        return scores[0]['rouge-l']['f']
    except Exception:
        return 0.0

def call_deepseek_judge(prompt, model="deepseek-chat"):
    # è°ƒç”¨ DeepSeek API è¿›è¡Œåˆ¤åˆ«
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå…¬æ­£ã€ä¸¥è°¨çš„åŒ»ç–—é—®ç­”è¯„åˆ¤ä¸“å®¶ã€‚è¯·åªè¾“å‡º YES æˆ– NOï¼Œä¸è¦åŒ…å«å…¶ä»–åºŸè¯ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
            max_tokens=10
        )
        return response.choices[0].message.content.strip().upper()
    except Exception as e:
        print(f"âš ï¸ DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
        return "ERROR"

def evaluate_single_sample(query_engine, question, reference):
    # è¯„ä¼°å•æ¡æ•°æ®ï¼šRAGç”Ÿæˆ -> Rougeè®¡ç®— -> DeepSeekæ‰“åˆ†
    # 1. RAG ç”Ÿæˆå›ç­”
    response_obj = query_engine.query(question)
    pred_response = response_obj.response
    
    # è·å–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    retrieved_contexts = [n.get_content() for n in response_obj.source_nodes]
    context_text = "\n\n".join(retrieved_contexts)[:3000]

    # 2. è®¡ç®— Rouge-L
    rouge_score = compute_rouge(pred_response, reference)

    # 3. DeepSeek è¯„ä¼°å‡†ç¡®ç‡
    # åˆ¤æ–­ç”Ÿæˆçš„å›ç­”æ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆæ„æ€ä¸€è‡´
    acc_prompt = (
        f"ã€ä»»åŠ¡ã€‘ï¼šåˆ¤æ–­æ¨¡å‹å›ç­”æ˜¯å¦åœ¨åŒ»å­¦äº‹å®å±‚é¢ä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´ã€‚\n"
        f"ã€é—®é¢˜ã€‘ï¼š{question}\n"
        f"ã€æ ‡å‡†ç­”æ¡ˆã€‘ï¼š{reference}\n"
        f"ã€æ¨¡å‹å›ç­”ã€‘ï¼š{pred_response}\n\n"
        f"è¯·åˆ¤æ–­ï¼šæ¨¡å‹å›ç­”æ˜¯å¦æ­£ç¡®ï¼Ÿå¦‚æœæ˜¯ï¼Œè¾“å‡º 'YES'ï¼›å¦‚æœé”™è¯¯æˆ–ç­”éæ‰€é—®ï¼Œè¾“å‡º 'NO'ã€‚"
    )
    acc_res = call_deepseek_judge(acc_prompt)
    is_accurate = 1 if "YES" in acc_res else 0

    # 4. DeepSeek è¯„ä¼°å¹»è§‰
    # åˆ¤æ–­å›ç­”æ˜¯å¦å®Œå…¨åŸºäºå‚è€ƒæ–‡æ¡£ (Faithfulness)
    hal_prompt = (
        f"ã€ä»»åŠ¡ã€‘ï¼šåˆ¤æ–­æ¨¡å‹å›ç­”æ˜¯å¦åŒ…å«å‚è€ƒæ–‡æ¡£ä¸­æœªæåŠçš„ä¿¡æ¯ï¼ˆå³å¹»è§‰ï¼‰ã€‚\n"
        f"ã€å‚è€ƒæ–‡æ¡£ã€‘ï¼š\n{context_text}\n\n"
        f"ã€æ¨¡å‹å›ç­”ã€‘ï¼š{pred_response}\n\n"
        f"è¯·åˆ¤æ–­ï¼šæ¨¡å‹å›ç­”ä¸­çš„å…³é”®ä¿¡æ¯æ˜¯å¦éƒ½èƒ½åœ¨å‚è€ƒæ–‡æ¡£ä¸­æ‰¾åˆ°æ”¯æŒï¼Ÿ\n"
        f"å¦‚æœåŒ…å«æ–‡æ¡£é‡Œæ²¡è¯´çš„é¢å¤–ä¿¡æ¯ï¼ˆå¯èƒ½æ˜¯å¹»è§‰ï¼‰ï¼Œè¾“å‡º 'YES'ï¼›\n"
        f"å¦‚æœå®Œå…¨åŸºäºæ–‡æ¡£å›ç­”ï¼Œè¾“å‡º 'NO'ã€‚"
    )
    hal_res = call_deepseek_judge(hal_prompt)
    # è¿™é‡Œ YES ä»£è¡¨æœ‰å¹»è§‰ï¼ŒNO ä»£è¡¨æ— å¹»è§‰
    is_hallucinated = 1 if "YES" in hal_res else 0

    return {
        "question": question,
        "reference": reference,
        "prediction": pred_response,
        "rouge_l": rouge_score,
        "accurate": is_accurate,
        "hallucinated": is_hallucinated,
        "contexts": context_text[:200] + "..."
    }

# ================= ğŸš€ ä¸»ç¨‹åº =================

def main():
    print(f"ğŸš€ å¼€å§‹è¯„ä¼°æµç¨‹...")
    print(f"ğŸ“Š å½“å‰è¯„ä¼°æ¨¡å‹: {MODEL_PATH}")
    print(f"âš–ï¸  è£åˆ¤æ¨¡å‹: DeepSeek-V3 (API)")

    # 1. åŠ è½½æœ¬åœ° Embedding
    print("Loading Embedding Model...")
    embed_model = HuggingFaceEmbedding(model_name=EMBED_PATH, device="cuda", trust_remote_code=True)
    Settings.embedding_model = embed_model
    
    # 2. åŠ è½½æœ¬åœ° LLM (å¾…è¯„ä¼°çš„å¯¹è±¡)
    print("Loading Local LLM...")
    llm = HuggingFaceLLM(
        context_window=4096,
        max_new_tokens=512,
        tokenizer_name=MODEL_PATH,
        model_name=MODEL_PATH,
        device_map="auto",
        model_kwargs={"torch_dtype": torch.float16, "load_in_4bit": True},
        generate_kwargs={"temperature": 0.1, "do_sample": False}
    )
    Settings.llm = llm

    # 3. è¿æ¥å‘é‡åº“
    print("Connecting to ChromaDB...")
    db = chromadb.PersistentClient(path=CHROMA_PATH)
    chroma_collection = db.get_or_create_collection(COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)
    
    # æ„å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(similarity_top_k=3)

    # 4. å‡†å¤‡æµ‹è¯•é›†
    print("Loading Dataset...")
    dataset = load_dataset("./Huatuo26M-Lite", split="train")
    # éšæœºæ‰“ä¹±å¹¶å–å‰ N ä¸ª
    test_set = dataset.shuffle(seed=2024).select(range(TEST_SIZE))
    
    results = []
    
    # 5. å¾ªç¯è¯„ä¼°
    print(f"ğŸ å¼€å§‹è¯„æµ‹ {TEST_SIZE} æ¡æ ·æœ¬ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    for i, item in enumerate(tqdm(test_set)):
        try:
            res = evaluate_single_sample(query_engine, item['question'], item['answer'])
            results.append(res)
            time.sleep(0.5) 
        except Exception as e:
            print(f"âŒ æ ·æœ¬ {i} è¯„ä¼°å‡ºé”™: {e}")
            continue

    # 6. ç»Ÿè®¡ç»“æœ
    if not results:
        print("âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆç»“æœã€‚")
        return

    avg_rouge = sum(r['rouge_l'] for r in results) / len(results)
    avg_acc = sum(r['accurate'] for r in results) / len(results)
    avg_hal = sum(r['hallucinated'] for r in results) / len(results)
    
    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*40)
    print(f"ğŸ“ æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š - {os.path.basename(MODEL_PATH)}")
    print("="*40)
    print(f"âœ… å‡†ç¡®ç‡ (Accuracy):        {avg_acc:.2%}")
    print(f"ğŸ“š å¼•ç”¨ä¸€è‡´æ€§ (Rouge-L):    {avg_rouge:.4f}")
    print(f"ğŸ‘» å¹»è§‰ç‡ (Hallucination):  {avg_hal:.2%}")
    print("="*40)
    
    # ä¿å­˜ JSON æ–‡ä»¶
    output_filename = f"eval_report_{os.path.basename(MODEL_PATH)}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜è‡³: {output_filename}")

if __name__ == "__main__":
    main()