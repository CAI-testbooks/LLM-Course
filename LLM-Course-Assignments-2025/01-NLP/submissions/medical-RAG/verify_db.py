import os
import chromadb
import torch
from llama_index.core import VectorStoreIndex
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import StorageContext

# --- é…ç½®å‚æ•°---
CHROMA_PATH = "./chroma_db"
COLLECTION_NAME = "medical_rag"
LOCAL_MODEL_PATH = "./BAAI/bge-m3"

def check_chroma_direct():
    print("-" * 50)
    print("ğŸ” [é˜¶æ®µä¸€] åº•å±‚æ•°æ®æ£€æŸ¥ (Direct Inspection)")
    print("-" * 50)
    
    if not os.path.exists(CHROMA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç›®å½• {CHROMA_PATH}ï¼Œè¯·å…ˆè¿è¡Œæ„å»ºè„šæœ¬ï¼")
        return False

    try:
        # è¿æ¥æ•°æ®åº“
        db = chromadb.PersistentClient(path=CHROMA_PATH)
        collection = db.get_collection(COLLECTION_NAME)
        
        # 1. æ£€æŸ¥æ•°æ®é‡
        count = collection.count()
        print(f"ğŸ“Š æ•°æ®åº“å½“å‰å­˜å‚¨æ¡ç›®æ•°: {count}")
        
        if count == 0:
            print("âš ï¸ è­¦å‘Šï¼šæ•°æ®åº“æ˜¯ç©ºçš„ï¼æ„å»ºè¿‡ç¨‹å¯èƒ½å‡ºé”™ã€‚")
            return False
            
        # 2. æŠ½æŸ¥ç¬¬ä¸€æ¡æ•°æ® (Peek)
        # æ£€æŸ¥æ˜¯å¦åŒ…å« text å’Œ metadata
        data = collection.peek(limit=1)
        if data and data['documents']:
            print(f"\nğŸ“ [æ•°æ®æŠ½æ ·]:")
            print(f"   ID: {data['ids'][0]}")
            print(f"   Metadatas: {data['metadatas'][0]}")
            print(f"   Text (å‰100å­—): {data['documents'][0][:100]}...")
        else:
            print("âš ï¸ è­¦å‘Šï¼šæ— æ³•è¯»å–æ•°æ®å†…å®¹ã€‚")
            
        print("âœ… åº•å±‚æ•°æ®ç»“æ„æ­£å¸¸ã€‚")
        return True
        
    except Exception as e:
        print(f"âŒ Chroma è¯»å–å¤±è´¥: {e}")
        return False

def check_semantic_retrieval():
    print("\n" + "-" * 50)
    print("ğŸ§  [é˜¶æ®µäºŒ] è¯­ä¹‰æ£€ç´¢æµ‹è¯• (Semantic Test)")
    print("-" * 50)

    print(f"â³ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹ {LOCAL_MODEL_PATH} (ç”¨äºå°†æµ‹è¯•é—®é¢˜å‘é‡åŒ–)...")
    try:
        embed_model = HuggingFaceEmbedding(
            model_name=LOCAL_MODEL_PATH,
            device="cuda" if torch.cuda.is_available() else "cpu",
            trust_remote_code=True
        )
        
        # è¿æ¥ LlamaIndex
        db = chromadb.PersistentClient(path=CHROMA_PATH)
        chroma_collection = db.get_collection(COLLECTION_NAME)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        
        # åŠ è½½ç´¢å¼•
        index = VectorStoreIndex.from_vector_store(
            vector_store,
            embed_model=embed_model
        )
        
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = index.as_retriever(similarity_top_k=3)
        
        # æµ‹è¯•é—®é¢˜ (é’ˆå¯¹ Huatuo æ•°æ®çš„å¸¸è§é—®é¢˜)
        test_query = "æ„Ÿå†’äº†å¤´ç—›æ€ä¹ˆåŠï¼Ÿ"
        print(f"\nâ“ æµ‹è¯•æé—®: '{test_query}'")
        
        results = retriever.retrieve(test_query)
        
        if not results:
            print("âŒ æ£€ç´¢å¤±è´¥ï¼šæœªè¿”å›ä»»ä½•ç»“æœã€‚")
            return

        print(f"ğŸ‰ æ£€ç´¢æˆåŠŸï¼æ‰¾åˆ°äº† {len(results)} æ¡ç›¸å…³ç»“æœï¼š\n")
        
        for i, node in enumerate(results):
            print(f"--- [ç»“æœ {i+1}] (ç›¸ä¼¼åº¦å¾—åˆ†: {node.score:.4f}) ---")
            # æ‰“å°å†…å®¹é¢„è§ˆ
            content_preview = node.node.get_content().replace('\n', ' ')[:150]
            print(f"ğŸ“„ å†…å®¹: {content_preview}...")
            # æ‰“å°å…ƒæ•°æ®
            print(f"ğŸ”— æ¥æº: {node.metadata.get('source', 'Unknown')}")
            print("")

    except Exception as e:
        print(f"âŒ è¯­ä¹‰æ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    if check_chroma_direct():
        check_semantic_retrieval()