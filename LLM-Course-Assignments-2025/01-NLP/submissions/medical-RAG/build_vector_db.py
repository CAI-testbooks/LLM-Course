import os
import shutil
from datasets import load_dataset 
from tqdm import tqdm
from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# --- 1. é…ç½®å‚æ•° ---
LOCAL_DATASET_PATH = "./Huatuo26M-Lite" 

CHROMA_PATH = "./chroma_db"
COLLECTION_NAME = "medical_rag"

# è®¾ç½®ä¸º Noneï¼Œè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ•°æ®
MAX_DOCS = None  
BATCH_SIZE = 100 

# æœ¬åœ°æ¨¡å‹è·¯å¾„
LOCAL_MODEL_PATH = "./BAAI/bge-m3"

# --- 2. åˆå§‹åŒ–æ¨¡å‹ ---
print(f"æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹: {LOCAL_MODEL_PATH}...")
embed_model = HuggingFaceEmbedding(
    model_name=LOCAL_MODEL_PATH, 
    device="cuda", 
    trust_remote_code=True
)
Settings.embedding_model = embed_model

# --- 3. å‡†å¤‡å‘é‡æ•°æ®åº“ ---
print(f"æ­£åœ¨åˆå§‹åŒ– ChromaDB: {CHROMA_PATH}")
# æ¸…ç†æ—§æ•°æ®ï¼Œé¿å…é‡å¤å †å 
if os.path.exists(CHROMA_PATH):
    shutil.rmtree(CHROMA_PATH) 

db = chromadb.PersistentClient(path=CHROMA_PATH)
chroma_collection = db.get_or_create_collection(COLLECTION_NAME)
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

def stream_documents():
    print(f"ğŸ“‚ æ­£åœ¨ä»æœ¬åœ°æ–‡ä»¶å¤¹åŠ è½½æ•°æ®é›†: {LOCAL_DATASET_PATH}...")
    
    if not os.path.exists(LOCAL_DATASET_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è·¯å¾„ {LOCAL_DATASET_PATH}")
        print("è¯·å…ˆåœ¨ç»ˆç«¯è¿è¡Œ git clone å‘½ä»¤ä¸‹è½½æ•°æ®é›†ï¼")
        return

    try:
        dataset = load_dataset(LOCAL_DATASET_PATH, split="train")
        print(f"ğŸ“Š æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(dataset)} æ¡æ•°æ®")
    except Exception as e:
        print(f"åŠ è½½å¤±è´¥: {e}")
        return

    current_batch = []
    count = 0

    # éå†æ•´ä¸ªæ•°æ®é›†
    for i, item in tqdm(enumerate(dataset), desc="Processing", total=len(dataset)):
        if not item.get('question') or not item.get('answer'):
            continue

        text_chunk = f"é—®é¢˜ï¼š{item['question']}\n\nå›ç­”ï¼š{item['answer']}"
        
        doc = Document(
            text=text_chunk,
            metadata={
                "source": "Huatuo-26M-Lite",
                "original_question": item['question']
            },
            excluded_llm_metadata_keys=['source', 'original_question']
        )
        
        current_batch.append(doc)
        
        # æ‰¹å¤„ç†
        if len(current_batch) >= BATCH_SIZE:
            yield current_batch
            current_batch = []
            
        count += 1
        if MAX_DOCS is not None and count >= MAX_DOCS:
            break
    
    # å¤„ç†å‰©ä½™çš„æ–‡æ¡£
    if current_batch:
        yield current_batch

# --- 4. æ‰§è¡Œæ„å»º ---
def build():
    # æ˜¾å¼ä¼ é€’ embed_model
    index = VectorStoreIndex.from_vector_store(
        vector_store, 
        storage_context=storage_context,
        embed_model=embed_model
    )
    
    total_chunks = 0
    for batch_docs in stream_documents():
        # ä½¿ç”¨ insert_nodes æ‰¹é‡æ’å…¥
        index.insert_nodes(batch_docs)
        total_chunks += len(batch_docs)
        # æ‰“å°è¿›åº¦
        if total_chunks % 1000 == 0:
            print(f" --> å·²å…¥åº“ {total_chunks} æ¡æ•°æ®")

    print(f"\nâœ… å…¨é‡æ„å»ºå®Œæˆï¼å…±è®¡ {total_chunks} ä¸ªå‘é‡å—å·²å­˜å…¥ {CHROMA_PATH}")

if __name__ == "__main__":
    build()