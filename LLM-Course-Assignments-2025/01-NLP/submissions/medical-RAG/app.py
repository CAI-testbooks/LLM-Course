import streamlit as st
import torch
import os
from llama_index.core import VectorStoreIndex, Settings, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
import chromadb

# ================= é…ç½®åŒºåŸŸ =================
# é¡µé¢é…ç½®
st.set_page_config(page_title="åŒ»ç–— RAG åŠ©æ‰‹", page_icon="ğŸ¥", layout="wide")

# æœ¬åœ°è·¯å¾„
CHROMA_PATH = "./chroma_db"
COLLECTION_NAME = "medical_rag"
EMBED_MODEL_PATH = "./BAAI/bge-m3"
LLM_MODEL_PATH = "./Qwen/Qwen-Medical-Merged"

# ================= æ¨¡å‹åŠ è½½=================
@st.cache_resource
def load_rag_engine():
    status = st.empty()
    status.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ (åŠ è½½æ¨¡å‹ä¸­ï¼Œè¯·ç¨å€™)...")

    # 1. åŠ è½½ Embedding æ¨¡å‹
    embed_model = HuggingFaceEmbedding(
        model_name=EMBED_MODEL_PATH,
        device="cuda",
        trust_remote_code=True
    )
    Settings.embedding_model = embed_model

    # 2. åŠ è½½ LLM (Qwen-2.5)
    llm = HuggingFaceLLM(
        context_window=32000, 
        max_new_tokens=512,
        generate_kwargs={"temperature": 0.1, "do_sample": True},
        tokenizer_name=LLM_MODEL_PATH,
        model_name=LLM_MODEL_PATH,
        device_map="auto",
        model_kwargs={"torch_dtype": torch.float16, "load_in_4bit": True},
    )
    Settings.llm = llm

    # 3. è¿æ¥å‘é‡æ•°æ®åº“
    if not os.path.exists(CHROMA_PATH):
        st.error("âŒ æœªæ‰¾åˆ°å‘é‡åº“ï¼è¯·å…ˆè¿è¡Œæ„å»ºè„šæœ¬ã€‚")
        st.stop()
        
    db = chromadb.PersistentClient(path=CHROMA_PATH)
    chroma_collection = db.get_or_create_collection(COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    
    # 4. åŠ è½½ç´¢å¼•
    index = VectorStoreIndex.from_vector_store(
        vector_store,
        embed_model=embed_model
    )

    # 5. æ„å»ºèŠå¤©å¼•æ“
    # mode="condense_plus_context": é€‚åˆå¤šè½®å¯¹è¯ï¼Œä¼šæŠŠå†å²è®°å½•å‹ç¼©æˆæ–°çš„æŸ¥è¯¢
    chat_engine = index.as_chat_engine(
        chat_mode="condense_plus_context",
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ™ºèƒ½åŠ©æ‰‹ã€‚
        è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„ã€å‚è€ƒæ–‡æ¡£ã€‘å›ç­”ç”¨æˆ·çš„åŒ»ç–—é—®é¢˜ã€‚
        å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´æ˜â€œèµ„æ–™åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ï¼Œä¸è¦ç¼–é€ ã€‚
        å›ç­”æ—¶è¯·ä¿æŒå®¢è§‚ã€ä¸¥è°¨ï¼Œå¹¶ä½¿ç”¨ä¸­æ–‡ã€‚""",
        similarity_top_k=3,
        verbose=True
    )
    
    status.empty() # æ¸…é™¤åŠ è½½æç¤º
    return chat_engine

# ================= ç•Œé¢é€»è¾‘ =================

st.title("ğŸ¥ åé©¼åŒ»ç–— RAG é—®ç­”ç³»ç»Ÿ")
st.caption("åŸºäº Qwen-2.5-7B ä¸ Huatuo-26M æ„å»º | æ”¯æŒå¤šè½®å¯¹è¯ä¸å¼•ç”¨æº¯æº")

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []

if "engine" not in st.session_state:
    st.session_state.engine = load_rag_engine()

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # å¦‚æœå†å²æ¶ˆæ¯é‡Œæœ‰å¼•ç”¨æºï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
        if "sources" in message:
            with st.expander("ğŸ“š å‚è€ƒæ¥æº (å†å²)"):
                st.markdown(message["sources"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ç—‡çŠ¶æˆ–åŒ»ç–—é—®é¢˜..."):
    # 1. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢æ–‡çŒ®å¹¶ç”Ÿæˆå›ç­”..."):
            try:
                # è°ƒç”¨ RAG å¼•æ“
                response = st.session_state.engine.chat(prompt)
                full_response = response.response
                
                # å±•ç¤ºå›ç­”
                message_placeholder.markdown(full_response)
                
                # --- å…³é”®ï¼šè§£æå¹¶å±•ç¤ºå¼•ç”¨æ¥æº ---
                source_text = ""
                if response.source_nodes:
                    with st.expander("ğŸ“š æŸ¥çœ‹å‚è€ƒæ¥æº (Evidence)"):
                        for idx, node in enumerate(response.source_nodes):
                            # è·å–å…ƒæ•°æ®
                            meta = node.metadata
                            score = node.score
                            content = node.node.get_content()[:100] + "..." # åªæ˜¾ç¤ºå‰100å­—é¢„è§ˆ
                            
                            # æ ¼å¼åŒ–æ˜¾ç¤º
                            one_source = f"**[æ¥æº {idx+1}]** (ç›¸ä¼¼åº¦: {score:.2f})\n\n> {content}\n"
                            st.markdown(one_source)
                            st.divider()
                            source_text += one_source

                # ä¿å­˜åŠ©æ‰‹å›å¤åˆ°å†å²è®°å½•
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": full_response,
                    "sources": source_text # è®°å½•å¼•ç”¨ä»¥ä¾¿åç»­æŸ¥çœ‹
                })
                
            except Exception as e:
                st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")