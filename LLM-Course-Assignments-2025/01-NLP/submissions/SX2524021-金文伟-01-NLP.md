# MedQA-RAG: A Medical QA System Powered by DeepSeek-R1
## 智疗问答：基于DeepSeek-R1的医疗RAG系统

SX2524021-金文伟

## 1. 项目概述

本项目面向“中文领域特定问答系统（Domain-Specific QA）”任务，选择**医疗问答**作为专业领域，基于公开数据集 **cMedQA2** 构建一个支持**多轮对话、长上下文、引用来源可解释、拒绝不确定回答**的 RAG（Retrieval-Augmented Generation）问答系统，并提供可复现的**向量库构建、评估与 Web Demo 部署**流程。

系统核心流程为：用户问题 →（可选：多轮查询改写）→ Chroma 向量库检索 Top-K 相关 QA 文档 → 拼接检索上下文 → 大模型生成回答（带证据约束与拒答策略）→ Web 端展示回答与检索来源。

---

## 2. 数据来源与处理

### 2.1 数据来源

- 数据集：cMedQA2  
  仓库地址：<https://github.com/zhangsheng93/cMedQA2>

数据集中包含：
- `question.csv`: `question_id, content`
- `answer.csv`: `ans_id, question_id, content`
- `train_candidates.txt / dev_candidates.txt / test_candidates.txt`: 候选答案对（测试集含 label，评估仅使用 `label=1` 的正样本）

### 2.2 数据处理与清洗

本项目以 cMedQA2 原始格式为主，进行的处理包括：
1. **去重**：对训练候选中的 `(question_id, pos_ans_id)` 去重，避免重复写入向量库导致检索偏置。
2. **构建文档**：将“问题 + 正回答”拼接为一条知识文档：
   - `问题：{question}\n\n回答：{pos_answer}`
3. **分块（Chunking）**：针对中文医疗文本采用递归分块策略（优先中文标点分隔），典型参数：
   - `chunk_size=500`，`chunk_overlap=100`
   - separators：`\n\n, \n, 。, ；, ，, 空格, ""`

> 以上处理由 `build_vector_store.py` 完成，可一键复现。

---

## 3. 方法与系统设计

### 3.1 模型与向量库

- 对话大模型（本地）：`ollama:deepseek-r1:8b`
- Embedding 模型（本地）：`qwen3-embedding:4b`
- 向量数据库：Chroma（持久化目录：`./chroma_rag_db`）
- 框架：LangChain 1.0（LCEL 方式组织 Prompt | Model | Retriever）

未进行大模型 LoRA/SFT 微调的原因：本作业以可复现与工程落地为目标，受限于本地算力资源，优先采用“检索增强 + 提示词约束 + 拒答门控”等低算力优化手段提升质量。

### 3.2 RAG 流程（检索增强生成）

1. **检索**：对用户 query 做向量检索，返回 Top-K 相关文档 chunk。
2. **上下文构造**：将检索结果按条目编号拼接为 context，并携带可追溯的 metadata（如 answer_id / question_id / chunk_id / score）。
3. **回答生成**：将问题 + context 输入大模型生成回答。提示词要求：
   - 尽量基于检索资料回答
   - 资料不足时明确“资料不足/不确定”，给出保守建议（如就医科室、危险信号、需要补充信息）
   - 不输出具体处方/剂量等高风险内容

### 3.3 多轮对话与长上下文（>32k tokens）的工程实现

由于直接将全部历史对话原文无限堆叠会导致上下文溢出与推理成本增加，本项目采用工程化“长上下文”策略：

- **滚动摘要（Summary Memory）**：将较早对话压缩为“关键事实摘要”（症状、时间线、检查、既往史等）。
- **最近窗口（Recent Window）**：保留最近 N 轮原文对话，保证对话自然性与指代连贯。
- **（可选）查询改写（Query Rewrite）**：基于“摘要 + 最近对话”将当前用户问题改写成独立可检索 query，提高多轮检索命中率。

该策略可以在对话轮次持续增长时仍保持系统稳定运行，满足“长上下文（>32k tokens）”的功能诉求（以工程记忆替代单次超长 prompt）。

### 3.4 引用来源显示与拒绝不确定回答

- **引用来源显示**：Web Demo 在 UI 中展示每轮检索的资料条目（含 answer_id、question_id、score），并在回答下方同步展示“本轮检索资料”以便追溯。
- **拒答（不确定回答）策略**：当检索结果相似度不足（距离超过阈值）时，系统拒绝给出武断结论，转而提示“资料不足/不确定”并给出安全建议与补充信息需求，从而降低幻觉风险。

---

## 4. 实验设置

### 4.1 对比方案

- **Baseline（无RAG）**：不加载向量库，直接将测试问题输入大模型生成回答。
- **RAG（Top-K 检索增强）**：加载 Chroma 向量库，检索 Top-K 相关文档拼接 context 后生成回答。

### 4.2 评估数据与指标

- 测试集：使用 `test_candidates.txt` 中 `label=1` 的样本（本次扫描实验取 `n=100` 条）。
- 指标：
  - **Accuracy（准确率）**：LLM-as-judge 判定回答是否与参考答案在医学含义上基本一致。
  - **Hallucination Rate（幻觉率）**：LLM-as-judge 判定回答是否出现明显编造、与证据冲突或证据不支持的关键医学事实。

> 注：引用 F1 指标在本次报告中未作为主结果展示（见第 6 节“问题分析”）。

---

## 5. 实验结果（表格 + 曲线）

### 5.1 指标表格（n=100）

| method   | top_k   |   n |   accuracy |   hallucination_rate |
|:---------|:--------|----:|-----------:|---------------------:|
| Baseline | -       | 100 |       0.76 |                 0.32 |
| RAG      | 1       | 100 |       0.96 |                 0.08 |
| RAG      | 2       | 100 |       1.00 |                 0.07 |
| RAG      | 4       | 100 |       1.00 |                 0.02 |
| RAG      | 8       | 100 |       0.99 |                 0.03 |

**结论：**
- 引入 RAG 后准确率从 **0.76** 显著提升到 **0.96~1.00**。
- 幻觉率从 **0.32** 显著下降到 **0.02~0.08**，其中 Top-K=4 时最低（0.02）。

### 5.2 曲线图（Top-K 扫描）

将下列图片放入仓库 `img/` 或报告同目录并引用即可：

- `topk_accuracy.png`：Top-K vs Accuracy  
- `topk_hallucination_rate.png`：Top-K vs Hallucination Rate

（示例插入方式：）
```md

![topk_accuracy.png](img%2Ftopk_accuracy.png)
![topk_hallucination_rate.png](img%2Ftopk_hallucination_rate.png)![Top-K vs Hallucination Rate](img/topk_hallucination_rate.png)
