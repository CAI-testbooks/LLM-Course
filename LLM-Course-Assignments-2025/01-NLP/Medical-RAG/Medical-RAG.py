import streamlit as st
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"#ä» Hugging Face ä¸‹è½½ BAAI/bge-m3 åµŒå…¥æ¨¡å‹æ—¶ æ— æ³•è¿æ¥åˆ°äº’è”ç½‘  éœ€ä¿®æ”¹
import json
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
import torch
# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================

ST_TITLE = "é€šç”¨ä¸­æ–‡åŒ»ç–—é¢†åŸŸæ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
#MODEL_NAME = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"  # æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_NAME = "/root/autodl-tmp/Medical-RAG/Tune-model/medical-qwen-merged"  # ä¿®æ”¹ä¸ºmerageåçš„æ¨¡å‹è·¯å¾„
EMBEDDING_MODEL = "BAAI/bge-m3"
VECTOR_DB_PATH = "/root/autodl-tmp/Medical-RAG/chroma_db_medical"  # â† å‘é‡åº“æŒä¹…åŒ–ç›®å½• æœ¬åœ°å·²å­˜åœ¨ Chroma å‘é‡æ•°æ®åº“ï¼ˆå¦‚ ./chroma_db_medicalï¼‰ï¼Œå°±ç›´æ¥åŠ è½½ï¼›å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä»æ–‡æ¡£æ„å»ºå¹¶å‘ç£ç›˜ä¿å­˜ã€‚
# ==========================================
# è‡ªå®šä¹‰ JSONL åŠ è½½å‡½æ•°
# ==========================================
def load_alpaca_json_as_documents(file_path):
    """
    ä» Alpaca æ ¼å¼çš„ JSON æ•°ç»„æ–‡ä»¶åŠ è½½ä¸º LangChain Documents
    é€‚é…æ ¼å¼ï¼š[{"instruction": "é—®é¢˜", "input": "", "output": "ç­”æ¡ˆ"}, ...]
    
    Args:
        file_path (str): JSONæ–‡ä»¶è·¯å¾„ï¼ˆAlpacaæ ¼å¼ï¼Œæ•°ç»„ï¼‰
        æ‹¼æ¥é—®é¢˜+ç­”æ¡ˆ
    
    Returns:
        list[Document]: LangChain Documentåˆ—è¡¨
    """
    docs = []
    try:
        # âœ… å…³é”®ä¿®æ”¹1ï¼šæ•´æ–‡ä»¶åŠ è½½JSONæ•°ç»„ï¼ˆæ›¿ä»£JSONLé€è¡Œè¯»å–ï¼‰
        with open(file_path, "r", encoding="utf-8") as f:
            data_list = json.load(f)  # ç›´æ¥åŠ è½½æ•´ä¸ªJSONæ•°ç»„
        
        # éå†æ¯ä¸ªAlpacaæ ¼å¼çš„æ¡ç›®
        for idx, item in enumerate(data_list, 1):
            # âœ… å…³é”®ä¿®æ”¹2ï¼šå­—æ®µæ˜ å°„ï¼ˆAlpacaæ ¼å¼â†’QAï¼‰
            # æå–é—®é¢˜ï¼ˆinstructionï¼‰ã€ç­”æ¡ˆï¼ˆoutputï¼‰ï¼Œinputä¸ºç©ºåˆ™å¿½ç•¥
            instruction = item.get("instruction", "").strip()  # å¯¹åº”åŸé—®é¢˜
            output = item.get("output", "").strip()            # å¯¹åº”åŸç­”æ¡ˆ
            input_text = item.get("input", "").strip()         # Alpacaçš„inputå­—æ®µï¼ˆåŒ»ç–—åœºæ™¯ä¸ºç©ºï¼‰

            # è¿‡æ»¤æ— æ•ˆæ¡ç›®ï¼ˆæ— é—®é¢˜/æ— ç­”æ¡ˆï¼‰
            if not instruction or not output:
                st.warning(f"è·³è¿‡æ— æ•ˆæ¡ç›®ï¼ˆç´¢å¼•{idx}ï¼‰ï¼šæ— é—®é¢˜æˆ–æ— ç­”æ¡ˆ")
                continue

            # âœ… ç°åœ¨æ€»æ˜¯æ‹¼æ¥é—®é¢˜+ç­”æ¡ˆï¼ˆä¸å†æœ‰æ¡ä»¶åˆ¤æ–­ï¼‰
            page_content = f"é—®é¢˜ï¼š{instruction}\nç­”æ¡ˆï¼š{output}"

            # âœ… å…ƒæ•°æ®ä¼˜åŒ–ï¼šä¿ç•™åŸå§‹ä¿¡æ¯ç”¨äºè¿½è¸ª
            metadata = {
                "source_instruction": instruction,  # åŸå§‹é—®é¢˜ï¼ˆæ›¿ä»£åŸsource_questionï¼‰
                "source_file": os.path.basename(file_path),
                "item_index": idx,  # æ¡ç›®ç´¢å¼•ï¼ˆæ›¿ä»£åŸè¡Œå·ï¼ŒJSONæ•°ç»„æ— è¡Œå·ï¼‰
                "has_input": True if input_text else False  # æ ‡è®°æ˜¯å¦æœ‰inputï¼ˆåŒ»ç–—åœºæ™¯ä¸ºFalseï¼‰
            }

            # åˆ›å»ºLangChain Documentå¯¹è±¡å¹¶åŠ å…¥åˆ—è¡¨
            docs.append(Document(page_content=page_content, metadata=metadata))

    except json.JSONDecodeError as e:
        st.error(f"JSONè§£æå¤±è´¥ï¼š{file_path} ä¸æ˜¯åˆæ³•çš„JSONæ•°ç»„æ ¼å¼ - {e}")
        return []
    except FileNotFoundError:
        st.error(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        return []
    except Exception as e:
        st.error(f"åŠ è½½æ–‡ä»¶å‡ºé”™ï¼š{e}")
        return []

    st.success(f"æˆåŠŸåŠ è½½ {len(docs)} æ¡æœ‰æ•ˆåŒ»ç–—QAæ•°æ®ï¼ˆæ¥è‡ª{os.path.basename(file_path)}ï¼‰")
    return docs

# ==========================================
# åˆå§‹åŒ– RAG ç³»ç»Ÿ
# ==========================================
@st.cache_resource
def initialize_rag_system():
    dataset_dir = "/root/autodl-tmp/Medical-RAG/dataset"
    if not os.path.exists(dataset_dir):
        return None, f"æ‰¾ä¸åˆ°æ•°æ®é›†ç›®å½•: {dataset_dir}"

    # å‘é‡åŒ–é…ç½®ï¼ˆå¿…é¡»æå‰å®šä¹‰ï¼Œç”¨äºåŠ è½½æˆ–åˆ›å»ºï¼‰
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True}
    )

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æŒä¹…åŒ–çš„å‘é‡åº“
    if os.path.exists(VECTOR_DB_PATH):
        st.info("æ£€æµ‹åˆ°å·²æœ‰å‘é‡åº“ï¼Œæ­£åœ¨åŠ è½½...")
        vectorstore = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        st.success("âœ… å‘é‡åº“åŠ è½½å®Œæˆï¼")
    else:
        # === éœ€è¦é‡æ–°æ„å»ºå‘é‡åº“ ===
        json_files = [
            os.path.join(dataset_dir, "alpaca_formatted_test_data.json"),
            os.path.join(dataset_dir, "alpaca_formatted_validation_data.json"),
            os.path.join(dataset_dir, "alpaca_formatted_train_data.json") # å¦‚æœæ˜¯æ„å»ºRAGç³»ç»Ÿçš„è¯ï¼Œä½¿ç”¨å…¨éƒ¨çš„answerè¿›è¡Œå‘é‡æ•°æ®åº“æŒä¹…åŒ–å³å¯ï¼Œåç»­åŠ å…¥QAå¯¹è¿›è¡Œå¾®è°ƒå³å¯
        ]
        
        docs = []
        for file_path in json_files:
            if os.path.exists(file_path):
                st.info(f"æ­£åœ¨åŠ è½½: {os.path.basename(file_path)}")
                file_docs = load_alpaca_json_as_documents(file_path)# â† æ‹¼æ¥Q Aå¯¹
                docs.extend(file_docs)
                st.success(f"å®ŒæˆåŠ è½½: {len(file_docs)} æ¡è®°å½• from {os.path.basename(file_path)}")
            else:
                st.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        if not docs:
            return None, "æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£"

       
        splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        splits = splitter.split_documents(docs)

        st.info("æ­£åœ¨æ„å»ºå‘é‡åº“")
        #é¦–æ¬¡åŠ è½½å·¨æ…¢æ— æ¯” è€å¿ƒç­‰å¾… å¤§çº¦å‡ åˆ†é’Ÿ
        #é¦–æ¬¡å¼€å¯é¡µé¢å¡é¡¿åï¼Œå¯ä»¥é‡æ–°python -m streamlit run exp.py ï¼Œå†æ‰“å¼€é¡µé¢å·¨å¿«
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=VECTOR_DB_PATH
        )
        st.success("âœ… å‘é‡åº“æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜è‡³æœ¬åœ°ï¼")

    # æ£€ç´¢å™¨ - è°ƒæ•´kå€¼ä»¥é€‚åº”æ›´å¤§çš„chunk_size
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # åŠ è½½æœ¬åœ° Qwen-2.5-7B æ¨¡å‹ï¼ˆå¸¦é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
    # åŠ è½½ tokenizer 
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    # åŠ è½½ model
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,    
        device_map="auto",
        trust_remote_code=True,
        #load_in_4bit=True,#æ˜¾å­˜ä¸å¤Ÿå¯ä»¥æ‰“å¼€

    )

    # åˆ›å»º pipeline æ—¶æ˜¾å¼æŒ‡å®š pad/eos token
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.1,
        top_p=0.95,
        repetition_penalty=1.1,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,   # â† å¿…é¡»
        eos_token_id=tokenizer.eos_token_id,   # â† æ¨è
        clean_up_tokenization_spaces=True,
        early_stopping=True      # ç”Ÿæˆåˆ°ç»“æŸç¬¦è‡ªåŠ¨åœæ­¢ï¼Œé¿å…å†—ä½™
    )
    #llm_pipeline = HuggingFacePipeline(pipeline=pipe)

    #llm = HuggingFacePipeline(pipeline=pipe)
    
    
    llm_pipeline = HuggingFacePipeline(pipeline=pipe)

    llm = ChatHuggingFace(
        llm=llm_pipeline,       # â† å¿…é¡»ç”¨ llm= å‚æ•°
        tokenizer=tokenizer,
        streaming=True
    )

    # åŒæ—¶ä¼˜åŒ–Promptæ¨¡æ¿ï¼ˆå‡å°‘æ¨¡å‹æ— æ„ä¹‰åˆ—ä¸¾ï¼‰
    template = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦åŠ©æ‰‹ã€‚
    å›ç­”è¦æ±‚ï¼š1. æ¡ç†æ¸…æ™°ï¼›2. ç¦æ­¢é‡å¤è¡¨è¿°ï¼›3. ç”Ÿæˆç­”æ¡ˆæ—¶ï¼Œä¸åšå†—ä½™æ¨ç†
    å¦‚æœä¸çŸ¥é“ï¼Œè¯·ç›´æ¥è¯´"æ ¹æ®ç°æœ‰åŒ»å­¦èµ„æ–™ï¼Œæˆ‘æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ"ã€‚

    åŒ»å­¦çŸ¥è¯†ï¼š
    {context}

    ç”¨æˆ·é—®é¢˜ï¼š
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)


    # RAG é“¾
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain, "ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ"


# ==========================================
# Streamlit UI
# ==========================================
st.set_page_config(page_title=ST_TITLE, page_icon="ğŸ¥")
st.title(ST_TITLE)
st.markdown("### ğŸ’Š åŸºäºåŒ»å­¦çŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
st.markdown("---")

with st.sidebar:
    st.header("ç³»ç»ŸçŠ¶æ€é¢æ¿")
    with st.spinner("æ­£åœ¨åŠ è½½åŒ»å­¦çŸ¥è¯†åº“..."):
        rag_chain, msg = initialize_rag_system()

    if rag_chain:
        st.success("âœ… åŒ»å­¦çŸ¥è¯†åº“å·²æŒ‚è½½ (RAG Ready)")
        st.info(f"ğŸ§  æ¨¡å‹: {MODEL_NAME}")
    else:
        st.error(f"âŒ å¯åŠ¨å¤±è´¥: {msg}")
        st.stop()

    st.markdown("---")
    st.markdown("**å…è´£å£°æ˜**")
    st.markdown("âš ï¸ æœ¬ç³»ç»Ÿä»…æä¾›åŒ»å­¦çŸ¥è¯†å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€‚å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³å°±åŒ»ã€‚")
    
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ–å¯¹è¯å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥å…³äºä¸­æ–‡åŒ»ç–—é¢†åŸŸçš„é—®é¢˜..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        try:
            for chunk in rag_chain.stream(prompt):
                full_response += chunk
                response_placeholder.markdown(full_response + "â–Œ")
            response_placeholder.markdown(full_response)
        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°é”™è¯¯: {str(e)}"
            st.error(error_msg)
            full_response = error_msg

    st.session_state.messages.append({"role": "assistant", "content": full_response})