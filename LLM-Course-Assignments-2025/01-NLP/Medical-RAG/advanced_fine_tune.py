# advanced_fine_tune.py - é’ˆå¯¹ AutoDL çš„ä¼˜åŒ–å¾®è°ƒè„šæœ¬ï¼ˆQwen2.5-7B + LoRA + 4-bitï¼‰
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import numpy as np
import torch
import json
from datasets import Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
import matplotlib.pyplot as plt
import pandas as pd
from peft import prepare_model_for_kbit_training
# ========================
# è·¯å¾„é…ç½®
# ========================
DATASET_DIR = "/root/autodl-tmp/Medical-RAG/dataset"
TRAIN_FILE = os.path.join(DATASET_DIR, "alpaca_formatted_train_data_8k.json")
VAL_FILE = os.path.join(DATASET_DIR, "alpaca_formatted_validation_data.json")
OUTPUT_BASE = "/root/autodl-tmp/Medical-RAG/Tune-model"
MODEL_PATH = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"
# æ£€æŸ¥è®­ç»ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(TRAIN_FILE):
    raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {TRAIN_FILE}")

has_validation = os.path.exists(VAL_FILE)
print("âœ… æ•°æ®æ–‡ä»¶æ£€æŸ¥å®Œæˆ")


# ========================
# æ¨¡å‹ä¸ Tokenizer
# ========================
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True,use_cache=False)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    load_in_4bit=True,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
)
model = prepare_model_for_kbit_training(model)
# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
def load_and_process_dataset(path):
    df = pd.read_json(path, orient='records')# è¯»å– JSON æ–‡ä»¶ä¸º pandas DataFrame
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].astype(str)# è¯»å– JSON æ–‡ä»¶ä¸º pandas DataFrame
    return Dataset.from_pandas(df)# è½¬ä¸º Hugging Face Dataset æ ¼å¼ï¼ˆä¾¿äºåç»­ map æ“ä½œï¼‰



def preprocess_function(examples):
    instructions = examples['instruction']
    inputs = examples.get('input', [""] * len(instructions))
    outputs = examples['output']
    #é€šç”¨ prompt
    full_texts = []
    for instr, inp, out in zip(instructions, inputs, outputs):
        text = (
            "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚<|im_end|>\n"
            f"<|im_start|>user\n{instr}\n{inp}<|im_end|>\n"
            f"<|im_start|>assistant\n{out}<|im_end|>"
        )
        full_texts.append(text)

    model_inputs = tokenizer(
        full_texts,
        max_length=512,
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    )

    labels = model_inputs["input_ids"].clone()
    for i in range(labels.size(0)):
        prompt = full_texts[i].split("<|im_start|>assistant\n")[0] + "<|im_start|>assistant\n"
        prompt_tokens = tokenizer(prompt, add_special_tokens=True)["input_ids"]
        assistant_start = len(prompt_tokens)
        if assistant_start < labels.size(1):
            labels[i, :assistant_start] = -100

    model_inputs["labels"] = labels
    return model_inputs

train_dataset = load_and_process_dataset(TRAIN_FILE)
val_dataset = load_and_process_dataset(VAL_FILE)

print(f"  Train: {TRAIN_FILE}")
print(f"  Val:   {VAL_FILE} ({'å­˜åœ¨' if has_validation else 'ä¸å­˜åœ¨'})")
print("\nğŸ” è®­ç»ƒé›†ç¤ºä¾‹:")
if len(train_dataset) > 0:
    print(train_dataset[0])
else:
    print("è®­ç»ƒé›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")

if val_dataset and len(val_dataset) > 0:
    print("\nğŸ” éªŒè¯é›†ç¤ºä¾‹:")
    print(val_dataset[0])

# æ£€æŸ¥æ•°æ®ç»“æ„
print("\nğŸ“Š æ•°æ®é›†ç»“æ„ä¿¡æ¯:")
print(f"è®­ç»ƒé›†åˆ—å: {train_dataset.column_names}")
print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
if val_dataset:
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
train_tokenized = train_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=train_dataset.column_names
)
val_tokenized = val_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=val_dataset.column_names
)

print(f"âœ… é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›† {len(train_tokenized)} æ¡ï¼ŒéªŒè¯é›† {len(val_tokenized) if val_tokenized else 0} æ¡")

# ========================
# LoRA é…ç½®
# ========================
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    bias="none",
    r=16,
    lora_alpha=64,
    lora_dropout=0.1,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)

# åº”ç”¨ LoRA
model = get_peft_model(model, peft_config)  # â† æ­¤æ—¶ model æ‰æ˜¯ PeftModel
model.print_trainable_parameters()

# æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å‚æ•° requires_grad=True
trainable_params = [p for p in model.parameters() if p.requires_grad]
print(f"Number of trainable parameters: {len(trainable_params)}")
if len(trainable_params) == 0:
    raise RuntimeError("âŒ æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼LoRA æœªæ­£ç¡®æ³¨å…¥ã€‚")
# ========================
# è®­ç»ƒå‚æ•°
# ========================
training_args = TrainingArguments(
    output_dir=os.path.join(OUTPUT_BASE, "medical-qwen-lora"),
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_steps=300,
    logging_steps=50,
    save_steps=300,
    evaluation_strategy="steps" if val_tokenized else "no",
    eval_steps=300,#æ¯ 300 æ­¥è¯„ä¼°ä¸€æ¬¡ï¼ˆå¯é€‰ï¼‰
    learning_rate=5e-4,
    fp16=True,
    logging_dir=os.path.join(OUTPUT_BASE, "logs"),
    save_total_limit=2,
    load_best_model_at_end=True if val_tokenized else False,
    metric_for_best_model="eval_loss" if val_tokenized else None,
    greater_is_better=False,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    gradient_checkpointing=True,
    report_to=["tensorboard"],

)

# ========================
# Data Collator
# ========================
# 
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)


# ========================
# å¯è§†åŒ–è®­ç»ƒloss
# ========================
from transformers import TrainerCallback

train_losses = []
eval_losses = []
steps = []
# å›è°ƒå‡½æ•°
class LossLoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            train_losses.append(logs["loss"])
            steps.append(state.global_step)

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is not None and "eval_loss" in metrics:
            eval_losses.append(metrics["eval_loss"])

loss_callback = LossLoggingCallback()



# ========================
# å¯åŠ¨è®­ç»ƒ
# ========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    data_collator=data_collator,
    callbacks=[loss_callback],  # â†â†â† æ–°å¢å›è°ƒï¼Œè¿›è¡Œlosså¯è§†åŒ–æ“ä½œ
)

print("\nğŸš€ å¼€å§‹LORAå¾®è°ƒè®­ç»ƒ...")
trainer.train()

# ========================
# ä¿å­˜æ¨¡å‹
# ========================
final_lora_dir = os.path.join(OUTPUT_BASE, "medical-qwen-lora-final")
model.save_pretrained(final_lora_dir)
tokenizer.save_pretrained(final_lora_dir)
print(f"\nâœ… LoRA é€‚é…å™¨å·²ä¿å­˜è‡³: {final_lora_dir}")


# ========================
# å¯è§†åŒ–è®­ç»ƒlosså±•ç¤º
# ========================
if len(train_losses) > 0:
    plt.figure(figsize=(12, 5))

    # trainâ€”loss
    plt.subplot(1, 2, 1)
    plt.plot(steps, train_losses, label="Train Loss", marker='o', markersize=3)
    plt.title("Training Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # eval-loss
    if eval_losses:
        # eval æ¯ eval_steps ä¸€æ¬¡ï¼Œä» eval_steps å¼€å§‹
        eval_steps_list = [i * training_args.eval_steps for i in range(1, len(eval_losses) + 1)]
        plt.subplot(1, 2, 2)
        plt.plot(eval_steps_list, eval_losses, label="Eval Loss", color="red", marker='s', markersize=3)
        plt.title("Evaluation Loss")
        plt.xlabel("Step")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    loss_save_path = os.path.join(OUTPUT_BASE, "logs", "loss_save.png")
    plt.savefig(loss_save_path, dpi=150)
    plt.close()  # é¿å…åœ¨ notebook ä¸­æ˜¾ç¤º
    print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜è‡³: {loss_save_path}")
    
    

print("ğŸ‰LORA å¾®è°ƒå®Œæˆï¼")