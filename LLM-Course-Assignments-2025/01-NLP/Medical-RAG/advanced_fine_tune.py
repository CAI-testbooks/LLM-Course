# advanced_fine_tune.py - é’ˆå¯¹ AutoDL çš„ä¼˜åŒ–å¾®è°ƒè„šæœ¬ï¼ˆQwen2.5-7B + LoRA + 4-bitï¼‰
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import numpy as np
import torch
import json
from datasets import Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
import matplotlib.pyplot as plt
# ========================
# è·¯å¾„é…ç½®
# ========================
DATASET_DIR = "/root/autodl-tmp/Medical-RAG/dataset"
TRAIN_FILE = os.path.join(DATASET_DIR, "train_data_8k.json")
VAL_FILE = os.path.join(DATASET_DIR, "validation_data.json")
OUTPUT_BASE = "/root/autodl-tmp/Medical-RAG/Tune-model"

# æ£€æŸ¥è®­ç»ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(TRAIN_FILE):
    raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {TRAIN_FILE}")

has_validation = os.path.exists(VAL_FILE)
print("âœ… æ•°æ®æ–‡ä»¶æ£€æŸ¥å®Œæˆ")
print(f"  Train: {TRAIN_FILE}")
print(f"  Val:   {VAL_FILE} ({'å­˜åœ¨' if has_validation else 'ä¸å­˜åœ¨'})")

# ========================
# ä¿®æ­£çš„æ•°æ®é›†åŠ è½½å‡½æ•°ï¼ˆå¤„ç†JSONLæ ¼å¼ï¼‰
# ========================
def load_jsonl_like(file_path):
    """æ­£ç¡®åŠ è½½JSONLæ ¼å¼çš„æ•°æ®é›†"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å°è¯•ç›´æ¥è§£æJSONæ•°ç»„
    try:
        data = json.loads(content)
        if isinstance(data, list):
            return data
    except json.JSONDecodeError:
        pass
    
    # å¦‚æœæ˜¯çœŸæ­£çš„JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
    try:
        records = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    records.append(json.loads(line))
        return records
    except json.JSONDecodeError:
        pass
    
    # å¦‚æœæ˜¯æ ¼å¼é”™è¯¯çš„JSONæ•°ç»„ï¼ˆç¼ºå°‘é€—å·æˆ–å¼•å·é”™è¯¯ï¼‰
    # æŒ‰ }{ åˆ†å‰²è¿›è¡Œä¿®å¤
    records = []
    for obj_str in content.strip().split('}{'):
        if not obj_str.startswith('{'):
            obj_str = '{' + obj_str
        if not obj_str.endswith('}'):
            obj_str = obj_str + '}'
        try:
            records.append(json.loads(obj_str))
        except json.JSONDecodeError:
            continue  # è·³è¿‡æŸåè¡Œ
    return records

def create_flat_dataset(file_path):
    """åˆ›å»ºæ‰å¹³åŒ–çš„æ•°æ®é›†"""
    raw_records = load_jsonl_like(file_path)
    
    # å±•å¹³ä¸º {"question": "...", "answer": "..."} åˆ—è¡¨
    flat_data = []
    for idx, rec in enumerate(raw_records):
        if isinstance(rec, dict):
            questions_list = rec.get("questions", [])
            answers_list = rec.get("answers", [])
            
            # å¤„ç†å¤šä¸ªé—®é¢˜å¯¹åº”ä¸€ä¸ªç­”æ¡ˆçš„æƒ…å†µ
            if answers_list:
                answer = str(answers_list[0]) if answers_list else ""
                for q_list in questions_list:
                    if isinstance(q_list, list):
                        for q in q_list:  # æ”¯æŒå¤šé—®ä¸€ç­”
                            flat_data.append({
                                "question": str(q).strip(), 
                                "answer": answer.strip()
                            })
                    elif isinstance(q_list, str):
                        # å¦‚æœé—®é¢˜å­—æ®µç›´æ¥æ˜¯å­—ç¬¦ä¸²
                        flat_data.append({
                            "question": q_list.strip(), 
                            "answer": answer.strip()
                        })
    
    return Dataset.from_list(flat_data)

# åŠ è½½æ•°æ®é›†
train_dataset = create_flat_dataset(TRAIN_FILE)
val_dataset = create_flat_dataset(VAL_FILE) if has_validation else None

print("\nğŸ” è®­ç»ƒé›†ç¤ºä¾‹:")
if len(train_dataset) > 0:
    print(train_dataset[0])
else:
    print("è®­ç»ƒé›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")

if val_dataset and len(val_dataset) > 0:
    print("\nğŸ” éªŒè¯é›†ç¤ºä¾‹:")
    print(val_dataset[0])

# æ£€æŸ¥æ•°æ®ç»“æ„
print("\nğŸ“Š æ•°æ®é›†ç»“æ„ä¿¡æ¯:")
print(f"è®­ç»ƒé›†åˆ—å: {train_dataset.column_names}")
print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
if val_dataset:
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")

# ========================
# æ¨¡å‹ä¸ Tokenizer
# ========================
MODEL_PATH = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    load_in_4bit=True,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
)
model.enable_input_require_grads()

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)
tokenizer.pad_token = tokenizer.eos_token

# ========================
# æ„å»º Qwen å¯¹è¯æ¨¡æ¿
# ========================
def format_qwen_prompt(question: str, answer: str) -> str:
    return f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—é—®ç­”åŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
{answer}<|im_end|>"""

def preprocess_function(examples):
    """å¤„ç†æ‰å¹³åŒ–æ•°æ®çš„é¢„å¤„ç†å‡½æ•°ï¼Œæ©ç éassistantéƒ¨åˆ†"""
    batch_input_ids = []
    batch_attention_mask = []
    batch_labels = []
    
    # è·å–æ ·æœ¬æ•°é‡
    num_samples = len(examples["question"]) if "question" in examples else 0
    
    # éå†æ¯ä¸ªæ ·æœ¬
    for i in range(num_samples):
        question = str(examples["question"][i]).strip()
        answer = str(examples["answer"][i]).strip()
        
        # è·³è¿‡ç©ºæ•°æ®
        if not question or not answer:
            continue
            
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = format_qwen_prompt(question, answer)
        
        # æ„å»ºä»…åŒ…å«systemå’Œuseréƒ¨åˆ†çš„promptï¼ˆç”¨äºç¡®å®šæ©ç ä½ç½®ï¼‰
        user_prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—é—®ç­”åŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
        
        # åˆ†è¯å¤„ç†
        full_tokens = tokenizer(
            full_prompt,
            truncation=True,
            max_length=1024,
            padding=False,
            return_tensors="pt",
        )
        
        user_tokens = tokenizer(
            user_prompt,
            truncation=True,
            max_length=1024,
            padding=False,
            return_tensors="pt",
        )
        
        # è·å–input_ids
        input_ids = full_tokens["input_ids"][0]
        
        # åˆ›å»ºlabelsï¼Œåˆå§‹åŒ–ä¸º-100ï¼ˆå¿½ç•¥lossè®¡ç®—ï¼‰
        labels = torch.full_like(input_ids, -100)
        
        # æ‰¾åˆ°assistantéƒ¨åˆ†å¼€å§‹çš„ä½ç½®
        user_len = len(user_tokens["input_ids"][0])
        
        # ç¡®ä¿assistantéƒ¨åˆ†åœ¨åºåˆ—èŒƒå›´å†…
        if user_len < len(input_ids):
            # ä»assistantéƒ¨åˆ†å¼€å§‹çš„ä½ç½®è®¾ç½®labelsä¸ºå®é™…token
            labels[user_len:] = input_ids[user_len:]
        
        # åˆ›å»ºattention_mask
        attention_mask = torch.ones_like(input_ids)
        
        batch_input_ids.append(input_ids)
        batch_attention_mask.append(attention_mask)
        batch_labels.append(labels)

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›ç©ºå­—å…¸
    if len(batch_input_ids) == 0:
        return {"input_ids": [], "attention_mask": [], "labels": []}

    # å¡«å……åˆ°ç›¸åŒé•¿åº¦
    max_length = max(len(ids) for ids in batch_input_ids)
    
    padded_input_ids = []
    padded_attention_mask = []
    padded_labels = []
    
    for input_ids, attention_mask, labels in zip(batch_input_ids, batch_attention_mask, batch_labels):
        # å¡«å……input_ids
        if len(input_ids) < max_length:
            pad_len = max_length - len(input_ids)
            padded_input_ids.append(torch.cat([input_ids, torch.full((pad_len,), tokenizer.pad_token_id)]))
            padded_attention_mask.append(torch.cat([attention_mask, torch.zeros((pad_len,))]))
            padded_labels.append(torch.cat([labels, torch.full((pad_len,), -100)]))
        else:
            padded_input_ids.append(input_ids)
            padded_attention_mask.append(attention_mask)
            padded_labels.append(labels)
    
    return {
        "input_ids": torch.stack(padded_input_ids),
        "attention_mask": torch.stack(padded_attention_mask),
        "labels": torch.stack(padded_labels)
    }

# ========================
# é¢„å¤„ç†æ•°æ®é›†
# ========================
print("\nğŸ”„ æ­£åœ¨é¢„å¤„ç†æ•°æ®...")
train_tokenized = train_dataset.map(
    preprocess_function,
    batched=True,
    num_proc=4,
    remove_columns=train_dataset.column_names,
    desc="Tokenizing train set"
).filter(lambda x: len(x["input_ids"]) > 0)

if val_dataset:
    val_tokenized = val_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=4,
        remove_columns=val_dataset.column_names,
        desc="Tokenizing validation set"
    ).filter(lambda x: len(x["input_ids"]) > 0)
else:
    val_tokenized = None

print(f"âœ… é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›† {len(train_tokenized)} æ¡ï¼ŒéªŒè¯é›† {len(val_tokenized) if val_tokenized else 0} æ¡")

# ========================
# LoRA é…ç½®
# ========================
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,
    lora_alpha=64,
    lora_dropout=0.1,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    init_lora_weights=False,
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ========================
# è®­ç»ƒå‚æ•°
# ========================
training_args = TrainingArguments(
    output_dir=os.path.join(OUTPUT_BASE, "medical-qwen-lora"),
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_steps=300,
    logging_steps=50,
    save_steps=300,
    evaluation_strategy="steps" if val_tokenized else "no",
    eval_steps=300,#æ¯ 300 æ­¥è¯„ä¼°ä¸€æ¬¡ï¼ˆå¯é€‰ï¼‰
    learning_rate=2e-4,
    fp16=True,
    logging_dir=os.path.join(OUTPUT_BASE, "logs"),
    save_total_limit=2,
    load_best_model_at_end=True if val_tokenized else False,
    metric_for_best_model="eval_loss" if val_tokenized else None,
    greater_is_better=False,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    gradient_checkpointing=True,
    report_to=["tensorboard"],

)

# ========================
# Data Collator
# ========================
# 
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# æµ‹è¯• collator
sample_batch = [train_tokenized[i] for i in range(min(2, len(train_tokenized)))]
try:
    batch = data_collator(sample_batch)
    print("âœ… Data collator æµ‹è¯•é€šè¿‡ï¼")
    print("Batch keys:", list(batch.keys()))
    print("input_ids shape:", batch["input_ids"].shape)
except Exception as e:
    print("âŒ Data collator æŠ¥é”™:", e)
    raise
# ========================
# å¯è§†åŒ–è®­ç»ƒloss
# ========================
from transformers import TrainerCallback

train_losses = []
eval_losses = []
steps = []
# å›è°ƒå‡½æ•°
class LossLoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            train_losses.append(logs["loss"])
            steps.append(state.global_step)

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is not None and "eval_loss" in metrics:
            eval_losses.append(metrics["eval_loss"])

loss_callback = LossLoggingCallback()



# ========================
# å¯åŠ¨è®­ç»ƒ
# ========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    data_collator=data_collator,
    callbacks=[loss_callback],  # â†â†â† æ–°å¢å›è°ƒï¼Œè¿›è¡Œlosså¯è§†åŒ–æ“ä½œ
)

print("\nğŸš€ å¼€å§‹LORAå¾®è°ƒè®­ç»ƒ...")
trainer.train()

# ========================
# ä¿å­˜æ¨¡å‹
# ========================
final_lora_dir = os.path.join(OUTPUT_BASE, "medical-qwen-lora-final")
model.save_pretrained(final_lora_dir)
tokenizer.save_pretrained(final_lora_dir)
print(f"\nâœ… LoRA é€‚é…å™¨å·²ä¿å­˜è‡³: {final_lora_dir}")


# ========================
# å¯è§†åŒ–è®­ç»ƒlosså±•ç¤º
# ========================
if len(train_losses) > 0:
    plt.figure(figsize=(12, 5))

    # trainâ€”loss
    plt.subplot(1, 2, 1)
    plt.plot(steps, train_losses, label="Train Loss", marker='o', markersize=3)
    plt.title("Training Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # eval-loss
    if eval_losses:
        # eval æ¯ eval_steps ä¸€æ¬¡ï¼Œä» eval_steps å¼€å§‹
        eval_steps_list = [i * training_args.eval_steps for i in range(1, len(eval_losses) + 1)]
        plt.subplot(1, 2, 2)
        plt.plot(eval_steps_list, eval_losses, label="Eval Loss", color="red", marker='s', markersize=3)
        plt.title("Evaluation Loss")
        plt.xlabel("Step")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    loss_save_path = os.path.join(OUTPUT_BASE, "logs", "loss_save.png")
    plt.savefig(loss_save_path, dpi=150)
    plt.close()  # é¿å…åœ¨ notebook ä¸­æ˜¾ç¤º
    print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜è‡³: {loss_save_path}")

print("ğŸ‰LORA å¾®è°ƒå®Œæˆï¼")