# advanced_fine_tune.py - é’ˆå¯¹ AutoDL çš„ä¼˜åŒ–å¾®è°ƒè„šæœ¬ï¼ˆQwen2.5-7B + LoRA + 4-bitï¼‰
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import numpy as np
import torch
import json
from datasets import Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
import matplotlib.pyplot as plt
import pandas as pd
from peft import prepare_model_for_kbit_training
from transformers import TrainerCallback

# ========================
# è·¯å¾„é…ç½®
# ========================
DATASET_DIR = "/root/autodl-tmp/Medical-RAG/dataset"
TRAIN_FILE = os.path.join(DATASET_DIR, "alpaca_formatted_train_data.json")
VAL_FILE = os.path.join(DATASET_DIR, "alpaca_formatted_validation_data.json")
OUTPUT_BASE = "/root/autodl-tmp/Medical-RAG/Tune-model"
MODEL_PATH = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"

# æ£€æŸ¥è®­ç»ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(TRAIN_FILE):
    raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {TRAIN_FILE}")

has_validation = os.path.exists(VAL_FILE)
print("âœ… æ•°æ®æ–‡ä»¶æ£€æŸ¥å®Œæˆ")

# ========================
# æ¨¡å‹ä¸ Tokenizer
# ========================
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH, 
    trust_remote_code=True,
    use_cache=False,
    padding_side="right"  # å…³é”®ï¼šå³paddingé¿å…å½±å“ç”Ÿæˆåœæ­¢
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"  # ç¡®ä¿ç”Ÿæˆæ—¶ä»å·¦åˆ°å³ï¼Œåœæ­¢ä¿¡å·æœ‰æ•ˆ

# 4-bité‡åŒ–é…ç½®ä¼˜åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=bnb_config,  # ä½¿ç”¨æ˜¾å¼çš„é‡åŒ–é…ç½®
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
)
model = prepare_model_for_kbit_training(model)
model.config.use_cache = False  # è®­ç»ƒæ—¶å…³é—­cacheï¼Œé¿å…å¹²æ‰°
model.config.pretraining_tp = 1

# ========================
# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼šè§£å†³æ ‡ç­¾é”™è¯¯å¯¼è‡´çš„é‡å¤ï¼‰
# ========================
def load_and_process_dataset(path):
    df = pd.read_json(path, orient='records')
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].astype(str).str.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼ï¼Œé¿å…æ— æ•ˆtoken
    # è¿‡æ»¤ç©ºæ•°æ®
    df = df[(df['instruction'].notna()) & (df['output'].notna())]
    return Dataset.from_pandas(df)

def preprocess_function(examples):
    instructions = examples['instruction']
    inputs = examples.get('input', [""] * len(instructions))
    outputs = examples['output']
    full_texts = []
    
    # ä¼˜åŒ–promptæ¨¡æ¿ï¼šæ˜ç¡®ç»“æŸæ ‡è¯†ï¼Œå¼•å¯¼æ¨¡å‹åœæ­¢ç”Ÿæˆ
    for instr, inp, out in zip(instructions, inputs, outputs):
        # å…³é”®ä¿®æ”¹ï¼šåœ¨assistantå›å¤æœ«å°¾åŠ å…¥æ˜ç¡®çš„ç»“æŸæ ‡è®°
        text = (
            "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚å›ç­”è¦æ±‚ï¼š1. æ¡ç†æ¸…æ™°ï¼›2. ç¦æ­¢é‡å¤è¡¨è¿°ï¼›3. å›ç­”æ—¶ï¼Œä¸åšå†—ä½™æ¨ç†ã€‚<|im_end|>\n"
            f"<|im_start|>user\n{instr}\n{inp}<|im_end|>\n"
            f"<|im_start|>assistant\n{out}<|im_end|>"  # ä¿ç•™åŸå§‹ç»“æŸæ ‡è®°ï¼Œå¼ºåŒ–åœæ­¢ä¿¡å·
        )
        full_texts.append(text)

    # ä¼˜åŒ–tokenizerå‚æ•°ï¼šé¿å…æˆªæ–­assistantéƒ¨åˆ†
    model_inputs = tokenizer(
        full_texts,
        max_length=256,  
        truncation=True,
        truncation_strategy="only_first",  # ä¼˜å…ˆæˆªæ–­promptï¼Œä¿ç•™å›ç­”
        padding="max_length",
        return_tensors="pt",
        return_attention_mask=True  # æ˜¾å¼è¿”å›attention mask
    )

    # æ ¸å¿ƒä¼˜åŒ–ï¼šç²¾å‡†è®¡ç®—assistantéƒ¨åˆ†èµ·å§‹ä½ç½®ï¼Œé¿å…æ ‡ç­¾é”™è¯¯
    labels = model_inputs["input_ids"].clone()
    labels[:] = -100  # å…ˆå…¨éƒ¨ç½®ä¸º-100ï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰
    
    for i in range(len(full_texts)):
        # æ‹†åˆ†promptå’Œå›ç­”éƒ¨åˆ†
        prompt_part = full_texts[i].split("<|im_start|>assistant\n")[0] + "<|im_start|>assistant\n"
        # è®¡ç®—promptéƒ¨åˆ†çš„tokenæ•°ï¼ˆä¸æ·»åŠ é¢å¤–special tokenï¼Œé¿å…åç§»ï¼‰
        prompt_tokens = tokenizer(
            prompt_part, 
            add_special_tokens=False,  # å…³é”®ï¼šå’Œfull_textsçš„tokenizationä¿æŒä¸€è‡´
            return_attention_mask=False
        )["input_ids"]
        assistant_start_idx = len(prompt_tokens)
        
        # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        if assistant_start_idx < model_inputs["input_ids"].shape[1]:
            # åªå¯¹assistantéƒ¨åˆ†è®¡ç®—æŸå¤±
            labels[i, assistant_start_idx:] = model_inputs["input_ids"][i, assistant_start_idx:]
            
            # é¢å¤–ä¼˜åŒ–ï¼šå°†å›ç­”æœ«å°¾çš„<|im_end|>ä¹Ÿè®¡å…¥æŸå¤±ï¼Œå¼ºåŒ–åœæ­¢ä¿¡å·
            end_token = tokenizer("<|im_end|>", add_special_tokens=False)["input_ids"][0]
            labels[i, assistant_start_idx:] = torch.where(
                model_inputs["input_ids"][i, assistant_start_idx:] == end_token,
                end_token,
                labels[i, assistant_start_idx:]
            )

    model_inputs["labels"] = labels
    model_inputs["attention_mask"] = model_inputs["attention_mask"].bool()  # ç¡®ä¿maskç±»å‹æ­£ç¡®
    return model_inputs

train_dataset = load_and_process_dataset(TRAIN_FILE)
val_dataset = load_and_process_dataset(VAL_FILE) if has_validation else None

print(f"  Train: {TRAIN_FILE} (æœ‰æ•ˆæ•°æ®ï¼š{len(train_dataset)})")
print(f"  Val:   {VAL_FILE} ({'å­˜åœ¨' if has_validation else 'ä¸å­˜åœ¨'}ï¼Œæœ‰æ•ˆæ•°æ®ï¼š{len(val_dataset) if val_dataset else 0})")

if len(train_dataset) > 0:
    print("\nğŸ” è®­ç»ƒé›†ç¤ºä¾‹:")
    print(train_dataset[0])
else:
    raise ValueError("è®­ç»ƒé›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–è¿‡æ»¤æ¡ä»¶")

# é¢„å¤„ç†æ•°æ®é›†
train_tokenized = train_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=train_dataset.column_names,
    #num_proc=os.cpu_count()  # å¤šè¿›ç¨‹åŠ é€Ÿ
)

val_tokenized = None
if val_dataset and len(val_dataset) > 0:
    val_tokenized = val_dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=val_dataset.column_names,
        #num_proc=os.cpu_count()
    )

print(f"âœ… é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›† {len(train_tokenized)} æ¡ï¼ŒéªŒè¯é›† {len(val_tokenized) if val_tokenized else 0} æ¡")

# ========================
# LoRA é…ç½®ï¼ˆå¾®è°ƒå‚æ•°é™ä½è¿‡æ‹Ÿåˆï¼‰
# ========================
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    bias="none",
    r=8,  # é™ä½rå€¼ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©
    lora_alpha=32,  # å¯¹åº”rå€¼è°ƒæ•´
    lora_dropout=0.15,  # å¢å¤§dropoutï¼ŒæŠ‘åˆ¶è¿‡æ‹Ÿåˆ
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,
    use_rslora=True,  # æå‡LoRAç¨³å®šæ€§
)

# åº”ç”¨ LoRA
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.4f}%)")
if trainable_params == 0:
    raise RuntimeError("âŒ æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼LoRA æœªæ­£ç¡®æ³¨å…¥ã€‚")

# ========================
# è®­ç»ƒå‚æ•°ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ+æå‡ç¨³å®šæ€§ï¼‰
# ========================
training_args = TrainingArguments(
    output_dir=os.path.join(OUTPUT_BASE, "medical-qwen-lora"),
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_ratio=0.05,  # æ”¹ç”¨æ¯”ä¾‹ï¼Œé€‚é…ä¸åŒæ•°æ®é›†å¤§å°
    logging_steps=10,
    save_steps=300,
    evaluation_strategy="steps" if val_tokenized else "no",
    eval_steps=300,
    learning_rate=2e-4,  # é™ä½å­¦ä¹ ç‡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼ˆåŸ5e-4åå¤§ï¼‰
    fp16=True,
    fp16_full_eval=True,  # éªŒè¯æ—¶ä¹Ÿç”¨fp16ï¼Œæå‡æ•ˆç‡å’Œç¨³å®šæ€§
    logging_dir=os.path.join(OUTPUT_BASE, "logs"),
    save_total_limit=2,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    gradient_checkpointing=True,
    load_best_model_at_end=True if val_tokenized else False,
    metric_for_best_model="eval_loss" if val_tokenized else None,
    greater_is_better=False,
    gradient_checkpointing_kwargs={"use_reentrant": False},  # é€‚é…æ–°ç‰ˆPyTorch
    report_to=["tensorboard"],
    optim="adamw_torch",
    weight_decay=0.01,  # åŠ å…¥æƒé‡è¡°å‡ï¼ŒæŠ‘åˆ¶è¿‡æ‹Ÿåˆ
    max_grad_norm=1.0,  # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´è®­ç»ƒä¸ç¨³å®š
    lr_scheduler_type="cosine",  # ä½™å¼¦å­¦ä¹ ç‡è¡°å‡ï¼Œè®©è®­ç»ƒæ›´å¹³ç¨³
)

# ========================
# Data Collator
# ========================
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    pad_to_multiple_of=8,  # å¯¹é½ç¡¬ä»¶ï¼Œæå‡æ•ˆç‡
    return_tensors="pt"
)

# ========================
# å›è°ƒå‡½æ•°
# ========================
train_losses = []
eval_losses = []
steps = []

class LossLoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            train_losses.append(logs["loss"])
            steps.append(state.global_step)

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is not None and "eval_loss" in metrics:
            eval_losses.append(metrics["eval_loss"])

callbacks = [LossLoggingCallback()]

# ========================
# å¯åŠ¨è®­ç»ƒ
# ========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    data_collator=data_collator,
    callbacks=callbacks
)

print("\nğŸš€ å¼€å§‹LORAå¾®è°ƒè®­ç»ƒ...")
trainer.train()

# ========================
# ä¿å­˜æ¨¡å‹
# ========================
final_lora_dir = os.path.join(OUTPUT_BASE, "medical-qwen-lora-final")
model.save_pretrained(final_lora_dir)
tokenizer.save_pretrained(final_lora_dir)
print(f"\nâœ… LoRA é€‚é…å™¨å·²ä¿å­˜è‡³: {final_lora_dir}")

# ========================
# å¯è§†åŒ–è®­ç»ƒlosså±•ç¤º
# ========================
if len(train_losses) > 0:
    plt.figure(figsize=(12, 5))

    # trainâ€”loss
    plt.subplot(1, 2, 1)
    plt.plot(steps, train_losses, label="Train Loss", marker='o', markersize=3)
    plt.title("Training Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # eval-loss
    if eval_losses:
        eval_steps_list = [training_args.eval_steps * (i+1) for i in range(len(eval_losses))]
        plt.subplot(1, 2, 2)
        plt.plot(eval_steps_list, eval_losses, label="Eval Loss", color="red", marker='s', markersize=3)
        plt.title("Evaluation Loss")
        plt.xlabel("Step")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    loss_save_path = os.path.join(OUTPUT_BASE, "logs", "loss_save.png")
    plt.savefig(loss_save_path, dpi=150)
    plt.close()
    print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜è‡³: {loss_save_path}")


print("ğŸ‰LORA å¾®è°ƒå®Œæˆï¼")