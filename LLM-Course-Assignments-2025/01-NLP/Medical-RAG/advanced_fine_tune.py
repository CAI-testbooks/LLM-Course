# advanced_fine_tune.py - é’ˆå¯¹ AutoDL çš„ä¼˜åŒ–å¾®è°ƒè„šæœ¬ï¼ˆQwen2.5-7B + LoRA + 4-bitï¼‰
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
import matplotlib.pyplot as plt
# ========================
# è·¯å¾„é…ç½®
# ========================
DATASET_DIR = "/root/autodl-tmp/Medical-RAG/dataset"
TRAIN_FILE = os.path.join(DATASET_DIR, "train_data_8k.json")
VAL_FILE = os.path.join(DATASET_DIR, "validation_data.json")
OUTPUT_BASE = "/root/autodl-tmp/Medical-RAG/Tune-model"

# æ£€æŸ¥è®­ç»ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(TRAIN_FILE):
    raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {TRAIN_FILE}")

has_validation = os.path.exists(VAL_FILE)
print("âœ… æ•°æ®æ–‡ä»¶æ£€æŸ¥å®Œæˆ")
print(f"  Train: {TRAIN_FILE}")
print(f"  Val:   {VAL_FILE} ({'å­˜åœ¨' if has_validation else 'ä¸å­˜åœ¨'})")

# ========================
# åŠ è½½æ•°æ®é›†ï¼ˆJSONLæ ¼å¼ï¼‰
# ========================
def load_jsonl_dataset(file_path):
    """åŠ è½½JSONLæ ¼å¼çš„æ•°æ®é›†"""
    return load_dataset("json", data_files=file_path, split="train")

train_dataset = load_jsonl_dataset(TRAIN_FILE)
val_dataset = load_jsonl_dataset(VAL_FILE) if has_validation else None

print("\nğŸ” è®­ç»ƒé›†ç¤ºä¾‹:")
print(train_dataset[0])
if val_dataset:
    print("\nğŸ” éªŒè¯é›†ç¤ºä¾‹:")
    print(val_dataset[0])

# æ£€æŸ¥æ•°æ®ç»“æ„
print("\nğŸ“Š æ•°æ®é›†ç»“æ„ä¿¡æ¯:")
print(f"è®­ç»ƒé›†åˆ—å: {train_dataset.column_names}")
print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
if val_dataset:
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")

# ========================
# æ¨¡å‹ä¸ Tokenizer
# ========================
MODEL_PATH = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    load_in_4bit=True,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
)
model.enable_input_require_grads()

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)
tokenizer.pad_token = tokenizer.eos_token

# ========================
# æ„å»º Qwen å¯¹è¯æ¨¡æ¿
# ========================
def format_qwen_prompt(question: str, answer: str) -> str:
    return f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—é—®ç­”åŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
{answer}<|im_end|>"""

def preprocess_function(examples):
    """å¤„ç†JSONLæ ¼å¼æ•°æ®çš„é¢„å¤„ç†å‡½æ•°"""
    batch_prompts = []
    
    # éå†æ¯ä¸ªæ ·æœ¬
    for i in range(len(examples.get("questions", []))):
        # è·å–é—®é¢˜åˆ—è¡¨
        questions_list = examples["questions"][i] if i < len(examples["questions"]) else []
        # è·å–ç­”æ¡ˆ
        answer = examples["answers"][i] if i < len(examples["answers"]) else ""
        
        # å¤„ç†é—®é¢˜åˆ—è¡¨
        if isinstance(questions_list, list) and len(questions_list) > 0:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªé—®é¢˜ä½œä¸ºä¸»è¦é—®é¢˜
            question = str(questions_list[0]).strip()
        else:
            question = ""
        
        # å¤„ç†ç­”æ¡ˆ
        answer = str(answer).strip()
        
        # è·³è¿‡ç©ºæ•°æ®
        if not question or not answer:
            continue
            
        # æ„å»ºprompt
        prompt = format_qwen_prompt(question, answer)
        batch_prompts.append(prompt)

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›ç©ºå­—å…¸
    if len(batch_prompts) == 0:
        return {"input_ids": [], "attention_mask": []}

    # åˆ†è¯å¤„ç†
    tokenized = tokenizer(
        batch_prompts,
        truncation=True,
        max_length=1024,
        padding="max_length",
        return_tensors="pt",
    )
    return tokenized

# ========================
# é¢„å¤„ç†æ•°æ®é›†
# ========================
print("\nğŸ”„ æ­£åœ¨é¢„å¤„ç†æ•°æ®...")
train_tokenized = train_dataset.map(
    preprocess_function,
    batched=True,
    num_proc=4,
    remove_columns=train_dataset.column_names,
    desc="Tokenizing train set"
).filter(lambda x: len(x["input_ids"]) > 0)

if val_dataset:
    val_tokenized = val_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=4,
        remove_columns=val_dataset.column_names,
        desc="Tokenizing validation set"
    ).filter(lambda x: len(x["input_ids"]) > 0)
else:
    val_tokenized = None

print(f"âœ… é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›† {len(train_tokenized)} æ¡ï¼ŒéªŒè¯é›† {len(val_tokenized) if val_tokenized else 0} æ¡")

# ========================
# LoRA é…ç½®
# ========================
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,
    lora_alpha=64,
    lora_dropout=0.1,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    init_lora_weights=False,
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ========================
# è®­ç»ƒå‚æ•°
# ========================
training_args = TrainingArguments(
    output_dir=os.path.join(OUTPUT_BASE, "medical-qwen-lora"),
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_steps=300,
    logging_steps=50,
    save_steps=300,
    evaluation_strategy="steps" if val_tokenized else "no",
    eval_steps=300,#æ¯ 300 æ­¥è¯„ä¼°ä¸€æ¬¡ï¼ˆå¯é€‰ï¼‰
    learning_rate=2e-4,
    fp16=True,
    logging_dir=os.path.join(OUTPUT_BASE, "logs"),
    save_total_limit=2,
    load_best_model_at_end=True if val_tokenized else False,
    metric_for_best_model="eval_loss" if val_tokenized else None,
    greater_is_better=False,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    gradient_checkpointing=True,
    report_to=["tensorboard"],

)

# ========================
# Data Collator
# ========================
# 
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# æµ‹è¯• collator
sample_batch = [train_tokenized[i] for i in range(min(2, len(train_tokenized)))]
try:
    batch = data_collator(sample_batch)
    print("âœ… Data collator æµ‹è¯•é€šè¿‡ï¼")
    print("Batch keys:", list(batch.keys()))
    print("input_ids shape:", batch["input_ids"].shape)
except Exception as e:
    print("âŒ Data collator æŠ¥é”™:", e)
    raise
# ========================
# å¯è§†åŒ–è®­ç»ƒloss
# ========================
from transformers import TrainerCallback

train_losses = []
eval_losses = []
steps = []
# å›è°ƒå‡½æ•°
class LossLoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            train_losses.append(logs["loss"])
            steps.append(state.global_step)

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is not None and "eval_loss" in metrics:
            eval_losses.append(metrics["eval_loss"])

loss_callback = LossLoggingCallback()



# ========================
# å¯åŠ¨è®­ç»ƒ
# ========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    data_collator=data_collator,
    callbacks=[loss_callback],  # â†â†â† æ–°å¢å›è°ƒï¼Œè¿›è¡Œlosså¯è§†åŒ–æ“ä½œ
)

print("\nğŸš€ å¼€å§‹LORAå¾®è°ƒè®­ç»ƒ...")
trainer.train()

# ========================
# ä¿å­˜æ¨¡å‹
# ========================
final_lora_dir = os.path.join(OUTPUT_BASE, "medical-qwen-lora-final")
model.save_pretrained(final_lora_dir)
tokenizer.save_pretrained(final_lora_dir)
print(f"\nâœ… LoRA é€‚é…å™¨å·²ä¿å­˜è‡³: {final_lora_dir}")


# ========================
# å¯è§†åŒ–è®­ç»ƒlosså±•ç¤º
# ========================
if len(train_losses) > 0:
    plt.figure(figsize=(12, 5))

    # trainâ€”loss
    plt.subplot(1, 2, 1)
    plt.plot(steps, train_losses, label="Train Loss", marker='o', markersize=3)
    plt.title("Training Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # eval-loss
    if eval_losses:
        # eval æ¯ eval_steps ä¸€æ¬¡ï¼Œä» eval_steps å¼€å§‹
        eval_steps_list = [i * training_args.eval_steps for i in range(1, len(eval_losses) + 1)]
        plt.subplot(1, 2, 2)
        plt.plot(eval_steps_list, eval_losses, label="Eval Loss", color="red", marker='s', markersize=3)
        plt.title("Evaluation Loss")
        plt.xlabel("Step")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    loss_save_path = os.path.join(OUTPUT_BASE, "logs", "loss_save.png")
    plt.savefig(loss_save_path, dpi=150)
    plt.close()  # é¿å…åœ¨ notebook ä¸­æ˜¾ç¤º
    print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜è‡³: {loss_save_path}")

print("ğŸ‰LORA å¾®è°ƒå®Œæˆï¼")