# advanced_fine_tune.py - é’ˆå¯¹ AutoDL çš„ä¼˜åŒ–å¾®è°ƒè„šæœ¬ï¼ˆQwen2.5-7B + LoRA + 4-bitï¼‰
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
import matplotlib.pyplot as plt
# ========================
# è·¯å¾„é…ç½®
# ========================
DATASET_DIR = "/root/autodl-tmp/Medical-RAG/dataset"
TRAIN_FILE = os.path.join(DATASET_DIR, "train_data_8k.json")
VAL_FILE = os.path.join(DATASET_DIR, "validation_data.json")
OUTPUT_BASE = "/root/autodl-tmp/Medical-RAG/Tune-model"

# æ£€æŸ¥è®­ç»ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(TRAIN_FILE):
    raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {TRAIN_FILE}")

has_validation = os.path.exists(VAL_FILE)
print("âœ… æ•°æ®æ–‡ä»¶æ£€æŸ¥å®Œæˆ")
print(f"  Train: {TRAIN_FILE}")
print(f"  Val:   {VAL_FILE} ({'å­˜åœ¨' if has_validation else 'ä¸å­˜åœ¨'})")

# ========================
# åŠ è½½æ•°æ®é›†
# ========================
def load_json_dataset(file_path):
    return load_dataset("json", data_files=file_path)["train"]

train_dataset = load_json_dataset(TRAIN_FILE)
val_dataset = load_json_dataset(VAL_FILE) if has_validation else None

print("\nğŸ” è®­ç»ƒé›†ç¤ºä¾‹:")
print(train_dataset[0])
if val_dataset:
    print("\nğŸ” éªŒè¯é›†ç¤ºä¾‹:")
    print(val_dataset[0])

# ========================
# æ¨¡å‹ä¸ Tokenizer
# ========================
MODEL_PATH = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
    use_cache=False,
)
model.enable_input_require_grads()

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ========================
# æ„å»º Qwen å¯¹è¯æ¨¡æ¿
# ========================
def format_qwen_prompt(question: str, answer: str) -> str:
    return f"system\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—é—®ç­”åŠ©æ‰‹ã€‚\nuser\n{question}\assistant\n{answer}"

def preprocess_function(examples):
    questions = examples.get("questions", [])
    answers = examples.get("answers", [])

    batch_prompts = []
    for i in range(len(questions)):
        # å®‰å…¨æå– question
        q = questions[i]
        if isinstance(q, list):
            q = q[0] if len(q) > 0 else ""
        q = str(q).strip()

        # å®‰å…¨æå– answer
        a = answers[i] if i < len(answers) else ""
        if isinstance(a, list):
            a = a[0] if len(a) > 0 else ""
        a = str(a).strip()

        if not q or not a:
            batch_prompts.append("")
        else:
            batch_prompts.append(format_qwen_prompt(q, a))

    # ä¿®æ”¹ï¼šç¡®ä¿paddingå’Œtruncationè®¾ç½®æ­£ç¡®
    tokenized = tokenizer(
        batch_prompts,
        truncation=True,
        max_length=1028,
        padding="max_length",  # å…ˆç”¨å›ºå®šé•¿åº¦ paddingï¼Œé¿å… collator å‡ºé”™
        return_tensors="pt",  # ç›´æ¥è¿”å› tensor
    )
    return tokenized

# ========================
# é¢„å¤„ç†æ•°æ®é›†
# ========================
print("\nğŸ”„ æ­£åœ¨é¢„å¤„ç†æ•°æ®...")
train_tokenized = train_dataset.map(
    preprocess_function,
    batched=True,
    num_proc=4,
    remove_columns=train_dataset.column_names,
    desc="Tokenizing train set"
).filter(lambda x: len(x["input_ids"]) > 0)

if val_dataset:
    val_tokenized = val_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=4,
        remove_columns=val_dataset.column_names,
        desc="Tokenizing validation set"
    ).filter(lambda x: len(x["input_ids"]) > 0)
else:
    val_tokenized = None

print(f"âœ… é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›† {len(train_tokenized)} æ¡ï¼ŒéªŒè¯é›† {len(val_tokenized) if val_tokenized else 0} æ¡")

# ========================
# LoRA é…ç½®
# ========================
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,
    lora_alpha=64,
    lora_dropout=0.1,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    init_lora_weights=False,
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ========================
# è®­ç»ƒå‚æ•°
# ========================
training_args = TrainingArguments(
    output_dir=os.path.join(OUTPUT_BASE, "medical-qwen-lora"),
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_steps=100,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="steps" if val_tokenized else "no",
    eval_steps=100,#æ¯ 500 æ­¥è¯„ä¼°ä¸€æ¬¡ï¼ˆå¯é€‰ï¼‰
    learning_rate=2e-4,
    fp16=True,
    logging_dir=os.path.join(OUTPUT_BASE, "logs"),
    save_total_limit=2,
    load_best_model_at_end=True if val_tokenized else False,
    metric_for_best_model="eval_loss" if val_tokenized else None,
    greater_is_better=False,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    gradient_checkpointing=True,
    report_to=["tensorboard"],

)

# ========================
# Data Collator
# ========================
# 
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# æµ‹è¯• collator
sample_batch = [train_tokenized[i] for i in range(min(2, len(train_tokenized)))]
try:
    batch = data_collator(sample_batch)
    print("âœ… Data collator æµ‹è¯•é€šè¿‡ï¼")
    print("Batch keys:", list(batch.keys()))
    print("input_ids shape:", batch["input_ids"].shape)
except Exception as e:
    print("âŒ Data collator æŠ¥é”™:", e)
    raise
# ========================
# å¯è§†åŒ–è®­ç»ƒloss
# ========================
from transformers import TrainerCallback

train_losses = []
eval_losses = []
steps = []
# å›è°ƒå‡½æ•°
class LossLoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            train_losses.append(logs["loss"])
            steps.append(state.global_step)

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is not None and "eval_loss" in metrics:
            eval_losses.append(metrics["eval_loss"])

loss_callback = LossLoggingCallback()



# ========================
# å¯åŠ¨è®­ç»ƒ
# ========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    data_collator=data_collator,
    callbacks=[loss_callback],  # â†â†â† æ–°å¢å›è°ƒï¼Œè¿›è¡Œlosså¯è§†åŒ–æ“ä½œ
)

print("\nğŸš€ å¼€å§‹LORAå¾®è°ƒè®­ç»ƒ...")
trainer.train()

# ========================
# ä¿å­˜æ¨¡å‹
# ========================
final_lora_dir = os.path.join(OUTPUT_BASE, "medical-qwen-lora-final")
model.save_pretrained(final_lora_dir)
tokenizer.save_pretrained(final_lora_dir)
print(f"\nâœ… LoRA é€‚é…å™¨å·²ä¿å­˜è‡³: {final_lora_dir}")


# ========================
# å¯è§†åŒ–è®­ç»ƒlosså±•ç¤º
# ========================
if len(train_losses) > 0:
    plt.figure(figsize=(12, 5))

    # trainâ€”loss
    plt.subplot(1, 2, 1)
    plt.plot(steps, train_losses, label="Train Loss", marker='o', markersize=3)
    plt.title("Training Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    # eval-loss
    if eval_losses:
        # eval æ¯ eval_steps ä¸€æ¬¡ï¼Œä» eval_steps å¼€å§‹
        eval_steps_list = [i * training_args.eval_steps for i in range(1, len(eval_losses) + 1)]
        plt.subplot(1, 2, 2)
        plt.plot(eval_steps_list, eval_losses, label="Eval Loss", color="red", marker='s', markersize=3)
        plt.title("Evaluation Loss")
        plt.xlabel("Step")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    loss_save_path = os.path.join(final_lora_dir, "loss_save.png")
    plt.savefig(loss_save_path, dpi=150)
    plt.close()  # é¿å…åœ¨ notebook ä¸­æ˜¾ç¤º
    print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜è‡³: {loss_save_path}")

print("ğŸ‰LORA å¾®è°ƒå®Œæˆï¼")