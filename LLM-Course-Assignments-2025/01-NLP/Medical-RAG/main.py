import os
import sys

# å¯¼å…¥å¿…è¦çš„åº“
# æ•™å­¦ç‚¹ï¼šè¿™é‡Œå±•ç¤ºäº† LangChain çš„æ¨¡å—åŒ–è®¾è®¡ (åŠ è½½å™¨ã€åˆ‡å‰²å™¨ã€å‘é‡åº“ã€æ¨¡å‹)
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ==========================================
# é…ç½®åŒºåŸŸ (è¿™é‡Œæ˜¯å­¦ç”Ÿå”¯ä¸€éœ€è¦ä¿®æ”¹çš„åœ°æ–¹)
# ==========================================

# 1. è®¾ç½® API Key (å»ºè®®ä½¿ç”¨ ç¡…åŸºæµåŠ¨ æˆ– DeepSeek)
# è¯·å°†ä¸‹é¢çš„ 'sk-xxxxxxxx' æ›¿æ¢ä¸ºä½ ç”³è¯·åˆ°çš„çœŸå® Key
os.environ["OPENAI_API_KEY"] = "sk-gyuofotkkugmqvlmcuchjdzmipktruzczqvqtqyiyfqbqvsu"

# 2. è®¾ç½® Base URL (æŒ‡å‘å›½å†…ä¸­è½¬æœåŠ¡ï¼Œæ— éœ€ VPN)
os.environ["OPENAI_API_BASE"] = "https://api.siliconflow.cn/v1"

# 3. å®šä¹‰æ¨¡å‹åç§°
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"  # å¤§è¯­è¨€æ¨¡å‹
EMBEDDING_MODEL = "BAAI/bge-m3"  # å‘é‡æ¨¡å‹


# ==========================================
# ä¸»ç¨‹åºé€»è¾‘
# ==========================================

def main():
    print("-" * 50)
    print("ğŸ¥ ä¸­æ–‡åŒ»ç–—é¢†åŸŸæ™ºèƒ½é—®ç­”ç³»ç»Ÿå¯åŠ¨...")
    print("-" * 50)

    # ---------------------------------------------------
    # ç¬¬ä¸€æ­¥ï¼šæ•°æ®åŠ è½½ (Load)
    # æ•™å­¦è®²è§£ï¼šå°†éç»“æ„åŒ–çš„æ–‡æœ¬æ–‡ä»¶åŠ è½½åˆ°å†…å­˜ä¸­
    # ---------------------------------------------------
    print(f"[1/5] æ­£åœ¨åŠ è½½åŒ»ç–—çŸ¥è¯†åº“ knowledge.txt ...")
    try:
        loader = TextLoader("./knowledge.txt", encoding="utf-8")
        docs = loader.load()
    except FileNotFoundError:
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° knowledge.txt æ–‡ä»¶ï¼Œè¯·å…ˆç”ŸæˆåŒ»ç–—çŸ¥è¯†åº“æ•°æ®æ–‡ä»¶ï¼")
        return

    # ---------------------------------------------------
    # ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬åˆ‡åˆ† (Split)
    # æ•™å­¦è®²è§£ï¼šä¸ºäº†é€‚åº” LLM çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œæˆ‘ä»¬éœ€è¦æŠŠé•¿æ–‡æœ¬åˆ‡æˆå°å—
    # ---------------------------------------------------
    print(f"[2/5] æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,  # æ¯ä¸ªå—çš„å¤§å°
        chunk_overlap=50  # å—ä¹‹é—´çš„é‡å éƒ¨åˆ†ï¼Œé˜²æ­¢è¯­ä¹‰ä¸­æ–­
    )
    splits = text_splitter.split_documents(docs)
    print(f"      >>> æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ")

    # ---------------------------------------------------
    # ç¬¬ä¸‰æ­¥ï¼šå‘é‡åŒ–ä¸å­˜å‚¨ (Embed & Store)
    # æ•™å­¦è®²è§£ï¼šå°†æ–‡æœ¬ç‰‡æ®µè½¬åŒ–ä¸ºå‘é‡ï¼Œå¹¶å­˜å…¥ ChromaDB æ•°æ®åº“
    # ---------------------------------------------------
    print(f"[3/5] æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼• (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")

    # å®šä¹‰å‘é‡æ¨¡å‹ (Embedding)
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    # å­˜å…¥å‘é‡åº“
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings
    )
    print("      >>> å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")

    # ---------------------------------------------------
    # ç¬¬å››æ­¥ï¼šæ„å»º RAG é“¾ (Chain)
    # æ•™å­¦è®²è§£ï¼šå°† æ£€ç´¢(Retriever) + æç¤ºè¯(Prompt) + æ¨¡å‹(LLM) ä¸²è”èµ·æ¥
    # ---------------------------------------------------
    print(f"[4/5] æ­£åœ¨åˆå§‹åŒ–å¤§æ¨¡å‹ ({MODEL_NAME})...")

    # 1. å®šä¹‰æ£€ç´¢å™¨ (åªæŸ¥æ‰¾æœ€ç›¸å…³çš„ 3 æ¡ä¿¡æ¯)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # 2. å®šä¹‰å¤§è¯­è¨€æ¨¡å‹
    llm = ChatOpenAI(
        model_name=MODEL_NAME,
        temperature=0.1  # æ¸©åº¦è¶Šä½ï¼Œå›ç­”è¶Šä¸¥è°¨
    )

    # 3. å®šä¹‰ Prompt æ¨¡æ¿
    template = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ã€åŒ»å­¦çŸ¥è¯†ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´"æ ¹æ®ç°æœ‰åŒ»å­¦èµ„æ–™ï¼Œæˆ‘æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ"ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚

    ã€åŒ»å­¦çŸ¥è¯†ã€‘ï¼š
    {context}

    ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 4. ç»„è£… RAG é“¾ (LangChain LCEL è¯­æ³•)
    rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    # ---------------------------------------------------
    # ç¬¬äº”æ­¥ï¼šæ¼”ç¤ºæé—® (Invoke)
    # æ•™å­¦è®²è§£ï¼šå®é™…è¿è¡Œæ•ˆæœå±•ç¤º
    # ---------------------------------------------------
    print("-" * 50)

    # æµ‹è¯•é—®é¢˜ 1
    query1 = "é«˜è¡€å‹çš„å¸¸è§ç—‡çŠ¶å’Œæ²»ç–—æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ"
    print(f"ğŸ‘¤ æé—®: {query1}")
    print("ğŸ¤– æ€è€ƒä¸­...", end="", flush=True)
    response1 = rag_chain.invoke(query1)
    print(f"\rğŸ¥ å›ç­”: {response1}\n")

    print("-" * 30)

    # æµ‹è¯•é—®é¢˜ 2 (æµ‹è¯•ç´§æ€¥æƒ…å†µ)
    query2 = "å¦‚æœé‡åˆ°å¿ƒè„ç—…å‘ä½œçš„ç´§æ€¥æƒ…å†µï¼Œåº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿ"
    print(f"ğŸ‘¤ æé—®: {query2}")
    print("ğŸ¤– æ€è€ƒä¸­...", end="", flush=True)
    response2 = rag_chain.invoke(query2)
    print(f"\rğŸ¥ å›ç­”: {response2}\n")

    print("-" * 50)
    print("âœ… å®éªŒæ¼”ç¤ºç»“æŸ")


if __name__ == "__main__":
    main()