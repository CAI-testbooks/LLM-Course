import os
import json
import torch
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ===================== é…ç½®åŒº (å’Œä½ çš„RAGä¸»ä»£ç å®Œå…¨ä¸€è‡´ï¼Œä¸è¦ä¿®æ”¹) =====================
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
MODEL_NAME = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"
EMBEDDING_MODEL = "BAAI/bge-m3"
VECTOR_DB_PATH = "/root/autodl-tmp/Medical-RAG/chroma_db_medical"
TEST_DATA_PATH = "/root/autodl-tmp/Medical-RAG/dataset/alpaca_formatted_test_data.json"
OUTPUT_DIR = "/root/autodl-tmp/Medical-RAG/eval_results"
OUTPUT_FILE = "rag_results.json"  # RAGè¯„ä¼°æ–‡ä»¶æœ€ç»ˆåç§°

# ===================== å¯¼å…¥RAGæ‰€éœ€ä¾èµ– =====================
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# ===================== åˆå§‹åŒ–å®Œæ•´çš„RAGç³»ç»Ÿ (å¤åˆ»ä½ çš„åŸç‰ˆä»£ç ï¼Œæ— ä¿®æ”¹) =====================
def initialize_rag_chain():
    """åˆå§‹åŒ–å’Œä¸»ä»£ç ä¸€æ¨¡ä¸€æ ·çš„RAGé“¾ï¼Œæ— UIç›¸å…³ä»£ç """
    # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True}
    )

    # 2. åŠ è½½æœ¬åœ°å·²æ„å»ºå¥½çš„å‘é‡åº“ï¼ˆå¿…é¡»æå‰è¿è¡Œä¸»ä»£ç æ„å»ºå®Œæˆï¼‰
    vectorstore = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # 3. åŠ è½½åˆ†è¯å™¨å’Œå¤§æ¨¡å‹ (å’Œä¸»ä»£ç é…ç½®å®Œå…¨ä¸€è‡´)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )

    # 4. åˆ›å»ºç”Ÿæˆpipeline (å’Œä¸»ä»£ç é…ç½®å®Œå…¨ä¸€è‡´)
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.1,
        top_p=0.95,
        repetition_penalty=1.1,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        clean_up_tokenization_spaces=True,
        early_stopping=True
    )

    # 5. å°è£…LLMå’ŒPromptæ¨¡æ¿ (å’Œä¸»ä»£ç å®Œå…¨ä¸€è‡´)
    llm_pipeline = HuggingFacePipeline(pipeline=pipe)
    llm = ChatHuggingFace(llm=llm_pipeline, tokenizer=tokenizer, streaming=False)
    
    template = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦åŠ©æ‰‹ã€‚
    å›ç­”è¦æ±‚ï¼š1. æ¡ç†æ¸…æ™°ï¼›2. ç¦æ­¢é‡å¤è¡¨è¿°ï¼›3. ç”Ÿæˆç­”æ¡ˆæ—¶ï¼Œä¸åšå†—ä½™æ¨ç†
    å¦‚æœä¸çŸ¥é“ï¼Œè¯·ç›´æ¥è¯´"æ ¹æ®ç°æœ‰åŒ»å­¦èµ„æ–™ï¼Œæˆ‘æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ"ã€‚

    åŒ»å­¦çŸ¥è¯†ï¼š
    {context}

    ç”¨æˆ·é—®é¢˜ï¼š
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 6. æ„å»ºæœ€ç»ˆRAGé“¾
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå‘é‡åº“+å¤§æ¨¡å‹åŠ è½½æˆåŠŸ")
    return rag_chain

# ===================== æ‰¹é‡ç”ŸæˆRAGç­”æ¡ˆ =====================
def generate_rag_answer(rag_chain, instruction, input_text=""):
    query = f"{instruction}\n{input_text}".strip() if input_text else instruction
    full_output = rag_chain.invoke(query)
    
    # æå– assistant çš„å›ç­”éƒ¨åˆ†
    if "<|im_start|>assistant" in full_output:
        answer = full_output.split("<|im_start|>assistant")[-1].strip()
        # å»æ‰å¯èƒ½æ®‹ç•™çš„ <|im_end|>
        if "<|im_end|>" in answer:
            answer = answer.split("<|im_end|>")[0].strip()
        return answer
    else:
        return full_output.strip()

# ===================== ä¸»è¯„ä¼°æµç¨‹ =====================
def main():
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)

    # 1. åŠ è½½æµ‹è¯•é›†æ•°æ®
    print(f"ğŸ“„ åŠ è½½æµ‹è¯•é›†æ•°æ®: {TEST_DATA_PATH}")
    with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    print(f"âœ… å…±åŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")

    # 2. åˆå§‹åŒ–RAGé“¾
    rag_chain = initialize_rag_chain()

    # 3. æ‰¹é‡æ¨ç† + ç»“æœä¿å­˜
    results = [] 
    print("\nğŸš€ å¼€å§‹æ‰§è¡ŒRAGæ‰¹é‡è¯„ä¼°ï¼ˆæ£€ç´¢çŸ¥è¯†åº“+ç”Ÿæˆç­”æ¡ˆï¼‰...")
    for item in tqdm(test_data, desc="RAGè¯„ä¼°è¿›åº¦", ncols=100):
        instruction = item["instruction"]
        input_text = item.get("input", "").strip()
        reference = item["output"].strip()

        answer = generate_rag_answer(rag_chain, instruction, input_text)
        results.append({
            "instruction": instruction,
            "input": input_text,
            "reference": reference,
            "answer": answer
        })

    # 4. ä¿å­˜æœ€ç»ˆçš„RAGè¯„ä¼°æ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼RAGè¯„ä¼°æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path}")
    print(f"ğŸ“Š æ–‡ä»¶åŒ…å« {len(results)} æ¡æ•°æ®ï¼Œå­—æ®µä¸baseè¯„ä¼°å®Œå…¨ä¸€è‡´ï¼")

if __name__ == "__main__":
    main()