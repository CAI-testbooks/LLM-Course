#!/usr/bin/env python
# merge_model.py - å°† LoRA é€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ========================
# é…ç½®å‚æ•°
# ========================
# è®¾ç½®é•œåƒæº
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¾®è°ƒå‰çš„åŸå§‹æ¨¡å‹ï¼‰
BASE_MODEL_PATH = "/root/autodl-tmp/qwen/Qwen2.5-7B-Instruct"

# LoRA é€‚é…å™¨è·¯å¾„ï¼ˆå¾®è°ƒåä¿å­˜çš„ LoRA æƒé‡ï¼‰
LORA_ADAPTER_PATH = "/root/autodl-tmp/Medical-RAG/Tune-model/medical-qwen-lora-final"

# åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„
MERGED_MODEL_PATH = "/root/autodl-tmp/Medical-RAG/Tune-model/medical-qwen-merged"

# ========================
# åˆå¹¶ LoRA æƒé‡
# ========================
def merge_lora_weights():
    print("ğŸ” åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    print("ğŸ” åŠ è½½ LoRA é€‚é…å™¨...")
    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
    
    print("ğŸ”§ åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
    merged_model = model.merge_and_unload()
    
    print("ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
    merged_model.save_pretrained(
        MERGED_MODEL_PATH,
        safe_serialization=True,  # ä½¿ç”¨å®‰å…¨åºåˆ—åŒ–ä¿å­˜
        max_shard_size="5GB"      # åˆ†ç‰‡ä¿å­˜ï¼Œé¿å…å•ä¸ªæ–‡ä»¶è¿‡å¤§
    )
    
    # ä¿å­˜ tokenizerï¼ˆé€šå¸¸ä¸åŸºç¡€æ¨¡å‹ç›¸åŒï¼‰
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    tokenizer.save_pretrained(MERGED_MODEL_PATH)
    
    print(f"âœ… åˆå¹¶å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {MERGED_MODEL_PATH}")
    return merged_model

def main():
    print("ğŸš€ å¼€å§‹åˆå¹¶ LoRA æƒé‡...")
    print(f"  åŸºç¡€æ¨¡å‹è·¯å¾„: {BASE_MODEL_PATH}")
    print(f"  LoRA é€‚é…å™¨è·¯å¾„: {LORA_ADAPTER_PATH}")
    print(f"  åˆå¹¶åä¿å­˜è·¯å¾„: {MERGED_MODEL_PATH}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(BASE_MODEL_PATH):
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {BASE_MODEL_PATH}")
    
    if not os.path.exists(LORA_ADAPTER_PATH):
        raise FileNotFoundError(f"LoRA é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {LORA_ADAPTER_PATH}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(MERGED_MODEL_PATH, exist_ok=True)
    
    # æ‰§è¡Œåˆå¹¶
    merged_model = merge_lora_weights()
    
    print("\nâœ… æ¨¡å‹åˆå¹¶æˆåŠŸï¼")
    print(f"  - åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜è‡³: {MERGED_MODEL_PATH}")
    print("  - ç°åœ¨ä½ å¯ä»¥ç›´æ¥åŠ è½½åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œæ¨ç†")

if __name__ == "__main__":
    main()