#!/usr/bin/env python
# merge_model.py - å°† LoRA é€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼ˆé˜²ä¹±ç ç‰ˆï¼‰
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ========================
# é…ç½®å‚æ•°
# ========================
# è®¾ç½®é•œåƒæº
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¾®è°ƒå‰çš„åŸå§‹æ¨¡å‹ï¼‰
BASE_MODEL_PATH = "/root/autodl-tmp/qwen/Qwen2.5-7B-Instruct"

# LoRA é€‚é…å™¨è·¯å¾„ï¼ˆå¾®è°ƒåä¿å­˜çš„ LoRA æƒé‡ï¼‰
LORA_ADAPTER_PATH = "/root/autodl-tmp/Medical-RAG/Tune-model/medical-qwen-lora-final"

# åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„
MERGED_MODEL_PATH = "/root/autodl-tmp/Medical-RAG/Tune-model/medical-qwen-merged"

# ========================
# åˆå¹¶ LoRA æƒé‡ï¼ˆé˜²ä¹±ç ç‰ˆï¼‰
# ========================
def merge_lora_weights():
    print("ğŸ” åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizer...")
    
    # å…ˆåŠ è½½tokenizerï¼Œç¡®ä¿ç¼–ç æ­£ç¡®
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL_PATH, 
        trust_remote_code=True,
        use_fast=False,  # ä½¿ç”¨æ…¢é€Ÿä½†æ›´å‡†ç¡®çš„tokenizer
        padding_side="left"
    )
    
    # ç¡®ä¿pad_tokenè®¾ç½®æ­£ç¡®
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    print("ğŸ” åŠ è½½ LoRA é€‚é…å™¨...")
    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
    
    print("ğŸ”§ åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
    merged_model = model.merge_and_unload()
    
    print("ğŸ” æµ‹è¯•åˆå¹¶åæ¨¡å‹çš„ä¸­æ–‡ç”Ÿæˆèƒ½åŠ›...")
    # æµ‹è¯•ä¸­æ–‡ç”Ÿæˆ
    test_prompt = "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—é—®ç­”åŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\nä»€ä¹ˆæ˜¯é«˜è¡€å‹ï¼Ÿ<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(test_prompt, return_tensors="pt", truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = merged_model.generate(
            inputs.input_ids.to(merged_model.device),
            max_new_tokens=200,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
    print(f"æµ‹è¯•ç”Ÿæˆç»“æœ: {generated_text}")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
    if any('\u4e00' <= char <= '\u9fff' for char in generated_text):
        print("âœ… ä¸­æ–‡ç”Ÿæˆæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸ è­¦å‘Šï¼šç”Ÿæˆç»“æœå¯èƒ½ä¸åŒ…å«ä¸­æ–‡å­—ç¬¦")
    
    print("ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹å’Œtokenizer...")
    
    # ä¿å­˜æ¨¡å‹
    merged_model.save_pretrained(
        MERGED_MODEL_PATH,
        safe_serialization=True,
        max_shard_size="5GB"
    )
    
    # ä¿å­˜tokenizerï¼ˆä½¿ç”¨å¾®è°ƒæ—¶ä½¿ç”¨çš„tokenizerè®¾ç½®ï¼‰
    tokenizer.save_pretrained(MERGED_MODEL_PATH)
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶ï¼Œç¡®ä¿åŠ è½½æ—¶ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
    config_file = os.path.join(MERGED_MODEL_PATH, "config.json")
    if os.path.exists(config_file):
        import json
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # ç¡®ä¿é…ç½®ä¸­åŒ…å«ä¸­æ–‡æ”¯æŒç›¸å…³å‚æ•°
        config["trust_remote_code"] = True
        config["torch_dtype"] = "float16"
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… åˆå¹¶å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {MERGED_MODEL_PATH}")
    return merged_model, tokenizer

def main():
    print("ğŸš€ å¼€å§‹åˆå¹¶ LoRA æƒé‡...")
    print("âš ï¸ æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬åŒ…å«é˜²ä¹±ç æªæ–½")
    print(f"  åŸºç¡€æ¨¡å‹è·¯å¾„: {BASE_MODEL_PATH}")
    print(f"  LoRA é€‚é…å™¨è·¯å¾„: {LORA_ADAPTER_PATH}")
    print(f"  åˆå¹¶åä¿å­˜è·¯å¾„: {MERGED_MODEL_PATH}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(BASE_MODEL_PATH):
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {BASE_MODEL_PATH}")
    
    if not os.path.exists(LORA_ADAPTER_PATH):
        raise FileNotFoundError(f"LoRA é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {LORA_ADAPTER_PATH}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(MERGED_MODEL_PATH, exist_ok=True)
    
    # æ‰§è¡Œåˆå¹¶
    merged_model, tokenizer = merge_lora_weights()
    
    print("\nâœ… æ¨¡å‹åˆå¹¶æˆåŠŸï¼é˜²ä¹±ç æªæ–½å·²åº”ç”¨")
    print(f"  - åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜è‡³: {MERGED_MODEL_PATH}")
    print("  - tokenizer å·²æ­£ç¡®é…ç½®ä¸­æ–‡ç¼–ç ")
    print("  - ç°åœ¨ä½ å¯ä»¥ç›´æ¥åŠ è½½åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œæ¨ç†")
    
    # éªŒè¯åŠ è½½
    print("\nğŸ” éªŒè¯åˆå¹¶åæ¨¡å‹çš„åŠ è½½...")
    try:
        test_model = AutoModelForCausalLM.from_pretrained(
            MERGED_MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        test_tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH, trust_remote_code=True)
        print("âœ… åˆå¹¶åæ¨¡å‹åŠ è½½éªŒè¯é€šè¿‡ï¼")
    except Exception as e:
        print(f"âŒ åŠ è½½éªŒè¯å¤±è´¥: {e}")

if __name__ == "__main__":
    main()