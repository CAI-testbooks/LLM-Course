import streamlit as st
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import json
import time
import torch
from typing import List, Dict, Any
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSequenceClassification
import gc

# ==========================================
# ç¯å¢ƒä¸å¸¸é‡
# ==========================================

ST_TITLE = "é€šç”¨ä¸­æ–‡åŒ»ç–—é¢†åŸŸæ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
#MODEL_NAME = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"
MODEL_NAME = "/root/autodl-tmp/Medical-RAG/Tune-model/medical-qwen-merged"  # ä¿®æ”¹ä¸ºmerageåçš„æ¨¡å‹è·¯å¾„
EMBEDDING_MODEL = "BAAI/bge-m3"
VECTOR_DB_PATH = "/root/autodl-tmp/Medical-RAG/chroma_db_medical"
DATASET_DIR = "/root/autodl-tmp/Medical-RAG/dataset"

# æ£€ç´¢ä¸é‡æ’åºå‚æ•°
MMR_FETCH_K = 15
RERANKER_RETRIEVE_K = 10
RERANKER_TOP_K = 3
RERANKER_MODEL = "BAAI/bge-reranker-v2-m3"
SCORE_THRESHOLD = 2.0

# ==========================================
# å·¥å…·å‡½æ•°
# ==========================================

def load_alpaca_json_as_documents(file_path):
    docs = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data_list = json.load(f)
        for idx, item in enumerate(data_list, 1):
            instruction = item.get("instruction", "").strip()
            output = item.get("output", "").strip()
            if not instruction or not output:
                continue
            page_content = f"é—®é¢˜ï¼š{instruction}\nç­”æ¡ˆï¼š{output}"
            metadata = {
                "source_instruction": instruction,
                "source_file": os.path.basename(file_path),
                "item_index": idx,
            }
            docs.append(Document(page_content=page_content, metadata=metadata))
    except Exception as e:
        st.error(f"åŠ è½½ {file_path} å‡ºé”™: {e}")
        return []
    return docs


def normalize_messages(messages):
    """å°† messages è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼åˆ—è¡¨ï¼Œè¿‡æ»¤æ‰éæ³•é¡¹ã€‚"""
    if not isinstance(messages, (list, tuple)):
        return []
    normalized = []
    for msg in messages:
        if not isinstance(msg, dict):
            continue
        role = msg.get("role")
        content = msg.get("content")
        if role in ("user", "assistant") and isinstance(content, str) and content.strip():
            normalized.append({"role": role, "content": content.strip()})
    return normalized


def truncate_chat_history(messages, tokenizer, max_tokens=1000):
    """æŒ‰ token æˆªæ–­å¯¹è¯å†å²ï¼ˆä»æœ€æ–°å¾€å›å–ï¼‰"""
    messages = normalize_messages(messages)
    if not messages:
        return ""

    formatted_lines = []
    total_tokens = 0
    for msg in reversed(messages):
        role_name = "æ‚£è€…" if msg["role"] == "user" else "åŒ»ç”Ÿ"
        line = f"{role_name}ï¼š{msg['content']}"
        tokens = len(tokenizer.encode(line, add_special_tokens=False))
        if total_tokens + tokens > max_tokens:
            break
        formatted_lines.append(line)
        total_tokens += tokens
    return "\n".join(reversed(formatted_lines))


# ==========================================
# å¢å¼ºç‰ˆ BGE-Reranker
# ==========================================
class BGERReranker:
    def __init__(self, model_name="BAAI/bge-reranker-v2-m3", device="cuda"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device).eval()
        self.device = device

    def rerank(self, query: str, documents: List[Document], top_k: int = 3, threshold: float = SCORE_THRESHOLD):
        if not documents:
            return [], True
        
        pairs = [[query, doc.page_content] for doc in documents]
        with torch.no_grad():
            inputs = self.tokenizer(
                pairs,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512
            ).to(self.device)
            scores = self.model(**inputs, return_dict=True).logits.view(-1, ).float()

        scored_docs = list(zip(documents, scores.cpu().numpy()))
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        if scored_docs and scored_docs[0][1] < threshold:
            return [], True

        top_docs = [doc for doc, _ in scored_docs[:top_k]]
        return top_docs, False


# ==========================================
# åˆå§‹åŒ– RAG ç³»ç»Ÿ
# ==========================================
@st.cache_resource
def initialize_rag_system():
    if not os.path.exists(DATASET_DIR):
        return None, f"æ‰¾ä¸åˆ°æ•°æ®é›†ç›®å½•: {DATASET_DIR}"

    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True}
    )

    if os.path.exists(VECTOR_DB_PATH):
        vectorstore = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
    else:
        json_files = [
            os.path.join(DATASET_DIR, "alpaca_formatted_test_data.json"),
            os.path.join(DATASET_DIR, "alpaca_formatted_validation_data.json"),
            os.path.join(DATASET_DIR, "alpaca_formatted_train_data.json")
        ]
        docs = []
        for file_path in json_files:
            if os.path.exists(file_path):
                docs.extend(load_alpaca_json_as_documents(file_path))
        if not docs:
            return None, "æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£"
        splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        splits = splitter.split_documents(docs)
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=VECTOR_DB_PATH
        )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

    def get_mmr_lambda(query: str) -> float:
        high_risk_keywords = ["æ€¥", "ç«‹å³", "æ€¥æ•‘", "ç”¨è¯", "å‰‚é‡", "æ‰‹æœ¯", "è¿‡æ•", "æ­»äº¡"]
        if any(kw in query for kw in high_risk_keywords):
            return 0.95
        else:
            return 0.6

    def mmr_retrieve_with_dynamic_lambda(query: str):
        lambda_mult = get_mmr_lambda(query)
        search_kwargs = {
            "k": RERANKER_RETRIEVE_K,
            "fetch_k": MMR_FETCH_K,
            "lambda_mult": lambda_mult
        }
        retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs=search_kwargs)
        return retriever.invoke(query)

    reranker = BGERReranker(model_name=RERANKER_MODEL, device="cuda")

    def retrieve_and_rerank(inputs: Dict[str, Any]) -> str:
        query = inputs["question"]
        chat_history_str = inputs.get("chat_history", "")
        context_query = f"{chat_history_str}\nå½“å‰é—®é¢˜ï¼š{query}".strip() if chat_history_str else query

        docs = mmr_retrieve_with_dynamic_lambda(query)
        top_docs, is_unanswerable = reranker.rerank(context_query, docs, top_k=RERANKER_TOP_K)
        
        if is_unanswerable or not top_docs:
            return "æ ¹æ®ç°æœ‰åŒ»å­¦èµ„æ–™ï¼Œæˆ‘æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚"
        
        return "\n\n".join([f"ã€å‚è€ƒç‰‡æ®µ {i+1}ã€‘\n{doc.page_content}" for i, doc in enumerate(top_docs)])

    retriever_runnable = RunnableLambda(retrieve_and_rerank)

    # LLM åˆå§‹åŒ–
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.1,
        top_p=0.95,
        repetition_penalty=1.1,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        clean_up_tokenization_spaces=True,
        early_stopping=True
    )
    from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
    llm_pipeline = HuggingFacePipeline(pipeline=pipe)
    llm = ChatHuggingFace(llm=llm_pipeline, tokenizer=tokenizer, streaming=True)

    template = """
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¸­æ–‡åŒ»å­¦ä¸“å®¶åŠ©æ‰‹ã€‚
è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹â€œåŒ»å­¦çŸ¥è¯†â€ä½œç­”ï¼Œä¸å¾—ç¼–é€ ã€æ¨æµ‹æˆ–å¼•å…¥å¤–éƒ¨çŸ¥è¯†ã€‚
è‹¥åŒ»å­¦çŸ¥è¯†ä¸­æ— ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰åŒ»å­¦èµ„æ–™ï¼Œæˆ‘æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚â€

å›ç­”è¦æ±‚ï¼š
1. è‹¥æ¶‰åŠæ€¥ç—‡ã€ç”¨è¯ã€æ‰‹æœ¯ã€è¿‡æ•ç­‰é«˜é£é™©å†…å®¹ï¼Œå¼€å¤´å¿…é¡»åŠ ï¼š
   â€œâš ï¸ æ³¨æ„ï¼šæ­¤ä¿¡æ¯ä¸èƒ½æ›¿ä»£ç´§æ€¥åŒ»ç–—æ•‘åŠ©ï¼Œè¯·ç«‹å³è”ç³»åŒ»ç”Ÿæˆ–å‰å¾€åŒ»é™¢ã€‚â€
2. å›ç­”éœ€æ¡ç†æ¸…æ™°ï¼Œä½¿ç”¨æœ‰åºåˆ—è¡¨ï¼ˆ1. 2. 3.ï¼‰æˆ–æ— åºåˆ—è¡¨ï¼ˆ- ...ï¼‰ç»„ç»‡å†…å®¹ï¼›
3. æ¯ä¸ªè§‚ç‚¹åªè¡¨è¾¾ä¸€æ¬¡ï¼Œç¦æ­¢å†—ä½™ï¼›
4. ä½¿ç”¨è§„èŒƒåŒ»å­¦æœ¯è¯­ï¼ˆå¦‚â€œå¿ƒè‚Œæ¢—æ­»â€è€Œéâ€œå¿ƒæ¢—â€ï¼‰ï¼›
5. è‹¥ä¿¡æ¯ä¸ç¡®å®šï¼Œè¯·ä½¿ç”¨â€œå¯èƒ½â€â€œéƒ¨åˆ†ç ”ç©¶è¡¨æ˜â€ç­‰æªè¾ã€‚

ä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆå¦‚æœ‰ï¼‰ï¼š
{chat_history}

åŒ»å­¦çŸ¥è¯†ï¼š
{context}

å½“å‰ç”¨æˆ·é—®é¢˜ï¼š
{question}
"""

    prompt = ChatPromptTemplate.from_template(template)

    # âœ… ä¿®æ­£ï¼šåªä» input_data å­—å…¸ä¸­æå– messages
    def format_chat_history_for_prompt(input_data: Dict[str, Any]) -> str:
        messages = input_data.get("messages", [])
        return truncate_chat_history(messages, tokenizer, max_tokens=800)

    rag_chain = (
        {
            "context": retriever_runnable,
            "chat_history": RunnableLambda(format_chat_history_for_prompt),
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    rag_chain._tokenizer = tokenizer
    rag_chain._model = model
    return rag_chain, "æˆåŠŸ"


# ==========================================
# èµ„æºæ¸…ç†
# ==========================================
def destroy_rag_system(rag_chain):
    try:
        if hasattr(rag_chain, '_model'):
            del rag_chain._model
        if hasattr(rag_chain, '_tokenizer'):
            del rag_chain._tokenizer
        gc.collect()
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"æ¸…ç†å¤±è´¥: {e}")


# ==========================================
# Streamlit UI
# ==========================================
st.set_page_config(page_title=ST_TITLE, page_icon="ğŸ¥")
st.title(ST_TITLE)
st.markdown("### ğŸ’Š åŸºäºåŒ»å­¦çŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼ˆåŠ¨æ€ MMR + ä¸Šä¸‹æ–‡æ„ŸçŸ¥ BGE-Rerankerï¼‰")
st.markdown("---")

with st.sidebar:
    st.header("ğŸ” ä¼˜åŒ–ç‰¹æ€§")
    st.info("â€¢ åŠ¨æ€ MMR Î»ï¼ˆé«˜é£é™©é—®é¢˜æ›´ç›¸å…³ï¼‰")
    st.info("â€¢ BGE-Reranker ä¸Šä¸‹æ–‡æ„ŸçŸ¥ + æ— ç­”æ¡ˆæ£€æµ‹")
    st.info("â€¢ Token-aware å¯¹è¯å†å²æˆªæ–­")
    st.info("â€¢ ç»“æ„åŒ– Prompt + å®‰å…¨å…œåº•")

    if "rag_chain" not in st.session_state:
        with st.spinner("æ­£åœ¨åŠ è½½åŒ»å­¦çŸ¥è¯†åº“ä¸æ¨¡å‹..."):
            st.session_state.rag_chain, msg = initialize_rag_system()

    if st.session_state.rag_chain:
        st.success("âœ… RAG ç³»ç»Ÿå·²å°±ç»ª")
        st.info(f"ğŸ§  LLM: {os.path.basename(MODEL_NAME)}")
        st.info(f"reranker: BGE-Reranker-v2-m3 (é˜ˆå€¼={SCORE_THRESHOLD})")
    else:
        st.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {msg}")
        st.stop()

    st.markdown("---")
    st.markdown("**å…è´£å£°æ˜**")
    st.markdown("âš ï¸ æœ¬ç³»ç»Ÿä»…æä¾›åŒ»å­¦çŸ¥è¯†å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€‚")

    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ–å¯¹è¯å†å²ï¼ˆå¹¶å‡€åŒ–ï¼‰
if "messages" not in st.session_state:
    st.session_state.messages = []
else:
    # å¯é€‰ï¼šå¯åŠ¨æ—¶å‡€åŒ–æ—§ä¼šè¯
    st.session_state.messages = normalize_messages(st.session_state.messages)

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥å¤„ç†
if prompt := st.chat_input("è¯·è¾“å…¥å…³äºä¸­æ–‡åŒ»ç–—é¢†åŸŸçš„é—®é¢˜..."):
    # æ˜¾ç¤ºå¹¶ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # âœ… å…³é”®ï¼šä¼ å…¥åŒ…å« messages çš„å­—å…¸
    input_dict = {
        "question": prompt,
        "messages": st.session_state.messages
    }

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        try:
            for chunk in st.session_state.rag_chain.stream(input_dict):
                full_response += chunk
                response_placeholder.markdown(full_response + "â–Œ")
            response_placeholder.markdown(full_response)
        except Exception as e:
            error_msg = f"ç³»ç»Ÿé”™è¯¯: {str(e)}"
            st.error(error_msg)
            full_response = error_msg

    # ä¿å­˜åŠ©æ‰‹å›å¤
    st.session_state.messages.append({"role": "assistant", "content": full_response})