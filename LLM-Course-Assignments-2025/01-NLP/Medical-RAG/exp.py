import streamlit as st
import os
import time

# å¼•å…¥ LangChain ç»„ä»¶
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================
ST_TITLE = "ğŸš X-2000 æ— äººæœº - æ™ºèƒ½æŠ€æœ¯æ”¯æŒç»ˆç«¯"
os.environ["OPENAI_API_KEY"] = "sk-gyuofotkkugmqvlmcuchjdzmipktruzczqvqtqyiyfqbqvsu"  # å¡«å…¥ä½ çš„ Key
os.environ["OPENAI_API_BASE"] = "https://api.siliconflow.cn/v1"
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"
EMBEDDING_MODEL = "BAAI/bge-m3"


# ==========================================
# æ ¸å¿ƒé€»è¾‘ (ä½¿ç”¨ @st.cache_resource ç¼“å­˜ï¼Œé˜²æ­¢æ¯æ¬¡åˆ·æ–°éƒ½é‡è·‘)
# æŠ€æœ¯ç‚¹ï¼šSingleton æ¨¡å¼åœ¨ Web å¼€å‘ä¸­çš„åº”ç”¨
# ==========================================
@st.cache_resource
def initialize_rag_system():
    """
    åˆå§‹åŒ– RAG ç³»ç»Ÿï¼šåŠ è½½æ•°æ® -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å‚¨
    åªè¿è¡Œä¸€æ¬¡ï¼Œåç»­ç›´æ¥è°ƒç”¨ç¼“å­˜çš„å¯¹è±¡ã€‚
    """
    # 1. åŠ è½½æ•°æ®
    script_dir = os.path.dirname(os.path.abspath(__file__))
    knowledge_path = os.path.join(script_dir, "knowledge.txt")
    if not os.path.exists(knowledge_path):
        return None, f"æ‰¾ä¸åˆ° knowledge.txt æ–‡ä»¶: {knowledge_path}"

    loader = TextLoader(knowledge_path, encoding="utf-8")
    docs = loader.load()

    # 2. åˆ‡åˆ†
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    splits = splitter.split_documents(docs)

    # 3. å‘é‡åŒ–
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

    # 4. æ„å»ºæ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # 5. å®šä¹‰ LLM (å¼€å¯æµå¼è¾“å‡º streaming=True)
    llm = ChatOpenAI(
        model_name=MODEL_NAME,
        temperature=0.1,
        streaming=True
    )

    # 6. å®šä¹‰ Prompt
    template = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ— äººæœºæŠ€æœ¯æ”¯æŒä¸“å®¶ã€‚è¯·ç»“åˆä»¥ä¸‹ã€ä¸Šä¸‹æ–‡ã€‘å’Œã€å†å²èŠå¤©è®°å½•ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    å¦‚æœä¸çŸ¥é“ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

    ã€ä¸Šä¸‹æ–‡ã€‘ï¼š
    {context}

    ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 7. æ„å»ºé“¾
    rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    return rag_chain, "ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ"


# ==========================================
# Streamlit UI ç•Œé¢é€»è¾‘
# ==========================================
st.set_page_config(page_title=ST_TITLE, page_icon="ğŸš")
st.title(ST_TITLE)

# ä¾§è¾¹æ ï¼šæ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
with st.sidebar:
    st.header("ç³»ç»ŸçŠ¶æ€é¢æ¿")
    with st.spinner("æ­£åœ¨å¯åŠ¨ç¥ç»ä¸­æ¢..."):
        rag_chain, msg = initialize_rag_system()

    if rag_chain:
        st.success("âœ… çŸ¥è¯†åº“å·²æŒ‚è½½ (RAG Ready)")
        st.info(f"ğŸ§  æ¨¡å‹: {MODEL_NAME}")
    else:
        st.error(f"âŒ å¯åŠ¨å¤±è´¥: {msg}")
        st.stop()

    st.markdown("---")
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ–å¯¹è¯å†å² (Session State)
# æŠ€æœ¯ç‚¹ï¼šWeb æ˜¯æ— çŠ¶æ€çš„ï¼Œå¿…é¡»æ‰‹åŠ¨ç»´æŠ¤ä¸Šä¸‹æ–‡
if "messages" not in st.session_state:
    st.session_state.messages = []

# 1. æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 2. å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥å…³äº X-2000 çš„é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·çš„é—®é¢˜
    st.chat_message("user").markdown(prompt)
    # å°†é—®é¢˜å­˜å…¥å†å²
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 3. ç”Ÿæˆå›ç­” (æµå¼)
    with st.chat_message("assistant"):
        response_placeholder = st.empty()  # å ä½ç¬¦
        full_response = ""

        # è°ƒç”¨ RAG é“¾ (Stream æ¨¡å¼)
        try:
            # è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œä¼šä¸æ–­åå‡ºå­—ç¬¦
            for chunk in rag_chain.stream(prompt):
                full_response += chunk
                # å®æ—¶åˆ·æ–°ç•Œé¢ï¼Œæ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ
                response_placeholder.markdown(full_response + "â–Œ")
                # time.sleep(0.01) # å¦‚æœç”±äºç½‘ç»œå¤ªå¿«çœ‹ä¸æ¸…æµå¼ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Š

            response_placeholder.markdown(full_response)
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {e}")
            full_response = f"æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°æ•…éšœ: {e}"

    # å°† AI çš„å›ç­”å­˜å…¥å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})