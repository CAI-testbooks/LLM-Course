import streamlit as st
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"#ä» Hugging Face ä¸‹è½½ BAAI/bge-m3 åµŒå…¥æ¨¡å‹æ—¶ æ— æ³•è¿æ¥åˆ°äº’è”ç½‘  éœ€ä¿®æ”¹
import json
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
import torch
# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================

ST_TITLE = "ä¸­æ–‡åŒ»ç–—é¢†åŸŸæ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
MODEL_NAME = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"  # æœ¬åœ°æ¨¡å‹è·¯å¾„
EMBEDDING_MODEL = "BAAI/bge-m3"
VECTOR_DB_PATH = "./chroma_db_medical"  # â† å‘é‡åº“æŒä¹…åŒ–ç›®å½• æœ¬åœ°å·²å­˜åœ¨ Chroma å‘é‡æ•°æ®åº“ï¼ˆå¦‚ ./chroma_db_historyï¼‰ï¼Œå°±ç›´æ¥åŠ è½½ï¼›å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä»æ–‡æ¡£æ„å»ºå¹¶å‘ç£ç›˜ä¿å­˜ã€‚
# ==========================================
# è‡ªå®šä¹‰ JSONL åŠ è½½å‡½æ•°
# ==========================================
def load_jsonl_as_documents(file_path):
    """ä» JSONL æ–‡ä»¶åŠ è½½ä¸º LangChain Documents"""
    docs = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                questions = data.get("questions", [])
                answers = data.get("answers", [])
                
                # æ”¯æŒå¤šä¸ªé—®é¢˜å¯¹åº”ä¸€ä¸ªç­”æ¡ˆï¼ˆå–ç¬¬ä¸€ä¸ªé—®é¢˜ä½œä¸º contentï¼‰
                if not questions or not answers:
                    st.warning(f"è·³è¿‡æ— æ•ˆè¡Œ {file_path}:{line_num}")
                    continue
                
                # å–ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨åµŒå¥—ï¼‰
                question = questions[0]
                if isinstance(question, list):
                    question = question[0] if question else ""
                
                answer = answers[0] if answers else ""
                
                # æ„é€ æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‰ï¼šåªç”¨é—®é¢˜ï¼Œæˆ–é—®é¢˜+ç­”æ¡ˆï¼‰
                text = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š{answer}"
                metadata = {
                    "question": question,
                    "answer": answer,
                    "source": os.path.basename(file_path),
                    "line": line_num
                }
                docs.append(Document(page_content=text, metadata=metadata))
            except json.JSONDecodeError as e:
                st.error(f"JSON è§£æå¤±è´¥ {file_path}:{line_num} - {e}")
                continue
    return docs

# ==========================================
# åˆå§‹åŒ– RAG ç³»ç»Ÿ
# ==========================================
@st.cache_resource
def initialize_rag_system():
    dataset_dir = "/root/autodl-tmp/Medical-RAG/dataset"
    if not os.path.exists(dataset_dir):
        return None, f"æ‰¾ä¸åˆ°æ•°æ®é›†ç›®å½•: {dataset_dir}"

    # å‘é‡åŒ–é…ç½®ï¼ˆå¿…é¡»æå‰å®šä¹‰ï¼Œç”¨äºåŠ è½½æˆ–åˆ›å»ºï¼‰
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True}
    )

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æŒä¹…åŒ–çš„å‘é‡åº“
    if os.path.exists(VECTOR_DB_PATH):
        st.info("æ£€æµ‹åˆ°å·²æœ‰å‘é‡åº“ï¼Œæ­£åœ¨åŠ è½½...")
        vectorstore = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        st.success("âœ… å‘é‡åº“åŠ è½½å®Œæˆï¼")
    else:
        # === éœ€è¦é‡æ–°æ„å»ºå‘é‡åº“ ===
        json_files = [
            os.path.join(dataset_dir, "test_data.json"),
            os.path.join(dataset_dir, "validation_data.json"),
            os.path.join(dataset_dir, "train_data_8k.json")
        ]
        
        docs = []
        for file_path in json_files:
            if os.path.exists(file_path):
                st.info(f"æ­£åœ¨åŠ è½½: {os.path.basename(file_path)}")
                file_docs = load_jsonl_as_documents(file_path)
                docs.extend(file_docs)
                st.success(f"å®ŒæˆåŠ è½½: {len(file_docs)} æ¡è®°å½• from {os.path.basename(file_path)}")
            else:
                st.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        if not docs:
            return None, "æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£"

        # åˆ‡åˆ†
        splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        splits = splitter.split_documents(docs)

        st.info("æ­£åœ¨æ„å»ºå‘é‡åº“")#é¦–æ¬¡åŠ è½½å·¨æ…¢æ— æ¯” è€å¿ƒç­‰å¾…
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=VECTOR_DB_PATH
        )
        st.success("âœ… å‘é‡åº“æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜è‡³æœ¬åœ°ï¼")

    # æ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # åŠ è½½æœ¬åœ° Qwen-2.5-7B æ¨¡å‹ï¼ˆå¸¦é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
    # åŠ è½½ tokenizer å¹¶ä¿®å¤ pad token
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # â† å…³é”®ä¿®å¤ï¼

    bnb_config = BitsAndBytesConfig(
        #load_in_4bit=True,#å¿…é¡»æ³¨é‡Šï¼Œå¦åˆ™ä¼šå‡ºç°æ¢³ç†å€¼naf ç­‰é—®é¢˜
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    # åˆ›å»º pipeline æ—¶æ˜¾å¼æŒ‡å®š pad/eos token
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        temperature=0.1,
        top_p=0.95,
        repetition_penalty=1.1,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,   # â† å¿…é¡»
        eos_token_id=tokenizer.eos_token_id,   # â† æ¨è
        clean_up_tokenization_spaces=True
    )
    #llm_pipeline = HuggingFacePipeline(pipeline=pipe)

    #llm = HuggingFacePipeline(pipeline=pipe)
    
    # æ­£ç¡®æ–¹å¼ï¼šå…ˆåŒ…è£…æˆ HuggingFacePipelineï¼Œå†ç”¨ ChatHuggingFace
    llm_pipeline = HuggingFacePipeline(pipeline=pipe)

    llm = ChatHuggingFace(
        llm=llm_pipeline,       # â† å¿…é¡»ç”¨ llm= å‚æ•°
        tokenizer=tokenizer,
        streaming=True
    )
    # Prompt
    template = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ã€‚è¯·ç»“åˆä»¥ä¸‹ã€åŒ»å­¦çŸ¥è¯†ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    å¦‚æœä¸çŸ¥é“ï¼Œè¯·ç›´æ¥è¯´"æ ¹æ®ç°æœ‰åŒ»å­¦èµ„æ–™ï¼Œæˆ‘æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ"ã€‚

    ã€åŒ»å­¦çŸ¥è¯†ã€‘ï¼š
    {context}

    ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)


    # RAG é“¾
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain, "ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ"


# ==========================================
# Streamlit UI
# ==========================================
st.set_page_config(page_title=ST_TITLE, page_icon="ğŸ¥")
st.title(ST_TITLE)
st.markdown("### ğŸ’Š åŸºäºåŒ»å­¦çŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
st.markdown("---")

with st.sidebar:
    st.header("ç³»ç»ŸçŠ¶æ€é¢æ¿")
    with st.spinner("æ­£åœ¨åŠ è½½åŒ»å­¦çŸ¥è¯†åº“..."):
        rag_chain, msg = initialize_rag_system()

    if rag_chain:
        st.success("âœ… åŒ»å­¦çŸ¥è¯†åº“å·²æŒ‚è½½ (RAG Ready)")
        st.info(f"ğŸ§  æ¨¡å‹: {MODEL_NAME}")
    else:
        st.error(f"âŒ å¯åŠ¨å¤±è´¥: {msg}")
        st.stop()

    st.markdown("---")
    st.markdown("**å…è´£å£°æ˜**")
    st.markdown("âš ï¸ æœ¬ç³»ç»Ÿä»…æä¾›åŒ»å­¦çŸ¥è¯†å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€‚å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³å°±åŒ»ã€‚")
    
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ–å¯¹è¯å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥å…³äºä¸­æ–‡åŒ»ç–—é¢†åŸŸçš„é—®é¢˜..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        try:
            for chunk in rag_chain.stream(prompt):
                full_response += chunk
                response_placeholder.markdown(full_response + "â–Œ")
            response_placeholder.markdown(full_response)
        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°é”™è¯¯: {str(e)}"
            st.error(error_msg)
            full_response = error_msg

    st.session_state.messages.append({"role": "assistant", "content": full_response})