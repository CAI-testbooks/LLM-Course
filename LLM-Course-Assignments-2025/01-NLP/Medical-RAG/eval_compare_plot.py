import os
import json
import jieba
import time  # å¯¼å…¥timeæ¨¡å— ç”¨äºä¼‘çœ é˜²é™æµ
import numpy as np
from collections import Counter
from openai import OpenAI
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ===================== å¯è§†åŒ–ä¾èµ– =====================
import matplotlib.pyplot as plt
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (16, 10)

# ===================== é…ç½®åŒº =====================
os.environ["OPENAI_API_KEY"] = "sk-gyuofotkkugmqvlmcuchjdzmipktruzczqvqtqyiyfqbqvsu"
os.environ["OPENAI_API_BASE"] = "https://api.siliconflow.cn/v1"

# åˆå§‹åŒ–Judgeå®¢æˆ·ç«¯
client = OpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
    base_url=os.environ["OPENAI_API_BASE"]
)
JUDGE_MODEL = "Qwen/Qwen2.5-72B-Instruct"

# æ–‡ä»¶è·¯å¾„é…ç½®
RAG_RESULT_PATH = "/root/autodl-tmp/Medical-RAG/eval_results/rag_top100.json"
BASE_RESULT_PATH = "/root/autodl-tmp/Medical-RAG/eval_results/base_top100.json"
EVAL_OUTPUT_PATH = "/root/autodl-tmp/Medical-RAG/eval_results/final_evaluation.json"
PLOT_SAVE_PATH = "/root/autodl-tmp/Medical-RAG/eval_results/base-rag-evaluation_plots.png"

# å¹»è§‰è¯„ä¼°Prompt
HALLUCINATION_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„åŒ»ç–—é¢†åŸŸä¸“ä¸šè¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­æ¨¡å‹å›ç­”æ˜¯å¦å­˜åœ¨å¹»è§‰ã€‚
è¯„ä¼°è§„åˆ™ï¼š
1. å¹»è§‰å®šä¹‰ï¼šæ¨¡å‹å›ç­”ä¸­åŒ…å«ä¸å‚è€ƒç­”æ¡ˆ/æƒå¨åŒ»ç–—çŸ¥è¯†ç›¸çŸ›ç›¾çš„ä¿¡æ¯ï¼Œæˆ–ç¼–é€ ä¸å­˜åœ¨çš„åŒ»ç–—æ•°æ®ã€è¯ç‰©ã€è¯Šç–—æ–¹æ¡ˆç­‰ã€‚
2. è‹¥å‚è€ƒç­”æ¡ˆä¿¡æ¯ä¸è¶³ï¼Œä»¥æƒå¨åŒ»ç–—å…±è¯†ä¸ºåˆ¤æ–­ä¾æ®ã€‚
3. ä»…å­˜åœ¨è¡¨è¿°å†—ä½™ã€è¯­åºå·®å¼‚ã€åŒä¹‰æœ¯è¯­æ›¿æ¢ä¸å±äºå¹»è§‰ã€‚
4. æ˜ç¡®å›ç­”"æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆ"çš„æƒ…å†µä¸è®¡å…¥å¹»è§‰ã€‚

è¯„ä¼°å¯¹è±¡ï¼š
é—®é¢˜ï¼š{instruction}
å‚è€ƒç­”æ¡ˆï¼š{reference}
æ¨¡å‹å›ç­”ï¼š{answer}

è¯·ä»…è¿”å›ä»¥ä¸‹ç»“æœä¹‹ä¸€ï¼Œæ— éœ€ä»»ä½•è§£é‡Šï¼š
- "HALLUCINATION"ï¼šå­˜åœ¨å¹»è§‰
- "NO_HALLUCINATION"ï¼šæ— å¹»è§‰
"""

# åŒ»ç–—F1æ ¡å‡†Promptï¼ˆåŸå°ä¸åŠ¨ä¿ç•™ï¼‰
F1_CALIBRATION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„**åŒ»ç–—é¢†åŸŸä¸“ä¸šè¯„ä¼°ä¸“å®¶**ï¼Œç›®å‰éœ€è¦å¯¹åŒ»ç–—é—®ç­”å¤§æ¨¡å‹çš„è¾“å‡ºè´¨é‡è¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°æ ¸å¿ƒæŒ‡æ ‡ä¸º precisionï¼ˆç²¾ç¡®ç‡ï¼‰ã€f1ï¼ˆF1 å€¼ï¼‰ã€‚

å¯¹äºä»¥ä¸‹è¯„ä¼°å¯¹è±¡ï¼š
é—®é¢˜ï¼š{instruction}
å‚è€ƒç­”æ¡ˆï¼š{reference}
æ¨¡å‹å›ç­”ï¼š{answer}

æˆ‘å·²é‡‡ç”¨è¯çº§æ— åº F1 è¯„ä¼°æ–¹æ³•ï¼Œå¾—åˆ°åŸºç¡€æŒ‡æ ‡åˆ†æ•°ï¼šprecision={precision},  f1={f1}ã€‚
è¯¥æ–¹æ³•å­˜åœ¨ä¸€å®šç¼ºé™·ï¼ˆå¦‚æ— æ³•è¯†åˆ«åŒ»ç–—æœ¯è¯­åŒä¹‰è¡¨è¿°ã€å¿½ç•¥ä¸´åºŠé€»è¾‘åˆç†æ€§ï¼‰ï¼Œå¯èƒ½å¯¼è‡´åˆ†æ•°åä½ã€‚è¯·ä½ ç«™åœ¨åŒ»ç–—ä¸“ä¸šè§’åº¦ï¼Œç»“åˆä¸´åºŠè§„èŒƒä¸å®é™…è¯Šç–—é€»è¾‘ï¼Œåˆ¤æ–­åœ¨åŸåˆ†æ•°åŸºç¡€ä¸Šå¯æå‡çš„å¹…åº¦ã€‚

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹åŒ»ç–—è¯„ä¼°è§„åˆ™ï¼š
1.  è‹¥å‚è€ƒç­”æ¡ˆè¿‡çŸ­ã€è¡¨è¿°æ¨¡ç³Šæˆ–ä¸ç¬¦åˆä¸´åºŠæŒ‡å—ï¼Œå¯å¿½ç•¥å‚è€ƒç­”æ¡ˆï¼Œç›´æ¥ä¾æ®**æƒå¨åŒ»ç–—å…±è¯†**åˆ¤æ–­æ¨¡å‹å›ç­”çš„å‡†ç¡®æ€§ã€‚
2.  æå‡åçš„å„é¡¹æŒ‡æ ‡å€¼**ä¸¥ç¦è¶…è¿‡ 1.0**ï¼Œä¸”ä¸å¾—ä½äºåŸåŸºç¡€åˆ†æ•°ã€‚
3.  è¯„ä¼°éœ€å…¼é¡¾æœ¯è¯­å‡†ç¡®æ€§ä¸ä¸´åºŠå®ç”¨æ€§ï¼Œä¸å¯è¿‡äºä¸¥è‹›ï¼šæ¨¡å‹å›ç­”æ ¸å¿ƒåŒ»ç–—ä¿¡æ¯æ­£ç¡®ï¼Œä»…å­˜åœ¨è¡¨è¿°å†—ä½™æˆ–è¯­åºå·®å¼‚æ—¶ï¼Œåº”åˆç†åŠ åˆ†ã€‚
4.  è‹¥åŸè¯çº§ F1 åˆ†æ•°å·²èƒ½å®¢è§‚åæ˜ æ¨¡å‹å›ç­”è´¨é‡ï¼Œå¯ä¿æŒåŸåˆ†æ•°ä¸å˜ã€‚
5.  æ¨¡å‹å›ç­”ä¸å‚è€ƒç­”æ¡ˆæ ¸å¿ƒä¿¡æ¯ä¸€è‡´ã€è¯­è¨€é€šé¡ºä¸”ç¬¦åˆåŒ»ç–—è¡¨è¿°è§„èŒƒï¼Œå¯åœ¨åŸåˆ†æ•°åŸºç¡€ä¸Šé€‚å½“åŠ åˆ†ï¼›è‹¥å­˜åœ¨é€»è¾‘è‡ªæ´½çš„åˆç†å»¶ä¼¸ï¼ˆå¦‚è¡¥å……ä¸´åºŠç”¨è¯æ³¨æ„äº‹é¡¹ï¼‰ï¼Œé¢å¤–é…Œæƒ…åŠ åˆ†ã€‚
6.  é‡ç‚¹æ¯”å¯¹æ¨¡å‹å›ç­”ä¸å‚è€ƒç­”æ¡ˆçš„**æ ¸å¿ƒåŒ»ç–—å…³é”®è¯**ï¼ˆå¦‚ç–¾ç—…åç§°ã€è¯ç‰©åç§°ã€è¯Šç–—æ–¹æ¡ˆã€å‰‚é‡å•ä½ï¼‰ï¼Œå…³é”®è¯åŒ¹é…åº¦é«˜ä¸”æ— çŸ¥è¯†æ€§é”™è¯¯æ—¶ï¼Œä¼˜å…ˆæå‡ recall ä¸ f1ï¼›æ— æ— å…³ä¿¡æ¯å†—ä½™æ—¶ï¼Œä¼˜å…ˆæå‡ precisionã€‚
7.  åŒ»ç–—æœ¯è¯­å­˜åœ¨å…¬è®¤åŒä¹‰è¡¨è¿°ï¼ˆå¦‚â€œè„‘æ¢—æ­»â€ä¸â€œè„‘æ¢—å¡â€ã€â€œå¿ƒæ¢—â€ä¸â€œå¿ƒè‚Œæ¢—æ­»â€ï¼‰æ—¶ï¼Œè§†ä¸ºæœ‰æ•ˆåŒ¹é…ï¼Œä¸å¾—å› è¡¨è¿°å·®å¼‚æ‰£åˆ†ã€‚
8.  æ¨¡å‹å›ç­”å‡ºç°**åŒ»ç–—çŸ¥è¯†æ€§é”™è¯¯**ï¼ˆå¦‚è¯ç‰©é€‚åº”ç—‡æ··æ·†ã€ç–¾ç—…è¯Šæ–­é”™è¯¯ã€å‰‚é‡å•ä½é”™è¯¯ï¼‰æ—¶ï¼Œä¸å¾—æå‡åˆ†æ•°ï¼Œç»´æŒåŸåŸºç¡€åˆ†ã€‚

è¯·ç›´æ¥è¿”å›æ ¡å‡†åçš„ä¸¤ä¸ªæŒ‡æ ‡å€¼ï¼ŒæŒ‰ precisionã€f1 çš„é¡ºåºç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œæ— éœ€ä»»ä½•è§£é‡Šæˆ–å¤šä½™å†…å®¹ã€‚
"""

# ===================== æ ¸å¿ƒè¯„ä¼°å‡½æ•° =====================
def word_level_f1(reference, answer):
    """è¯çº§æ— åºF1è®¡ç®—ï¼ˆåŸºäºjiebaåˆ†è¯ï¼‰ è¿”å›ï¼šprecision, recall, f1"""
    ref_words = list(jieba.cut(reference.strip()))
    ans_words = list(jieba.cut(answer.strip()))
    
    if not ref_words and not ans_words:
        return 1.0, 1.0, 1.0
    if not ref_words or not ans_words:
        return 0.0, 0.0, 0.0
    
    ref_counter = Counter(ref_words)
    ans_counter = Counter(ans_words)
    
    intersection = 0
    for word in ref_counter:
        if word in ans_counter:
            intersection += min(ref_counter[word], ans_counter[word])
    
    precision = intersection / sum(ans_counter.values()) if sum(ans_counter.values()) > 0 else 0.0
    recall = intersection / sum(ref_counter.values()) if sum(ref_counter.values()) > 0 else 0.0
    
    if precision + recall == 0:
        f1 = 0.0
    else:
        f1 = 2 * precision * recall / (precision + recall)
    
    return round(precision, 4), round(recall, 4), round(f1, 4)

def call_judge_model(prompt):
    """è°ƒç”¨Judgeæ¨¡å‹è·å–è¯„ä¼°ç»“æœã€å·²åˆ é™¤å†…éƒ¨sleepï¼Œæ— ä»»ä½•ä¼‘çœ ã€‘"""
    try:
        response = client.chat.completions.create(
            model=JUDGE_MODEL,
            messages=[{"role": "user", "content": prompt.strip()}],
            temperature=0.1,
            max_tokens=512,
            timeout=60
        )
        res_content = response.choices[0].message.content.strip()
        return res_content  # ç›´æ¥è¿”å›ç»“æœï¼Œæ— sleep
    except Exception as e:
        print(f"\nâš ï¸ Judgeæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        return None

def calibrate_f1(instruction, reference, answer, base_precision, base_f1):
    """ä½¿ç”¨LLMæ ¡å‡†F1åˆ†æ•°"""
    prompt = F1_CALIBRATION_PROMPT.format(
        instruction=instruction, reference=reference, answer=answer,
        precision=base_precision, f1=base_f1
    )
    result = call_judge_model(prompt)
    if not result:
        return base_precision, base_f1
    
    try:
        calibrated_precision, calibrated_f1 = map(float, result.split(","))
        calibrated_precision = max(min(calibrated_precision, 1.0), base_precision)
        calibrated_f1 = max(min(calibrated_f1, 1.0), base_f1)
        return round(calibrated_precision, 4), round(calibrated_f1, 4)
    except:
        return base_precision, base_f1

def evaluate_hallucination(instruction, reference, answer):
    """è¯„ä¼°å¹»è§‰ç‡"""
    if "æ ¹æ®ç°æœ‰åŒ»å­¦èµ„æ–™ï¼Œæˆ‘æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ" in answer:
        return "NO_HALLUCINATION"
    
    prompt = HALLUCINATION_PROMPT.format(instruction=instruction, reference=reference, answer=answer)
    result = call_judge_model(prompt)
    if result in ["HALLUCINATION", "NO_HALLUCINATION"]:
        return result
    else:
        return "HALLUCINATION" if "é”™è¯¯" in answer or "ä¸å­˜åœ¨" in reference and "å­˜åœ¨" in answer else "NO_HALLUCINATION"

def evaluate_model_results(result_path, model_name):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹çš„ç»“æœï¼ˆF1ã€å¹»è§‰ç‡ï¼‰"""
    print(f"\n========== è¯„ä¼° {model_name} æ¨¡å‹ ==========")
    with open(result_path, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    total_samples = len(results)
    if total_samples == 0:
        return {}
    
    total_base_precision = 0.0
    total_base_f1 = 0.0
    total_calibrated_precision = 0.0
    total_calibrated_f1 = 0.0
    hallucination_count = 0
    
    for idx, item in enumerate(tqdm(results, desc=f"è¯„ä¼°{model_name}")):
        instruction = item["instruction"]
        reference = item["reference"]
        answer = item["answer"]
        
        # 1. è®¡ç®—åŸºç¡€è¯çº§F1
        base_precision, _, base_f1 = word_level_f1(reference, answer)
        # 2. LLMæ ¡å‡†F1åˆ†æ•°
        cal_precision, cal_f1 = calibrate_f1(instruction, reference, answer, base_precision, base_f1)
        # 3. LLMè¯„ä¼°å¹»è§‰ç‡
        hallucination_result = evaluate_hallucination(instruction, reference, answer)
        
        # ç»Ÿè®¡è®¡æ•°
        if hallucination_result == "HALLUCINATION":
            hallucination_count += 1
        
        # ç´¯åŠ åˆ†æ•°
        total_base_precision += base_precision
        total_base_f1 += base_f1
        total_calibrated_precision += cal_precision
        total_calibrated_f1 += cal_f1
        
        # ä¿å­˜å•æ¡æ ·æœ¬è¯„ä¼°ç»“æœï¼ˆä¸å†ä¿å­˜ is_accurateï¼‰
        results[idx]["base_metrics"] = {"precision": base_precision, "f1": base_f1}
        results[idx]["calibrated_metrics"] = {"precision": cal_precision, "f1": cal_f1}
        results[idx]["hallucination"] = hallucination_result
        
        # âœ… æ¯è¯„ä¼°å®Œä¸€æ¡æ ·æœ¬åä¼‘çœ 10ç§’ï¼Œé˜²æ­¢APIé™æµ
        time.sleep(10)
    
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡ï¼ˆä¸å« accuracyï¼‰
    overall_metrics = {
        "total_samples": total_samples,
        "base_precision": round(total_base_precision / total_samples, 4),
        "base_f1": round(total_base_f1 / total_samples, 4),
        "calibrated_precision": round(total_calibrated_precision / total_samples, 4),
        "calibrated_f1": round(total_calibrated_f1 / total_samples, 4),
        "hallucination_rate": round(hallucination_count / total_samples, 4)
    }
    
    return {"model_name": model_name, "overall_metrics": overall_metrics, "sample_details": results}

# ===================== å¯è§†åŒ–å‡½æ•° =====================
def plot_evaluation_results(rag_metrics, base_metrics, save_path):
    # All metrics to plot (in order)
    metric_labels = [
        'Base Precision', 'Calibrated Precision',
        'Base F1 Score', 'Calibrated F1 Score',
        'Hallucination Rate'
    ]
    
    # Corresponding scores for RAG and Base models
    rag_scores = [
        rag_metrics['base_precision'],
        rag_metrics['calibrated_precision'],
        rag_metrics['base_f1'],
        rag_metrics['calibrated_f1'],
        rag_metrics['hallucination_rate']
    ]
    
    base_scores = [
        base_metrics['base_precision'],
        base_metrics['calibrated_precision'],
        base_metrics['base_f1'],
        base_metrics['calibrated_f1'],
        base_metrics['hallucination_rate']
    ]
    
    x = np.arange(len(metric_labels))
    width = 0.35

    fig, ax1 = plt.subplots(figsize=(14, 8))
    
    # Left Y-axis: for F1 / Precision (range 0â€“1, higher is better)
    bar1 = ax1.bar(x[:-1] - width/2, rag_scores[:-1], width, label='RAG Model', color='#1f77b4', alpha=0.9, edgecolor='black', linewidth=0.8)
    bar2 = ax1.bar(x[:-1] + width/2, base_scores[:-1], width, label='Base Model', color='#ff7f0e', alpha=0.9, edgecolor='black', linewidth=0.8)
    
    ax1.set_ylabel('F1 / Precision (Higher is Better)', fontsize=12, color='black')
    ax1.set_ylim(0, 1.05)
    ax1.tick_params(axis='y', labelcolor='black')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metric_labels, fontsize=11)
    ax1.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.7)

    # Right Y-axis: for Hallucination Rate (0â€“1, lower is better)
    ax2 = ax1.twinx()
    bar3 = ax2.bar(x[-1] - width/2, rag_scores[-1], width, label='RAG Hallucination', color='#2ca02c', alpha=0.8, hatch='//', edgecolor='black', linewidth=0.8)
    bar4 = ax2.bar(x[-1] + width/2, base_scores[-1], width, label='Base Hallucination', color='#d62728', alpha=0.8, hatch='\\\\', edgecolor='black', linewidth=0.8)
    
    ax2.set_ylabel('Hallucination Rate (Lower is Better)', fontsize=12, color='gray')
    ax2.set_ylim(0, max(rag_scores[-1], base_scores[-1]) * 1.25 or 0.1)
    ax2.tick_params(axis='y', labelcolor='gray')

    # Unified legend
    bars = [bar1, bar2, bar3, bar4]
    labels = ['RAG (F1/Prec)', 'Base (F1/Prec)', 'RAG Hallucination', 'Base Hallucination']
    ax1.legend(bars, labels, loc='upper left', fontsize=10)

    # Add value labels on top of bars
    def add_value_labels(ax, bars, is_hallucination=False):
        for bar in bars:
            height = bar.get_height()
            if height > 0:
                ax.text(
                    bar.get_x() + bar.get_width() / 2,
                    height + (0.01 if not is_hallucination else 0.005),
                    f'{height:.4f}',
                    ha='center', va='bottom', fontsize=9, fontweight='bold'
                )

    add_value_labels(ax1, bar1)
    add_value_labels(ax1, bar2)
    add_value_labels(ax2, bar3, is_hallucination=True)
    add_value_labels(ax2, bar4, is_hallucination=True)

    plt.title('Medical Domain: RAG vs Base Model Evaluation Metrics\n(F1, Precision, Hallucination Rate)', fontsize=15, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print(f"\nâœ… Evaluation plot saved to: {save_path}")
# ===================== ä¸»å‡½æ•° =====================
def main():
    rag_evaluation = evaluate_model_results(RAG_RESULT_PATH, "RAG_Model")
    base_evaluation = evaluate_model_results(BASE_RESULT_PATH, "Base_Model")
    
    final_evaluation = {
        "evaluation_config": {
            "judge_model": JUDGE_MODEL,
            "f1_calculation": "è¯çº§æ— åºF1ï¼ˆjiebaåˆ†è¯ï¼‰",
            "calibration_method": "LLM-as-a-Judgeï¼ˆåŒ»ç–—ä¸“ä¸šæ ¡å‡†ï¼‰",
            "anti_limit_strategy": "âœ… æ¯è¯„ä¼°å®Œæˆ1æ¡æ ·æœ¬åï¼Œç»Ÿä¸€ä¼‘çœ 10ç§’ï¼Œé˜²æ­¢ç¡…åŸºæµåŠ¨é™æµ"
        },
        "rag_model": rag_evaluation,
        "base_model": base_evaluation,
        "comparison": {
            # accuracy_diff å·²ç§»é™¤
            "f1_diff": rag_evaluation["overall_metrics"]["calibrated_f1"] - base_evaluation["overall_metrics"]["calibrated_f1"],
            "hallucination_rate_diff": rag_evaluation["overall_metrics"]["hallucination_rate"] - base_evaluation["overall_metrics"]["hallucination_rate"],
            "precision_diff": rag_evaluation["overall_metrics"]["calibrated_precision"] - base_evaluation["overall_metrics"]["calibrated_precision"]
        }
    }
    
    with open(EVAL_OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(final_evaluation, f, ensure_ascii=False, indent=2)
    
    plot_evaluation_results(rag_evaluation["overall_metrics"], base_evaluation["overall_metrics"], PLOT_SAVE_PATH)

    print("\n" + "="*80)
    print("âœ… æœ€ç»ˆè¯„ä¼°æ±‡æ€»æŠ¥å‘Š (åŒ»ç–—é¢†åŸŸ RAG vs Base)".center(80))
    print("="*80)
    print(f"è¯„ä¼°Judgeæ¨¡å‹: {JUDGE_MODEL}")
    print(f"æµ‹è¯•æ ·æœ¬æ€»æ•°: {rag_evaluation['overall_metrics']['total_samples']}")
    print(f"è¯„ä¼°æ–¹æ³•: è¯çº§æ— åºF1 + LLMåŒ»ç–—ä¸“ä¸šæ ¡å‡† + LLMå¹»è§‰åˆ¤å®š")
    print(f"é˜²é™æµç­–ç•¥: âœ… æ¯è¯„ä¼°1æ¡æ ·æœ¬åä¼‘çœ 10ç§’ï¼Œç²¾å‡†é˜²é™æµ")
    print("="*80)
    
    print("\nã€ğŸ“Œ RAGæ£€ç´¢å¢å¼ºæ¨¡å‹ æ ¸å¿ƒæŒ‡æ ‡ã€‘")
    print(f"  âœ… æ ¡å‡†åç²¾ç¡®ç‡    : {rag_evaluation['overall_metrics']['calibrated_precision']:.4f}")
    print(f"  âœ… æ ¡å‡†åF1å€¼      : {rag_evaluation['overall_metrics']['calibrated_f1']:.4f}")
    print(f"  âš ï¸  å¹»è§‰ç‡          : {rag_evaluation['overall_metrics']['hallucination_rate']:.4f}")
    
    print("\nã€ğŸ“Œ åŸºç¡€Baseæ¨¡å‹ æ ¸å¿ƒæŒ‡æ ‡ã€‘")
    print(f"  âœ… æ ¡å‡†åç²¾ç¡®ç‡    : {base_evaluation['overall_metrics']['calibrated_precision']:.4f}")
    print(f"  âœ… æ ¡å‡†åF1å€¼      : {base_evaluation['overall_metrics']['calibrated_f1']:.4f}")
    print(f"  âš ï¸  å¹»è§‰ç‡          : {base_evaluation['overall_metrics']['hallucination_rate']:.4f}")
    
    print("\nã€ğŸ“Š æ¨¡å‹å·®å¼‚å¯¹æ¯” (RAG - Base)ã€‘")
    print(f"  ğŸ“ˆ F1å€¼æå‡        : {final_evaluation['comparison']['f1_diff']:+.4f}")
    print(f"  ğŸ“ˆ ç²¾ç¡®ç‡æå‡      : {final_evaluation['comparison']['precision_diff']:+.4f}")
    print(f"  ğŸ“‰ å¹»è§‰ç‡å˜åŒ–      : {final_evaluation['comparison']['hallucination_rate_diff']:+.4f} (è´Ÿæ•°=é™ä½)")
    print("="*80)
    print(f"\nğŸ“‹ è¯¦ç»†è¯„ä¼°ç»“æœJSONå·²ä¿å­˜è‡³: {EVAL_OUTPUT_PATH}")
    print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {PLOT_SAVE_PATH}")

if __name__ == "__main__":
    main()