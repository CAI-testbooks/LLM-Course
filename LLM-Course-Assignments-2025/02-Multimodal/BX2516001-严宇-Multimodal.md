æˆ‘ä»¬å°†ä½¿ç”¨ChartQAæ•°æ®é›†å¯¹Qwen2-VL-7Bæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¯¥æ•°æ®é›†åŒ…å«å„ç§å›¾è¡¨ç±»å‹çš„å›¾åƒä»¥åŠä¸ä¹‹é…å¯¹çš„é—®ç­”å¯¹â€”â€”éå¸¸é€‚åˆå¢å¼ºæ¨¡å‹çš„è§†è§‰é—®ç­”èƒ½åŠ›ã€‚

ğŸ“– å…¶ä»–èµ„æº

å¦‚æœæ‚¨å¯¹æ›´å¤š VLM åº”ç”¨æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹ï¼š

å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æ–¹æ¡ˆï¼šæˆ‘å°†æŒ‡å¯¼æ‚¨ä½¿ç”¨æ–‡æ¡£æ£€ç´¢ (ColPali) å’Œè§†è§‰è¯­è¨€æ¨¡å‹ (VLM) æ„å»º RAG ç³»ç»Ÿã€‚
Phil Schmid çš„æ•™ç¨‹ï¼šæ·±å…¥æ¢è®¨å¦‚ä½•ä½¿ç”¨ TRL å¾®è°ƒå¤šæ¨¡æ€ LLMã€‚
Merve Noyan çš„smol-visionå­˜å‚¨åº“ï¼šä¸€ç³»åˆ—å…³äºå‰æ²¿è§†è§‰å’Œå¤šæ¨¡æ€ AI ä¸»é¢˜çš„å¼•äººå…¥èƒœçš„ç¬”è®°æœ¬ã€‚
å¾®è°ƒ VLM å›¾è¡¨.png

1. å®‰è£…ä¾èµ–é¡¹
æˆ‘ä»¬å…ˆæ¥å®‰è£…ä¸€äº›è¿›è¡Œå¾®è°ƒæ‰€å¿…éœ€çš„åº“ï¼ğŸš€

å·²å¤åˆ¶å·²å¤åˆ¶
 !pip install -U -q git+https://github.com/huggingface/trl.git bitsandbytes peft qwen-vl-utils trackio
 # å·²ä½¿ç”¨ä»¥ä¸‹ç‰ˆæœ¬æµ‹è¯•ï¼štrl==0.22.0.dev0, bitsandbytes==0.47.0, peft==0.17.1, qwen-vl-utils==0.0.11, trackio==0.2.8
æ­£åœ¨å®‰è£…æ„å»ºä¾èµ–é¡¹... [?25l [?25hdone
  è·å–æ„å»º wheel çš„è¦æ±‚... [?25l [?25hdone
  å‡†å¤‡å…ƒæ•°æ® (pyproject.toml)... [?25l [?25hdone 
[2K [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [32m844.5/844.5 kB [0m [31m15.6 MB/s [0m eta [36m0:00:00 [0m 
[2K [90mâ€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“[0m [32m59.6/59.6 MB [0m [31m43.7 MB/s] [0m eta] [36m0:00:00 [0m 
[2K] [90åˆ†é’Ÿâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0åˆ†é’Ÿ [32åˆ†é’Ÿ324.6/324.6 kB [0åˆ†é’Ÿ [31åˆ†é’Ÿ30.2 MB/s [0åˆ†é’Ÿé¢„è®¡ [36åˆ†é’Ÿ0:00:00 [0åˆ†é’Ÿ
[?25å°æ—¶
ç™»å½• Hugging Face ä¸Šä¼ ä½ ç²¾å¿ƒè°ƒæ•´çš„æ¨¡å‹ï¼ğŸ—ï¸

æ‚¨éœ€è¦ä½¿ç”¨ Hugging Face å¸æˆ·è¿›è¡Œèº«ä»½éªŒè¯ï¼Œæ‰èƒ½ç›´æ¥ä»æ­¤ç¬”è®°æœ¬ä¿å­˜å’Œåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
from huggingface_hub import notebook_login 

notebook_login()
2. åŠ è½½æ•°æ®é›†ğŸ“
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åŠ è½½HuggingFaceM4/ChartQAæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«å›¾è¡¨å›¾åƒä»¥åŠç›¸å…³çš„é—®ç­”ï¼Œéå¸¸é€‚åˆç”¨äºè®­ç»ƒè§†è§‰é—®ç­”ä»»åŠ¡ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸ºVLMç”Ÿæˆç³»ç»Ÿæ¶ˆæ¯ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿåƒä¸“å®¶ä¸€æ ·åˆ†æå›¾è¡¨å›¾åƒï¼Œå¹¶æ ¹æ®å›¾åƒæä¾›ç®€æ´æ˜äº†çš„ç­”æ¡ˆã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
system_message = """æ‚¨æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨è´Ÿè´£è§£è¯»å›¾è¡¨å›¾åƒä¸­çš„è§†è§‰æ•°æ®ã€‚
æ‚¨çš„ä»»åŠ¡æ˜¯åˆ†ææä¾›çš„å›¾è¡¨å›¾åƒï¼Œå¹¶ç”¨ç®€æ´çš„ç­”æ¡ˆï¼ˆé€šå¸¸æ˜¯å•ä¸ªè¯ã€æ•°å­—æˆ–çŸ­è¯­ï¼‰æ¥å›ç­”æŸ¥è¯¢ã€‚
å›¾è¡¨ç±»å‹å¤šæ ·ï¼ˆä¾‹å¦‚ï¼ŒæŠ˜çº¿å›¾ã€æŸ±çŠ¶å›¾ï¼‰ï¼ŒåŒ…å«é¢œè‰²ã€æ ‡ç­¾å’Œæ–‡æœ¬ã€‚
è¯·ä¸“æ³¨äºæ ¹æ®è§†è§‰ä¿¡æ¯æä¾›å‡†ç¡®ã€ç®€æ´çš„ç­”æ¡ˆã€‚é™¤éç»å¯¹å¿…è¦ï¼Œå¦åˆ™è¯·é¿å…é¢å¤–è§£é‡Šã€‚"""
æˆ‘ä»¬å°†æŠŠæ•°æ®é›†æ ¼å¼åŒ–ä¸ºèŠå¤©æœºå™¨äººç»“æ„ä»¥è¿›è¡Œäº¤äº’ã€‚æ¯æ¬¡äº¤äº’å°†åŒ…å«ç³»ç»Ÿæ¶ˆæ¯ã€å›¾åƒå’Œç”¨æˆ·æŸ¥è¯¢ï¼Œæœ€åæ˜¯æŸ¥è¯¢çš„ç­”æ¡ˆã€‚

ğŸ’¡æœ‰å…³æ­¤å‹å·çš„æ›´å¤šä½¿ç”¨æŠ€å·§ï¼Œè¯·æŸ¥çœ‹å‹å·å¡ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
def  format_data ( sample ):
     return {
       "images" : [sample[ "image" ]],
       "messages" : [ 

          { "role" : "system" ,
               "content" : [ 
                  { "type" : "text" ,
                       "text" : system_message 
                  } 
              ], 
          }, 
          { "role" : "user" ,
               "content" : [ 
                  { "type" : "image" ,
                       "image" : sample[ "image" ], 
                  }, 
                  { "type" : "text" ,
                       "text" : sample[ 'query' ], 
                  } 
              ], 
          }, 
          { "role" : "assistant" ,
               "content" : [ 
                  { "type" : "text" ,
                       "text" : sample[ "label" ][ 0 ] 
                  } 
              ], 
          }, 
      ] 
      }
              
                      
              
                      
                      
              
                      
å‡ºäºæ•™å­¦ç›®çš„ï¼Œæˆ‘ä»¬ä»…åŠ è½½æ•°æ®é›†ä¸­æ¯ä¸ªåˆ†å‰²éƒ¨åˆ†çš„ 10%ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸éœ€è¦åŠ è½½å…¨éƒ¨æ ·æœ¬ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
from datasets import load_dataset 

dataset_id = "HuggingFaceM4/ChartQA" 
train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=[ 'train[:10%]' , 'val[:10%]' , 'test[:10%]' ])
æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ•°æ®é›†çš„ç»“æ„ã€‚å®ƒåŒ…æ‹¬ä¸€å¼ å›¾åƒã€ä¸€ä¸ªæŸ¥è¯¢ã€ä¸€ä¸ªæ ‡ç­¾ï¼ˆå³ç­”æ¡ˆï¼‰ä»¥åŠæˆ‘ä»¬å°†è¦èˆå¼ƒçš„ç¬¬å››ä¸ªç‰¹å¾ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
è®­ç»ƒæ•°æ®é›†
ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨èŠå¤©æœºå™¨äººç»“æ„æ¥æ ¼å¼åŒ–æ•°æ®ã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºæ¨¡å‹æ­£ç¡®è®¾ç½®äº¤äº’ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
train_dataset = [format_data(sample) for sample in train_dataset] 
eval_dataset = [format_data(sample) for sample in eval_dataset] 
test_dataset = [format_data(sample) for sample in test_dataset]
å·²å¤åˆ¶å·²å¤åˆ¶
train_dataset[ 200 ]
3. åŠ è½½æ¨¡å‹å¹¶æ£€æŸ¥æ€§èƒ½ï¼ğŸ¤”
ç°åœ¨æˆ‘ä»¬å·²ç»åŠ è½½äº†æ•°æ®é›†ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬åŠ è½½æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬æ¥è¯„ä¼°å…¶æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä½¿ç”¨Qwen/Qwen2-VL-7B-Instructï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿç†è§£è§†è§‰æ•°æ®å’Œæ–‡æœ¬çš„è§†è§‰è¯­è¨€æ¨¡å‹ (VLM)ã€‚

å¦‚æœæ‚¨æ­£åœ¨å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆï¼Œè¯·è€ƒè™‘ä»¥ä¸‹å¼€æºé€‰é¡¹ï¼š

Meta AI çš„Llama-3.2-11B-Vision
Mistral AI çš„Pixtral-12B
Allen AI çš„Molmo-7B-D-0924
æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥æŸ¥çœ‹æ’è¡Œæ¦œï¼Œä¾‹å¦‚WildVision Arenaæˆ–OpenVLM æ’è¡Œæ¦œï¼Œä»¥æ‰¾åˆ°è¡¨ç°æœ€ä½³çš„ VLMã€‚

Qwen2_VLæ¶æ„

å·²å¤åˆ¶å·²å¤åˆ¶
import torch
 from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor 

model_id = "Qwen/Qwen2-VL-7B-Instruct"
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œä¸ºæ¨ç†åšå‡†å¤‡ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
model = Qwen2VLForConditionalGeneration.from_pretrained( 
    model_id, 
    device_map= "auto" , 
    torch_dtype=torch.bfloat16 
) 

processor = Qwen2VLProcessor.from_pretrained(model_id)
ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªæ ·æœ¬çš„å†…éƒ¨ç»“æ„ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
train_dataset[ 0 ]
æˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒ…å«ç³»ç»Ÿæ¶ˆæ¯çš„æ ·æœ¬æ¥è¯„ä¼° VLM çš„åŸå§‹ç†è§£èƒ½åŠ›ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„è¾“å…¥ï¼š

å·²å¤åˆ¶å·²å¤åˆ¶
train_dataset[ 0 ][ 'messages' ][ 1 : 2 ]
ç°åœ¨ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸ç¤ºä¾‹å¯¹åº”çš„å›¾è¡¨ã€‚ä½ èƒ½æ ¹æ®å›¾è¡¨ä¿¡æ¯å›ç­”è¿™ä¸ªé—®é¢˜å—ï¼Ÿ

å·²å¤åˆ¶å·²å¤åˆ¶
train_dataset[ 0 ][ 'images' ][ 0 ]
æˆ‘ä»¬æ¥åˆ›å»ºä¸€ä¸ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥æ¨¡å‹ã€å¤„ç†å™¨å’Œæ ·æœ¬ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿç®€åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¹¶è½»æ¾è¯„ä¼°è™šæ‹Ÿé€»è¾‘æ¨¡å‹ï¼ˆVLMï¼‰çš„æ€§èƒ½ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
from qwen_vl_utils import process_vision_info def generate_text_from_sample ( model, processor, sample, max_new_tokens= 1024 , device= "cuda" ):
     # åº”ç”¨èŠå¤©æ¨¡æ¿å‡†å¤‡æ–‡æœ¬è¾“å…¥
    text_input = processor.apply_chat_template( 
        sample[ 'messages' ][ 1 : 2 ],   # ä½¿ç”¨ä¸åŒ…å«ç³»ç»Ÿæ¶ˆæ¯çš„ç¤ºä¾‹
        tokenize= False , 
        add_generation_prompt= True 
    ) # å¤„ç†æ¥è‡ªç¤ºä¾‹çš„è§†è§‰è¾“å…¥
    image_inputs, _ = process_vision_info(sample[ 'messages' ]) # ä¸ºæ¨¡å‹å‡†å¤‡è¾“å…¥
    model_inputs = processor( 
        text=[text_input], 
        images=image_inputs, 
        return_tensors= "pt" , 
    ).to(device)   # å°†è¾“å…¥ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡# ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens) # ä¿®å‰ªç”Ÿæˆçš„ ID ä»¥ç§»é™¤è¾“å…¥ ID 
    trimmed_generated_ids = [ 
        out_ids[ len (in_ids):] for in_ids, out_ids in zip (model_inputs.input_ids, generated_ids) 
    ] # è§£ç è¾“å‡ºæ–‡æœ¬
    output_text = processor.batch_decode( 
        trimmed_generated_ids, 
        skip_special_tokens= True , 
        clean_up_tokenization_spaces= False 
    ) return output_text[ 0 ]   # è¿”å›ç¬¬ä¸€ä¸ªè§£ç åçš„è¾“å‡ºæ–‡æœ¬

 

    

    

    

     

    

    
å·²å¤åˆ¶å·²å¤åˆ¶
# å¦‚ä½•ä½¿ç”¨ç¤ºä¾‹è°ƒç”¨è¯¥æ–¹æ³•ï¼š 
output = generate_text_from_sample(model, processor, train_dataset[ 0 ]) 
output
è™½ç„¶æ¨¡å‹æˆåŠŸè·å–äº†æ­£ç¡®çš„è§†è§‰ä¿¡æ¯ï¼Œä½†å®ƒéš¾ä»¥å‡†ç¡®å›ç­”é—®é¢˜ã€‚è¿™è¡¨æ˜å¾®è°ƒå¯èƒ½æ˜¯æå‡å…¶æ€§èƒ½çš„å…³é”®ã€‚è®©æˆ‘ä»¬å¼€å§‹å¾®è°ƒè¿‡ç¨‹å§ï¼

ç§»é™¤æ¨¡å‹å¹¶æ¸…ç†GPU

åœ¨ä¸‹ä¸€èŠ‚å¼€å§‹è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬æ¸…é™¤å½“å‰å˜é‡å¹¶æ¸…ç† GPU ä»¥é‡Šæ”¾èµ„æºã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
import gc
 import time def clear_memory ():
     # å¦‚æœå˜é‡å­˜åœ¨äºå½“å‰å…¨å±€ä½œç”¨åŸŸä¸­ï¼Œåˆ™åˆ é™¤å®ƒä»¬if 'inputs' in globals (): del globals ()[ 'inputs' ]
     if 'model' in globals (): del globals ()[ 'model' ]
     if 'processor' in globals (): del globals ()[ 'processor' ]
     if 'trainer' in globals (): del globals ()[ 'trainer' ]
     if 'bnb_config' in globals (): del globals ()[ 'bnb_config' ] 
    time.sleep( 2 ) # åƒåœ¾å›æ”¶å¹¶æ¸…é™¤ CUDA å†…å­˜
    gc.collect() 
    time.sleep( 2 ) 
    torch.cuda.empty_cache() 
    torch.cuda.synchronize() 
    time.sleep( 2 ) 
    gc.collect() 
    time.sleep( 2 ) print ( print (f"GPU å·²åˆ†é…å†…å­˜ï¼š{torch.cuda.memory_allocated() / 1024 ** 3 : .2 f} GB" )
     print ( f"GPU ä¿ç•™å†…å­˜ï¼š{torch.cuda.memory_reserved() / 1024 ** 3 : .2 f} GB" ) 
clear_memory()

 
                        

    

    
4. ä½¿ç”¨ TRL å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ
4.1 åŠ è½½é‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒ âš™ï¸
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨bitsandbytesåŠ è½½é‡åŒ–æ¨¡å‹ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºé‡åŒ–çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿™ç¯‡åšæ–‡æˆ–è¿™ç¯‡åšæ–‡ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
from transformers import BitsAndBytesConfig # BitsAndBytesConfig int-4 é…ç½®
bnb_config = BitsAndBytesConfig( 
    load_in_4bit= True , 
    bnb_4bit_use_double_quant= True , 
    bnb_4bit_quant_type= "nf4" , 
    bnb_4bit_compute_dtype=torch.bfloat16 
) # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = Qwen2VLForConditionalGeneration.from_pretrained( 
    model_id, 
    device_map= "auto" , 
    torch_dtype=torch.bfloat16, 
    quantization_config=bnb_config 
) 
processor = Qwen2VLProcessor.from_pretrained(model_id)



4.2 è®¾ç½® QLoRA å’Œ SFTConfig ğŸš€
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é…ç½®QLoRAä»¥ç”¨äºè®­ç»ƒè®¾ç½®ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒQLoRA èƒ½å¤Ÿé«˜æ•ˆåœ°å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚ä¸é€šè¿‡åº”ç”¨ä½ç§©è¿‘ä¼¼æ¥é™ä½å†…å­˜ä½¿ç”¨çš„æ ‡å‡† LoRA ä¸åŒï¼ŒQLoRA æ›´è¿›ä¸€æ­¥ï¼Œé€šè¿‡é‡åŒ– LoRA é€‚é…å™¨çš„æƒé‡æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™è¿›ä¸€æ­¥é™ä½äº†å†…å­˜éœ€æ±‚å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œä½¿å…¶æˆä¸ºåœ¨ä¸ç‰ºç‰²æ¨¡å‹è´¨é‡çš„å‰æä¸‹ä¼˜åŒ–æ¨¡å‹æ€§èƒ½çš„ç»ä½³é€‰æ‹©ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
from peft import LoraConfig # é…ç½® LoRa 
peft_config = LoraConfig( 
    lora_alpha= 16 , 
    lora_dropout= 0.05 , 
    r= 8 , 
    bias= "none" , 
    target_modules=[ "q_proj" , "v_proj" ], 
    task_type= "CAUSAL_LM" , 
)

æˆ‘ä»¬å°†ä½¿ç”¨ç›‘ç£å¼å¾®è°ƒ (SFT) æ¥æå‡æ¨¡å‹åœ¨å½“å‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨TRL åº“ä¸­çš„SFTConfigç±»æ¥å®šä¹‰è®­ç»ƒå‚æ•°ã€‚SFT å…è®¸æˆ‘ä»¬æä¾›å¸¦æ ‡ç­¾çš„æ•°æ®ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ æ ¹æ®æ¥æ”¶åˆ°çš„è¾“å…¥ç”Ÿæˆæ›´å‡†ç¡®çš„å“åº”ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æˆ‘ä»¬çš„ç‰¹å®šç”¨ä¾‹ï¼Œä»è€Œåœ¨ç†è§£å’Œå“åº”è§†è§‰æŸ¥è¯¢æ–¹é¢è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
from trl import SFTConfig # é…ç½®è®­ç»ƒå‚æ•°
training_args = SFTConfig( 
    output_dir= "qwen2-7b-instruct-trl-sft-ChartQA" ,   # æ¨¡å‹ä¿å­˜ç›®å½•
    num_train_epochs= 3 ,   # è®­ç»ƒè½®æ•°
    per_device_train_batch_size= 4 ,   # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size= 4 ,   # è¯„ä¼°æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps= 8 ,   # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    gradient_checkpointing_kwargs={ "use_reentrant" : False },   # æ¢¯åº¦æ£€æŸ¥ç‚¹é€‰é¡¹
    max_length= None ,
     # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è®¾ç½®
    optim= "adamw_torch_fused" ,   # ä¼˜åŒ–å™¨ç±»å‹
    learning_rate= 2e-4 ,   # è®­ç»ƒå­¦ä¹ ç‡# æ—¥å¿—è®°å½•å’Œè¯„ä¼°
    logging_steps= 10 ,   # æ—¥å¿—è®°å½•é—´éš”
    eval_steps= 10 ,   # è¯„ä¼°æ­¥æ•°é—´éš”
    eval_strategy= "steps" ,   # è¯„ä¼°ç­–ç•¥
    save_strategy= "steps" ,   # æ¨¡å‹ä¿å­˜ç­–ç•¥
    save_steps= 20 ,   # ä¿å­˜æ­¥æ•°é—´éš”# æ··åˆç²¾åº¦å’Œæ¢¯åº¦è®¾ç½®
    bf16= True ,   # ä½¿ç”¨ bfloat16 ç²¾åº¦
    max_grad_norm= 0.3 ,   # æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°
    warmup_ratio= 0.03 ,   # é¢„çƒ­æ­¥æ•°å æ€»æ­¥æ•°çš„æ¯”ä¾‹# Hub å’ŒæŠ¥å‘Š
    push_to_hub= True ,   # æ˜¯å¦å°†æ¨¡å‹æ¨é€åˆ° Hugging Face Hub 
    report_to= "trackio" 
, #  ç”¨äºè·Ÿè¸ªæŒ‡æ ‡çš„æŠ¥å‘Šå·¥å…·


    
    
    
4.3 è®­ç»ƒæ¨¡å‹ğŸƒ
æˆ‘ä»¬å°†ä½¿ç”¨Trackio è®°å½•è®­ç»ƒè¿›åº¦ã€‚è®©æˆ‘ä»¬å°†ç¬”è®°æœ¬ç”µè„‘è¿æ¥åˆ° W&Bï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´æ•è·å…³é”®ä¿¡æ¯ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
import trackio  trackio.init(
      project= "qwen2-7b-instruct-trl-sft-ChartQA" ,
      name= "qwen2-7b-instruct-trl-sft-ChartQA" ,
      config=training_args,
      space_id=training_args.output_dir + "-trackio"  )


* Trackio é¡¹ç›®å·²åˆå§‹åŒ–ï¼šqwen2-7b-instruct-trl-sft-ChartQA 
* Trackio æŒ‡æ ‡å°†åŒæ­¥åˆ° Hugging Face æ•°æ®é›†ï¼šsergiopaniego/qwen2-7b-instruct-trl-sft-ChartQA-trackio-dataset 
* åˆ›å»ºæ–°ç©ºé—´ï¼šhttps://huggingface.co/spaces/sergiopaniego/qwen2-7b-instruct-trl-sft-ChartQA-trackio 
* è®¿é—®ä»¥ä¸‹é“¾æ¥æŸ¥çœ‹ä»ªè¡¨ç›˜ï¼šhttps://huggingface.co/spaces/sergiopaniego/qwen2-7b-instruct-trl-sft-ChartQA-trackio
ç°åœ¨ï¼Œæˆ‘ä»¬å°†å®šä¹‰SFTTrainer ï¼Œå®ƒæ˜¯transformers.Trainerç±»çš„å°è£…ï¼Œå¹¶ç»§æ‰¿äº†å…¶å±æ€§å’Œæ–¹æ³•ã€‚å½“æä¾›PeftConfigå¯¹è±¡æ—¶ï¼Œè¯¥ç±»ä¼šæ­£ç¡®åˆå§‹åŒ–PeftModelï¼Œä»è€Œç®€åŒ–å¾®è°ƒè¿‡ç¨‹ã€‚é€šè¿‡ä½¿ç”¨ SFTTrainer ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°ç®¡ç†è®­ç»ƒå·¥ä½œæµç¨‹ï¼Œå¹¶ç¡®ä¿è§†è§‰è¯­è¨€æ¨¡å‹è·å¾—æµç•…çš„å¾®è°ƒä½“éªŒã€‚åœ¨è¿›è¡Œæ¨ç†æ—¶ï¼Œæˆ‘ä»¬å®šä¹‰äº†è‡ªå·±çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ä¹‹å‰åº”ç”¨å¿…è¦çš„é¢„å¤„ç†ã€‚åœ¨è¿™é‡Œï¼ŒSFTTrainer ä¼šè‡ªåŠ¨æ¨æ–­è¯¥æ¨¡å‹æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åº”ç”¨ä¸€ä¸ªå°†è¾“å…¥è½¬æ¢ä¸ºé€‚å½“æ ¼å¼çš„è½¬æ¢å‡½æ•°ã€‚SFTTrainergenerate_text_from_sampleDataCollatorForVisionLanguageModeling

å·²å¤åˆ¶å·²å¤åˆ¶
from trl import SFTTrainer 

trainer = SFTTrainer( 
    model=model, 
    args=training_args, 
    train_dataset=train_dataset, 
    eval_dataset=eval_dataset, 
    peft_config=peft_config, 
    processing_class=processor, 
)
æ˜¯æ—¶å€™è®­ç»ƒæ¨¡å‹äº†ï¼ğŸ‰

å·²å¤åˆ¶å·²å¤åˆ¶
è®­ç»ƒå™¨.è®­ç»ƒ()
è®©æˆ‘ä»¬ä¿å­˜ç»“æœå§ğŸ’¾

å·²å¤åˆ¶å·²å¤åˆ¶
trainer.save_model(training_args.output_dir)
5. æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹ğŸ”
ç°åœ¨æˆ‘ä»¬å·²ç»æˆåŠŸå¾®è°ƒäº†è§†è§‰è¯­è¨€æ¨¡å‹ (VLM)ï¼Œæ˜¯æ—¶å€™è¯„ä¼°å®ƒçš„æ€§èƒ½äº†ï¼åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ChartQA æ•°æ®é›†ä¸­çš„ç¤ºä¾‹æ¥æµ‹è¯•æ¨¡å‹ï¼Œçœ‹çœ‹å®ƒå¦‚ä½•å›ç­”åŸºäºå›¾è¡¨å›¾åƒçš„é—®é¢˜ã€‚è®©æˆ‘ä»¬æ·±å…¥äº†è§£ä¸€ä¸‹ç»“æœå§ï¼ğŸš€

è®©æˆ‘ä»¬æ¸…ç†ä¸€ä¸‹GPUå†…å­˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ğŸ§¹

å·²å¤åˆ¶å·²å¤åˆ¶
æ¸…é™¤å†…å­˜()
æˆ‘ä»¬å°†ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„æµç¨‹é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
model = Qwen2VLForConditionalGeneration.from_pretrained( 
    model_id, 
    device_map= "auto" , 
    torch_dtype=torch.bfloat16 
) 

processor = Qwen2VLProcessor.from_pretrained(model_id)
æˆ‘ä»¬å°†æŠŠè®­ç»ƒå¥½çš„é€‚é…å™¨é™„åŠ åˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸Šã€‚è¯¥é€‚é…å™¨åŒ…å«äº†æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€åšçš„å¾®è°ƒï¼Œä½¿åŸºç¡€æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸æ”¹å˜å…¶æ ¸å¿ƒå‚æ•°çš„æƒ…å†µä¸‹åˆ©ç”¨è¿™äº›æ–°çŸ¥è¯†ã€‚é€šè¿‡é›†æˆè¯¥é€‚é…å™¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¿æŒæ¨¡å‹åŸæœ‰ç»“æ„çš„åŒæ—¶å¢å¼ºå…¶æ€§èƒ½ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
adapter_path = "sergiopaniego/qwen2-7b-instruct-trl-sft-ChartQA" 
model.load_adapter(adapter_path)
æˆ‘ä»¬å°†åˆ©ç”¨æ¨¡å‹æœ€åˆéš¾ä»¥æ­£ç¡®å›ç­”çš„æ•°æ®é›†ä¸­çš„å…ˆå‰æ ·æœ¬ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
train_dataset[ 0 ][ 'messages' ][: 2 ]
å·²å¤åˆ¶å·²å¤åˆ¶
 train_dataset[ 0 ][ 'images' ][ 0 ]

å·²å¤åˆ¶å·²å¤åˆ¶
output = generate_text_from_sample(model, processor, train_dataset[ 0 ]) 
output
ç”±äºè¯¥æ ·æœ¬å–è‡ªè®­ç»ƒé›†ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å·²ç»é‡åˆ°è¿‡å®ƒï¼Œè¿™å¯èƒ½è¢«è§†ä¸ºä¸€ç§ä½œå¼Šè¡Œä¸ºã€‚ä¸ºäº†æ›´å…¨é¢åœ°äº†è§£æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¸€ä¸ªæœªè§è¿‡çš„æ ·æœ¬å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
test_dataset[ 10 ][ 'messages' ][: 2 ]
å·²å¤åˆ¶å·²å¤åˆ¶
 test_dataset[ 10 ][ 'images' ][ 0 ]

å·²å¤åˆ¶å·²å¤åˆ¶
output = generate_text_from_sample(model, processor, test_dataset[ 10 ]) 
output
æ¨¡å‹å·²æˆåŠŸå­¦ä¹ å¹¶èƒ½å“åº”æ•°æ®é›†ä¸­æŒ‡å®šçš„æŸ¥è¯¢ã€‚æˆ‘ä»¬è¾¾æˆç›®æ ‡å•¦ï¼ğŸ‰âœ¨

ğŸ’» æˆ‘å¼€å‘äº†ä¸€ä¸ªç¤ºä¾‹åº”ç”¨ç¨‹åºæ¥æµ‹è¯•è¯¥æ¨¡å‹ï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°å®ƒã€‚æ‚¨å¯ä»¥è½»æ¾åœ°å°†å…¶ä¸å¦ä¸€ä¸ªåŒ…å«é¢„è®­ç»ƒæ¨¡å‹çš„ç©ºé—´è¿›è¡Œæ¯”è¾ƒï¼Œè¯¥ç©ºé—´å¯åœ¨æ­¤å¤„è·å–ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
from IPython.display import IFrame 

IFrame(src= "https://sergiopaniego-qwen2-vl-7b-trl-sft-chartqa.hf.space" , width= 1000 , height= 800 )
6. å¯¹æ¯”å¾®è°ƒæ¨¡å‹ä¸åŸºç¡€æ¨¡å‹+æç¤º ğŸ“Š
æˆ‘ä»¬å·²ç»æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¾®è°ƒ VLM æ¥ä½¿å…¶é€‚åº”æˆ‘ä»¬çš„ç‰¹å®šéœ€æ±‚ã€‚å¦ä¸€ç§å€¼å¾—è€ƒè™‘çš„æ–¹æ³•æ˜¯ç›´æ¥ä½¿ç”¨æç¤ºæˆ–å®æ–½ RAG ç³»ç»Ÿï¼Œè¿™å°†åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­ä»‹ç»ã€‚

å¯¹è™šæ‹Ÿé€»è¾‘æ¨¡å‹ (VLM) è¿›è¡Œå¾®è°ƒéœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè¿™ä¼šäº§ç”Ÿè´¹ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨æç¤ºåŠŸèƒ½ï¼Œçœ‹çœ‹èƒ½å¦åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹è·å¾—ç±»ä¼¼çš„ç»“æœã€‚

è®©æˆ‘ä»¬å†æ¬¡æ¸…ç†GPUå†…å­˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ğŸ§¹

å·²å¤åˆ¶å·²å¤åˆ¶
 clear_memory()
GPUåˆ†é…æ˜¾å­˜ï¼š0.02 GBï¼›
GPUä¿ç•™æ˜¾å­˜ï¼š0.27 GB
ğŸ—ï¸ é¦–å…ˆï¼Œæˆ‘ä»¬å°†æŒ‰ç…§ä¸ä¹‹å‰ç›¸åŒçš„æµç¨‹åŠ è½½åŸºçº¿æ¨¡å‹ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
model = Qwen2VLForConditionalGeneration.from_pretrained( 
    model_id, 
    device_map= "auto" , 
    torch_dtype=torch.bfloat16 
) 

processor = Qwen2VLProcessor.from_pretrained(model_id)
ğŸ“œ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨ä¹‹å‰çš„ç¤ºä¾‹ï¼Œä½†è¿™æ¬¡æˆ‘ä»¬å°†åŒ…å«å¦‚ä¸‹ç³»ç»Ÿæ¶ˆæ¯ã€‚æ·»åŠ æ­¤æ¶ˆæ¯æœ‰åŠ©äºä¸ºæ¨¡å‹æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå¯èƒ½æé«˜å…¶å“åº”å‡†ç¡®ç‡ã€‚

å·²å¤åˆ¶å·²å¤åˆ¶
train_dataset[ 0 ][: 2 ]
è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ï¼

å·²å¤åˆ¶å·²å¤åˆ¶
text = processor.apply_chat_template( 
    train_dataset[ 0 ][: 2 ], tokenize= False , add_generation_prompt= True 
) 

image_inputs, _ = process_vision_info(train_dataset[ 0 ]) 

inputs = processor( 
    text=[text], 
    images=image_inputs, 
    return_tensors= "pt" , 
) 

inputs = inputs.to( "cuda" ) 

generated_ids = model.generate(**inputs, max_new_tokens= 1024 ) 
generated_ids_trimmed = [out_ids[ len (in_ids):] for in_ids, out_ids in  zip (inputs.input_ids, generated_ids)] 

output_text = processor.batch_decode( 
    generated_ids_trimmed, 
    skip_special_tokens= True , 
    clean_up_tokenization_spaces= False 
) 

output_text[ 0 ]
ğŸ’¡ æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¯¥æ¨¡å‹æ— éœ€ä»»ä½•è®­ç»ƒï¼Œå³å¯åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œé¢å¤–çš„ç³»ç»Ÿæ¶ˆæ¯ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆã€‚æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯ï¼Œè¿™ç§æ–¹æ³•æˆ–è®¸å¯ä»¥ä½œä¸ºå¾®è°ƒçš„ä¸€ç§å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚
