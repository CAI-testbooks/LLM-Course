"""
å››æ™ºèƒ½ä½“æ°”è±¡RAGç³»ç»Ÿ
å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶
"""

import numpy as np
import torch
from typing import Dict, List, Tuple, Optional, Any
import json
import pickle
from pathlib import Path
import warnings
import logging
import re
from datetime import datetime, timedelta
import asyncio
from collections import defaultdict

warnings.filterwarnings('ignore')

from sentence_transformers import SentenceTransformer, util
from config import config


class BaseAgent:
    """æ™ºèƒ½ä½“åŸºç±»"""

    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description
        self.logger = logging.getLogger(f"Agent.{name}")

    def log(self, message: str, level: str = "info"):
        """æ—¥å¿—è®°å½•"""
        getattr(self.logger, level)(f"[{self.name}] {message}")

    def validate_input(self, input_data: Any) -> bool:
        """éªŒè¯è¾“å…¥"""
        return True

    def process(self, **kwargs) -> Dict:
        """å¤„ç†å‡½æ•°ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""
        raise NotImplementedError


class RetrievalAgent(BaseAgent):
    """æ£€ç´¢æ™ºèƒ½ä½“ - è´Ÿè´£çŸ¥è¯†æ£€ç´¢"""

    def __init__(self, knowledge_path: str = None):
        super().__init__(
            name="RetrievalAgent",
            description="è´Ÿè´£ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œæä¾›ç§‘å­¦ä¾æ®å’Œé¢„è­¦æŒ‡æ ‡"
        )

        self.knowledge_path = knowledge_path or config.paths.get(
            "knowledge_json",
            "/home/Liyang/agent/knowledge_base.json"
        )

        # åŠ è½½çŸ¥è¯†åº“
        self.knowledge_base = self._load_knowledge_base()

        # åˆå§‹åŒ–æ£€ç´¢æ¨¡å‹
        model_name = config.knowledge_config.get(
            'base_model',
            'paraphrase-multilingual-MiniLM-L12-v2'
        )
        self.model = SentenceTransformer(model_name)

        # å‡†å¤‡æ–‡æ¡£
        self.documents, self.doc_embeddings = self._prepare_documents()

        self.log(f"åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£")

    def _load_knowledge_base(self) -> Dict:
        """åŠ è½½çŸ¥è¯†åº“"""
        knowledge_path = Path(self.knowledge_path)

        if not knowledge_path.exists():
            self.log(f"è­¦å‘Š: çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_path}", "warning")
            return {"items": []}

        try:
            with open(knowledge_path, 'r', encoding='utf-8') as f:
                knowledge_base = json.load(f)
            return knowledge_base
        except Exception as e:
            self.log(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}", "error")
            return {"items": []}

    def _prepare_documents(self) -> Tuple[List[str], np.ndarray]:
        """å‡†å¤‡æ–‡æ¡£å’ŒåµŒå…¥å‘é‡"""
        documents = []

        # æ ¼å¼åŒ–çŸ¥è¯†åº“æ¡ç›®
        for item in self.knowledge_base.get('items', []):
            doc_text = self._format_document(item)
            documents.append(doc_text)

        if not documents:
            # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
            documents = [
                "é«˜æ¸©çƒ­æµª: æ°”æ¸©â‰¥35â„ƒæ—¶å®¹æ˜“å¯¼è‡´ä¸­æš‘ï¼Œå»ºè®®å‡å°‘æˆ·å¤–æ´»åŠ¨ï¼Œå¤šå–æ°´ã€‚",
                "æš´é›¨æ´ªæ°´: çŸ­æ—¶å¼ºé™é›¨å¯èƒ½å¯¼è‡´å†…æ¶ï¼Œæ³¨æ„äº¤é€šå®‰å…¨ï¼Œé¿å…æ¶‰æ°´ã€‚",
                "å°é£é˜²å¾¡: å°é£å¤©æ°”é£åŠ›å¼ºåŠ²ï¼Œè¯·å›ºå®šå¥½é—¨çª—ï¼Œé¿å…å¤–å‡ºã€‚",
                "å¹²æ—±åº”å¯¹: å¹²æ—±å¤©æ°”éœ€è¦èŠ‚çº¦ç”¨æ°´ï¼Œæ³¨æ„é˜²ç«ï¼Œå‡å°‘æˆ·å¤–æ´»åŠ¨ã€‚",
                "å¯’æ½®é˜²æŠ¤: å¯’æ½®å¤©æ°”æ°”æ¸©éª¤é™ï¼Œè¯·æ³¨æ„ä¿æš–ï¼Œé¢„é˜²æ„Ÿå†’ã€‚"
            ]
            self.log("ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£", "warning")

        # è®¡ç®—åµŒå…¥å‘é‡
        embeddings = self.model.encode(
            documents,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        return documents, embeddings

    def _format_document(self, item: Dict) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£"""
        parts = []
        if 'title' in item:
            parts.append(f"æ ‡é¢˜: {item['title']}")
        if 'category' in item:
            parts.append(f"ç±»åˆ«: {item['category']}")
        if 'scientific_basis' in item:
            parts.append(f"ç§‘å­¦ä¾æ®: {item['scientific_basis']}")
        if 'warning_indicators' in item:
            parts.append(f"é¢„è­¦æŒ‡æ ‡: {item['warning_indicators']}")
        return "\n".join(parts)

    def process(self, query: str, top_k: int = 5, **kwargs) -> Dict:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        self.log(f"æ£€ç´¢æŸ¥è¯¢: {query}")

        if len(self.documents) == 0:
            return {
                'success': False,
                'error': 'çŸ¥è¯†åº“ä¸ºç©º',
                'results': []
            }

        try:
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.model.encode(
                query,
                convert_to_numpy=True,
                normalize_embeddings=True
            )

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = util.cos_sim(query_embedding, self.doc_embeddings)[0]

            # è·å–top_kç»“æœ
            top_indices = torch.topk(
                similarities,
                k=min(top_k, len(self.documents))
            ).indices.tolist()

            # æ„å»ºç»“æœ
            results = []
            for idx in top_indices:
                similarity = similarities[idx].item()
                doc_text = self.documents[idx]

                # æå–å…³é”®ä¿¡æ¯
                category = "æœªçŸ¥"
                title = "æ— æ ‡é¢˜"

                # ä»æ–‡æ¡£æ–‡æœ¬ä¸­è§£æä¿¡æ¯
                for line in doc_text.split('\n'):
                    if line.startswith('ç±»åˆ«:'):
                        category = line.replace('ç±»åˆ«:', '').strip()
                    elif line.startswith('æ ‡é¢˜:'):
                        title = line.replace('æ ‡é¢˜:', '').strip()

                results.append({
                    'document': doc_text,
                    'category': category,
                    'title': title,
                    'similarity': similarity,
                    'confidence': min(0.99, similarity * 1.2)  # ç½®ä¿¡åº¦å¢å¼º
                })

            # æŒ‰ç±»åˆ«åˆ†ç»„
            category_groups = defaultdict(list)
            for result in results:
                category_groups[result['category']].append(result)

            # è®¡ç®—ç±»åˆ«æƒé‡
            category_scores = {}
            for category, items in category_groups.items():
                category_scores[category] = sum(item['similarity'] for item in items) / len(items)

            self.log(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")

            return {
                'success': True,
                'query': query,
                'results': results,
                'category_scores': dict(category_scores),
                'total_docs': len(self.documents),
                'top_categories': sorted(category_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            }

        except Exception as e:
            self.log(f"æ£€ç´¢å¤±è´¥: {e}", "error")
            return {
                'success': False,
                'error': str(e),
                'results': []
            }


class AnalysisAgent(BaseAgent):
    """åˆ†ææ™ºèƒ½ä½“ - è´Ÿè´£æ°”è±¡ç‰¹å¾æå–å’Œé£é™©è¯„ä¼°"""

    def __init__(self):
        super().__init__(
            name="AnalysisAgent",
            description="è´Ÿè´£åˆ†ææ°”è±¡ç‰¹å¾ï¼Œè¿›è¡Œé£é™©è¯„ä¼°å’Œè¶‹åŠ¿é¢„æµ‹"
        )

        # åˆå§‹åŒ–ç‰¹å¾æå–è§„åˆ™
        self.feature_rules = self._init_feature_rules()

        # é£é™©è¯„ä¼°æ¨¡å‹
        self.risk_levels = {
            'low': {'min': 0, 'max': 3, 'color': 'ğŸŸ¢', 'description': 'ä½é£é™©'},
            'medium': {'min': 4, 'max': 6, 'color': 'ğŸŸ¡', 'description': 'ä¸­é£é™©'},
            'high': {'min': 7, 'max': 9, 'color': 'ğŸŸ ', 'description': 'é«˜é£é™©'},
            'extreme': {'min': 10, 'max': 12, 'color': 'ğŸ”´', 'description': 'æé«˜é£é™©'}
        }

        self.log("åˆå§‹åŒ–å®Œæˆ")

    def _init_feature_rules(self) -> Dict:
        """åˆå§‹åŒ–ç‰¹å¾æå–è§„åˆ™"""
        return {
            'temperature': {
                'patterns': [
                    r'æ¸©åº¦\s*([0-9]+\.?[0-9]*)\s*â„ƒ',
                    r'([0-9]+\.?[0-9]*)\s*â„ƒ',
                    r'æ°”æ¸©\s*([0-9]+\.?[0-9]*)åº¦'
                ],
                'unit': 'â„ƒ',
                'risk_weight': 1.5
            },
            'humidity': {
                'patterns': [
                    r'æ¹¿åº¦\s*([0-9]+\.?[0-9]*)\s*%',
                    r'([0-9]+\.?[0-9]*)\s*%æ¹¿åº¦'
                ],
                'unit': '%',
                'risk_weight': 1.0
            },
            'precipitation': {
                'patterns': [
                    r'é™é›¨\s*([0-9]+\.?[0-9]*)\s*mm',
                    r'é™æ°´\s*([0-9]+\.?[0-9]*)\s*æ¯«ç±³',
                    r'é›¨é‡\s*([0-9]+\.?[0-9]*)'
                ],
                'unit': 'mm',
                'risk_weight': 1.8
            },
            'wind': {
                'patterns': [
                    r'é£é€Ÿ\s*([0-9]+\.?[0-9]*)\s*m/s',
                    r'é£åŠ›\s*([0-9]+\.?[0-9]*)\s*çº§'
                ],
                'unit': 'm/s',
                'risk_weight': 1.3
            }
        }

    def extract_features(self, query: str) -> Dict:
        """æå–æ°”è±¡ç‰¹å¾"""
        features = {'raw_features': {}, 'keywords': []}

        # æå–æ•°å€¼ç‰¹å¾
        for feature_name, rule in self.feature_rules.items():
            for pattern in rule['patterns']:
                matches = re.findall(pattern, query)
                if matches:
                    values = [float(match) for match in matches if self._is_number(match)]
                    if values:
                        avg_value = sum(values) / len(values)
                        features['raw_features'][feature_name] = {
                            'value': avg_value,
                            'unit': rule['unit'],
                            'risk_weight': rule['risk_weight']
                        }
                        break

        # æå–å…³é”®è¯
        weather_keywords = {
            'é«˜æ¸©': 'heat',
            'çƒ­æµª': 'heatwave',
            'æš´é›¨': 'heavy_rain',
            'å°é£': 'typhoon',
            'å¹²æ—±': 'drought',
            'å¯’æ½®': 'cold_wave',
            'å¤§é£': 'strong_wind',
            'å†°é›¹': 'hail',
            'é›·ç”µ': 'lightning',
            'é›¾éœ¾': 'haze',
            'æ²™å°˜': 'sandstorm'
        }

        for keyword, key in weather_keywords.items():
            if keyword in query:
                features['keywords'].append({
                    'keyword': keyword,
                    'key': key,
                    'risk_level': self._get_keyword_risk(keyword)
                })

        # æå–æ—¶é—´ä¿¡æ¯
        time_keywords = {
            'ä»Šå¤©': 'today',
            'æ˜å¤©': 'tomorrow',
            'åå¤©': 'day_after_tomorrow',
            'æœ¬å‘¨': 'this_week',
            'å‘¨æœ«': 'weekend',
            'æœªæ¥ä¸‰å¤©': 'next_3_days',
            'ä¸‹å‘¨': 'next_week',
            'è¿‘æœŸ': 'recent'
        }

        for keyword, key in time_keywords.items():
            if keyword in query:
                features['time_period'] = key
                break

        self.log(f"æå–åˆ°ç‰¹å¾: {features}")
        return features

    def _is_number(self, s: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å­—"""
        try:
            float(s)
            return True
        except ValueError:
            return False

    def _get_keyword_risk(self, keyword: str) -> str:
        """è·å–å…³é”®è¯é£é™©ç­‰çº§"""
        risk_map = {
            'é«˜æ¸©': 'high', 'çƒ­æµª': 'extreme', 'æš´é›¨': 'high',
            'å°é£': 'extreme', 'å¹²æ—±': 'medium', 'å¯’æ½®': 'medium',
            'å¤§é£': 'medium', 'å†°é›¹': 'high', 'é›·ç”µ': 'high',
            'é›¾éœ¾': 'low', 'æ²™å°˜': 'medium'
        }
        return risk_map.get(keyword, 'low')

    def assess_risk(self, features: Dict, retrieval_results: Dict = None) -> Dict:
        """é£é™©è¯„ä¼°"""
        risk_score = 0
        risk_factors = []

        # 1. æ•°å€¼ç‰¹å¾é£é™©è¯„ä¼°
        for feature_name, feature_data in features.get('raw_features', {}).items():
            value = feature_data['value']
            weight = feature_data['risk_weight']

            # æ ¹æ®ç‰¹å¾å€¼è®¡ç®—é£é™©
            if feature_name == 'temperature':
                if value >= 35:
                    risk_score += 3 * weight
                    risk_factors.append(f"é«˜æ¸©({value}â„ƒ)")
                elif value >= 30:
                    risk_score += 2 * weight
                    risk_factors.append(f"ç‚çƒ­({value}â„ƒ)")

            elif feature_name == 'precipitation':
                if value >= 50:
                    risk_score += 3 * weight
                    risk_factors.append(f"æš´é›¨({value}mm)")
                elif value >= 25:
                    risk_score += 2 * weight
                    risk_factors.append(f"å¤§é›¨({value}mm)")

            elif feature_name == 'wind':
                if value >= 10.8:  # 6çº§é£ä»¥ä¸Š
                    risk_score += 2 * weight
                    risk_factors.append(f"å¤§é£({value}m/s)")

        # 2. å…³é”®è¯é£é™©è¯„ä¼°
        for keyword_data in features.get('keywords', []):
            risk_level = keyword_data['risk_level']
            keyword = keyword_data['keyword']

            if risk_level == 'extreme':
                risk_score += 4
                risk_factors.append(f"{keyword}(æé«˜é£é™©)")
            elif risk_level == 'high':
                risk_score += 3
                risk_factors.append(f"{keyword}(é«˜é£é™©)")
            elif risk_level == 'medium':
                risk_score += 2
                risk_factors.append(f"{keyword}(ä¸­é£é™©)")
            else:
                risk_score += 1
                risk_factors.append(f"{keyword}(ä½é£é™©)")

        # 3. ç»“åˆæ£€ç´¢ç»“æœçš„ç±»åˆ«é£é™©
        if retrieval_results and retrieval_results.get('success'):
            top_categories = retrieval_results.get('top_categories', [])
            for category, score in top_categories:
                if any(high_risk in category for high_risk in ['é«˜æ¸©', 'å°é£', 'æš´é›¨', 'å¹²æ—±']):
                    risk_score += score * 2
                    risk_factors.append(f"ç›¸å…³ç±»åˆ«: {category}")

        # ç¡®å®šé£é™©ç­‰çº§
        risk_level = 'low'
        level_info = None

        for level_name, level_range in self.risk_levels.items():
            if level_range['min'] <= risk_score <= level_range['max']:
                risk_level = level_name
                level_info = level_range
                break
        else:
            # å¦‚æœè¶…è¿‡æœ€å¤§å€¼ï¼Œè®¾ä¸ºæœ€é«˜é£é™©
            risk_level = 'extreme'
            level_info = self.risk_levels['extreme']

        risk_assessment = {
            'risk_score': round(risk_score, 2),
            'risk_level': risk_level,
            'level_info': level_info,
            'risk_factors': risk_factors,
            'risk_components': {
                'feature_risk': round(risk_score * 0.6, 2),
                'keyword_risk': round(risk_score * 0.3, 2),
                'category_risk': round(risk_score * 0.1, 2)
            }
        }

        self.log(f"é£é™©è¯„ä¼°å®Œæˆ: {risk_assessment}")
        return risk_assessment

    def generate_analysis_report(self, query: str, features: Dict,
                                 risk_assessment: Dict) -> Dict:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = {
            'query': query,
            'timestamp': datetime.now().isoformat(),
            'feature_analysis': {},
            'risk_assessment': risk_assessment,
            'trend_analysis': {},
            'confidence': 0.85
        }

        # ç‰¹å¾åˆ†æ
        if features.get('raw_features'):
            report['feature_analysis']['numerical_features'] = features['raw_features']

        if features.get('keywords'):
            report['feature_analysis']['detected_keywords'] = features['keywords']

        if features.get('time_period'):
            report['feature_analysis']['time_period'] = features['time_period']

        # è¶‹åŠ¿åˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰
        trend_indicators = []

        if 'temperature' in features.get('raw_features', {}):
            temp = features['raw_features']['temperature']['value']
            if temp > 30:
                trend_indicators.append("æ¸©åº¦å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå¯èƒ½å‘å±•ä¸ºçƒ­æµªå¤©æ°”")

        if any(k['key'] == 'heavy_rain' for k in features.get('keywords', [])):
            trend_indicators.append("é™æ°´æ¡ä»¶å…·å¤‡ï¼Œå¯èƒ½å‘å±•ä¸ºæŒç»­æ€§é™é›¨")

        report['trend_analysis']['indicators'] = trend_indicators
        report['trend_analysis']['prediction_horizon'] = "æœªæ¥24-48å°æ—¶"

        return report

    def process(self, query: str, retrieval_results: Dict = None, **kwargs) -> Dict:
        """å¤„ç†åˆ†æä»»åŠ¡"""
        self.log(f"åˆ†ææŸ¥è¯¢: {query}")

        try:
            # 1. æå–ç‰¹å¾
            features = self.extract_features(query)

            # 2. é£é™©è¯„ä¼°
            risk_assessment = self.assess_risk(features, retrieval_results)

            # 3. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            analysis_report = self.generate_analysis_report(
                query, features, risk_assessment
            )

            analysis_report['success'] = True
            return analysis_report

        except Exception as e:
            self.log(f"åˆ†æå¤±è´¥: {e}", "error")
            return {
                'success': False,
                'error': str(e),
                'query': query
            }


class DecisionAgent(BaseAgent):
    """å†³ç­–æ™ºèƒ½ä½“ - è´Ÿè´£ç”Ÿæˆåº”å¯¹å»ºè®®å’Œæªæ–½"""

    def __init__(self):
        super().__init__(
            name="DecisionAgent",
            description="æ ¹æ®åˆ†æå’Œæ£€ç´¢ç»“æœï¼Œç”Ÿæˆå…·ä½“åº”å¯¹å»ºè®®å’Œå†³ç­–æ–¹æ¡ˆ"
        )

        # å†³ç­–è§„åˆ™åº“
        self.decision_rules = self._init_decision_rules()

        # å»ºè®®æ¨¡æ¿
        self.recommendation_templates = {
            'high_temperature': [
                "é¿å…åœ¨é«˜æ¸©æ—¶æ®µ(10:00-16:00)è¿›è¡Œæˆ·å¤–æ´»åŠ¨",
                "ç©¿ç€å®½æ¾ã€é€æ°”çš„æµ…è‰²è¡£ç‰©",
                "åŠæ—¶è¡¥å……æ°´åˆ†ï¼Œæ¯å¤©è‡³å°‘é¥®æ°´2-3å‡",
                "ä½¿ç”¨é˜²æ™’éœœ(SPF30+)ï¼Œä½©æˆ´å¤ªé˜³é•œå’Œé®é˜³å¸½",
                "å…³æ³¨è€äººã€å„¿ç«¥å’Œæ…¢æ€§ç—…æ‚£è€…çš„å¥åº·çŠ¶å†µ",
                "å¦‚å‡ºç°å¤´æ™•ã€æ¶å¿ƒç­‰ä¸­æš‘ç—‡çŠ¶ï¼Œç«‹å³åˆ°é˜´å‡‰å¤„ä¼‘æ¯å¹¶å°±åŒ»"
            ],
            'heavy_rain': [
                "å…³æ³¨æ°”è±¡é¢„è­¦ï¼Œé¿å…å‰å¾€ä½æ´¼åœ°å¸¦",
                "é©¾è½¦æ—¶æ³¨æ„å‡é€Ÿæ…¢è¡Œï¼Œä¿æŒå®‰å…¨è½¦è·",
                "é¿å…åœ¨æ ‘ä¸‹ã€å¹¿å‘Šç‰Œä¸‹åœç•™ï¼Œé˜²æ­¢é›·å‡»",
                "æ£€æŸ¥æˆ¿å±‹æ’æ°´ç³»ç»Ÿï¼Œé˜²æ­¢é›¨æ°´å€’çŒ",
                "å‡†å¤‡åº”æ€¥ç…§æ˜å’Œé€šè®¯è®¾å¤‡",
                "å¦‚é‡ç§¯æ°´è·¯æ®µï¼Œä¸è¦å¼ºè¡Œé€šè¿‡"
            ],
            'typhoon': [
                "åŠ å›ºé—¨çª—ï¼Œç§»é™¤é˜³å°ä¸Šçš„æ˜“å è½ç‰©å“",
                "å‚¨å¤‡3å¤©ä»¥ä¸Šçš„é£Ÿç‰©ã€æ°´å’Œè¯å“",
                "é¿å…å¤–å‡ºï¼Œå¦‚éœ€å¤–å‡ºè¯·è¿œç¦»æµ·å²¸å’Œå±±åŒº",
                "å…³æ³¨å®˜æ–¹å‘å¸ƒçš„å°é£è·¯å¾„å’Œé¢„è­¦ä¿¡æ¯",
                "å‡†å¤‡åº”æ€¥ç”µæºï¼Œä¿æŒé€šè®¯ç•…é€š",
                "å°é£è¿‡åæ³¨æ„æ£€æŸ¥æˆ¿å±‹å®‰å…¨ï¼Œé˜²èŒƒæ¬¡ç”Ÿç¾å®³"
            ],
            'drought': [
                "èŠ‚çº¦ç”¨æ°´ï¼Œä¼˜å…ˆä¿è¯ç”Ÿæ´»ç”¨æ°´",
                "è°ƒæ•´å†œä¸šçŒæº‰æ—¶é—´ï¼Œé¿å…ä¸­åˆé«˜æ¸©æ—¶æ®µ",
                "æ³¨æ„é˜²ç«ï¼Œä¸è¦åœ¨æ—åŒºå’Œé‡å¤–ç”¨ç«",
                "åšå¥½ä¸ªäººé˜²æŠ¤ï¼Œé˜²æ­¢çš®è‚¤å¹²ç‡¥å¼€è£‚",
                "å…³æ³¨æ°´åº“è“„æ°´æƒ…å†µå’Œä¾›æ°´é€šçŸ¥",
                "è€ƒè™‘é›¨æ°´æ”¶é›†å’Œä¸­æ°´å›ç”¨"
            ],
            'cold_wave': [
                "æ³¨æ„ä¿æš–ï¼Œç‰¹åˆ«æ˜¯å¤´éƒ¨ã€æ‰‹éƒ¨å’Œè„šéƒ¨",
                "ä½¿ç”¨å–æš–è®¾å¤‡æ—¶æ³¨æ„é€šé£ï¼Œé˜²æ­¢ä¸€æ°§åŒ–ç¢³ä¸­æ¯’",
                "è€äººã€å„¿ç«¥å’Œä½“å¼±è€…å‡å°‘å¤–å‡º",
                "æ³¨æ„æ°´ç®¡é˜²å†»ï¼Œé˜²æ­¢çˆ†è£‚",
                "é€‚å½“å¢åŠ é«˜çƒ­é‡é£Ÿç‰©æ‘„å…¥",
                "å…³æ³¨å¤©æ°”é¢„æŠ¥ï¼ŒåŠæ—¶æ·»åŠ è¡£ç‰©"
            ],
            'general': [
                "å…³æ³¨å½“åœ°æ°”è±¡éƒ¨é—¨çš„æœ€æ–°é¢„æŠ¥å’Œé¢„è­¦",
                "æ ¹æ®å¤©æ°”å˜åŒ–åŠæ—¶è°ƒæ•´å‡ºè¡Œè®¡åˆ’",
                "ä¿æŒé€šè®¯ç•…é€šï¼Œéšæ—¶äº†è§£å¤©æ°”ä¿¡æ¯",
                "å‡†å¤‡å¿…è¦çš„åº”æ€¥ç‰©èµ„",
                "å­¦ä¹ åŸºæœ¬çš„é˜²ç¾å‡ç¾çŸ¥è¯†"
            ]
        }

        self.log("åˆå§‹åŒ–å®Œæˆ")

    def _init_decision_rules(self) -> Dict:
        """åˆå§‹åŒ–å†³ç­–è§„åˆ™"""
        return {
            'heatwave': {
                'conditions': ['temperature>=35', 'has_heatwave'],
                'priority': 1,
                'action_type': 'immediate'
            },
            'heavy_rain_alert': {
                'conditions': ['precipitation>=50', 'has_heavy_rain'],
                'priority': 1,
                'action_type': 'immediate'
            },
            'typhoon_warning': {
                'conditions': ['has_typhoon', 'wind>=10.8'],
                'priority': 1,
                'action_type': 'emergency'
            },
            'drought_alert': {
                'conditions': ['has_drought', 'humidity<=30'],
                'priority': 2,
                'action_type': 'monitor'
            },
            'cold_protection': {
                'conditions': ['temperature<=10', 'has_cold_wave'],
                'priority': 2,
                'action_type': 'preventive'
            }
        }

    def generate_decisions(self, analysis_report: Dict,
                           retrieval_results: Dict) -> List[Dict]:
        """ç”Ÿæˆå†³ç­–å»ºè®®"""
        decisions = []

        # è·å–åˆ†æç»“æœ
        features = analysis_report.get('feature_analysis', {})
        risk_assessment = analysis_report.get('risk_assessment', {})
        risk_level = risk_assessment.get('risk_level', 'low')

        # ç¡®å®šéœ€è¦åº”å¯¹çš„å¤©æ°”ç±»å‹
        weather_types = set()

        # ä»å…³é”®è¯ä¸­æå–
        for keyword_data in features.get('detected_keywords', []):
            key = keyword_data['key']
            if key in ['heat', 'heatwave']:
                weather_types.add('high_temperature')
            elif key in ['heavy_rain', 'typhoon']:
                weather_types.add(key)
            elif key in ['drought', 'cold_wave']:
                weather_types.add(key)

        # ä»æ•°å€¼ç‰¹å¾ä¸­åˆ¤æ–­
        numerical_features = features.get('numerical_features', {})
        if 'temperature' in numerical_features:
            temp = numerical_features['temperature']['value']
            if temp >= 35:
                weather_types.add('high_temperature')
            elif temp <= 10:
                weather_types.add('cold_wave')

        if 'precipitation' in numerical_features:
            precip = numerical_features['precipitation']['value']
            if precip >= 50:
                weather_types.add('heavy_rain')

        # å¦‚æœæ²¡æœ‰ç‰¹å®šå¤©æ°”ç±»å‹ï¼Œä½¿ç”¨é€šç”¨å»ºè®®
        if not weather_types:
            weather_types.add('general')

        # æ ¹æ®é£é™©ç­‰çº§è°ƒæ•´å»ºè®®å¼ºåº¦
        priority_map = {
            'extreme': 'ç´§æ€¥åº”å¯¹',
            'high': 'é«˜åº¦é‡è§†',
            'medium': 'åŠ å¼ºé˜²èŒƒ',
            'low': 'æ­£å¸¸å…³æ³¨'
        }

        priority = priority_map.get(risk_level, 'æ­£å¸¸å…³æ³¨')

        # ç”Ÿæˆå…·ä½“å†³ç­–
        for weather_type in weather_types:
            if weather_type in self.recommendation_templates:
                recommendations = self.recommendation_templates[weather_type]

                # æ ¹æ®é£é™©ç­‰çº§è°ƒæ•´å»ºè®®æ•°é‡
                if risk_level in ['low', 'medium']:
                    recommendations = recommendations[:3]
                elif risk_level == 'high':
                    recommendations = recommendations[:5]
                # extremeé£é™©ç­‰çº§ä½¿ç”¨æ‰€æœ‰å»ºè®®

                decisions.append({
                    'weather_type': weather_type,
                    'priority': priority,
                    'recommendations': recommendations,
                    'applicable_conditions': self._get_applicable_conditions(weather_type)
                })

        # ä»æ£€ç´¢ç»“æœä¸­æå–é¢å¤–å»ºè®®
        if retrieval_results.get('success'):
            top_results = retrieval_results.get('results', [])[:2]
            for result in top_results:
                doc_text = result.get('document', '')
                # ä»æ–‡æ¡£ä¸­æå–å…³é”®å»ºè®®
                if 'åº”å¯¹:' in doc_text:
                    response_part = doc_text.split('åº”å¯¹:')[1]
                    key_points = [p.strip() for p in response_part.split('ã€‚') if p.strip()]

                    if key_points:
                        decisions.append({
                            'weather_type': result.get('category', 'é€šç”¨'),
                            'priority': 'çŸ¥è¯†åº“å»ºè®®',
                            'recommendations': key_points[:3],
                            'source': 'çŸ¥è¯†åº“',
                            'confidence': result.get('confidence', 0.7)
                        })

        self.log(f"ç”Ÿæˆ {len(decisions)} ä¸ªå†³ç­–å»ºè®®")
        return decisions

    def _get_applicable_conditions(self, weather_type: str) -> List[str]:
        """è·å–é€‚ç”¨æ¡ä»¶"""
        conditions_map = {
            'high_temperature': ['æ°”æ¸©â‰¥35â„ƒ', 'ç›¸å¯¹æ¹¿åº¦â‰¥60%', 'è¿ç»­é«˜æ¸©â‰¥3å¤©'],
            'heavy_rain': ['å°æ—¶é™é›¨é‡â‰¥50mm', 'æŒç»­é™é›¨â‰¥3å°æ—¶', 'ä¼´æœ‰é›·ç”µ'],
            'typhoon': ['é£åŠ›â‰¥10çº§', 'ä¼´æœ‰æš´é›¨', 'é£æš´æ½®é¢„è­¦'],
            'drought': ['è¿ç»­æ— é™æ°´â‰¥15å¤©', 'åœŸå£¤æ¹¿åº¦â‰¤30%', 'æ°´åº“è“„æ°´ä¸è¶³'],
            'cold_wave': ['24å°æ—¶é™æ¸©â‰¥8â„ƒ', 'æœ€ä½æ°”æ¸©â‰¤0â„ƒ', 'ä¼´æœ‰å¤§é£']
        }
        return conditions_map.get(weather_type, ['é€šç”¨å¤©æ°”æ¡ä»¶'])

    def generate_action_plan(self, decisions: List[Dict]) -> Dict:
        """ç”Ÿæˆè¡ŒåŠ¨æ–¹æ¡ˆ"""
        action_plan = {
            'immediate_actions': [],
            'short_term_actions': [],
            'monitoring_actions': [],
            'preparedness_actions': []
        }

        for decision in decisions:
            weather_type = decision['weather_type']
            priority = decision['priority']
            recommendations = decision['recommendations']

            if priority in ['ç´§æ€¥åº”å¯¹', 'é«˜åº¦é‡è§†']:
                action_plan['immediate_actions'].extend(recommendations[:2])
                action_plan['short_term_actions'].extend(recommendations[2:4])
            elif priority == 'åŠ å¼ºé˜²èŒƒ':
                action_plan['short_term_actions'].extend(recommendations[:3])
                action_plan['monitoring_actions'].extend(recommendations[3:])
            else:
                action_plan['preparedness_actions'].extend(recommendations[:3])

        # å»é‡
        for key in action_plan:
            action_plan[key] = list(set(action_plan[key]))

        return action_plan

    def process(self, analysis_report: Dict, retrieval_results: Dict, **kwargs) -> Dict:
        """å¤„ç†å†³ç­–ä»»åŠ¡"""
        self.log(f"ç”Ÿæˆå†³ç­–å»ºè®®")

        try:
            # 1. ç”Ÿæˆå†³ç­–å»ºè®®
            decisions = self.generate_decisions(analysis_report, retrieval_results)

            # 2. ç”Ÿæˆè¡ŒåŠ¨æ–¹æ¡ˆ
            action_plan = self.generate_action_plan(decisions)

            # 3. ç”Ÿæˆå†³ç­–æŠ¥å‘Š
            decision_report = {
                'success': True,
                'decisions': decisions,
                'action_plan': action_plan,
                'summary': self._generate_decision_summary(decisions),
                'timestamp': datetime.now().isoformat()
            }

            return decision_report

        except Exception as e:
            self.log(f"å†³ç­–ç”Ÿæˆå¤±è´¥: {e}", "error")
            return {
                'success': False,
                'error': str(e)
            }

    def _generate_decision_summary(self, decisions: List[Dict]) -> str:
        """ç”Ÿæˆå†³ç­–æ‘˜è¦"""
        if not decisions:
            return "å½“å‰å¤©æ°”æ¡ä»¶æ­£å¸¸ï¼Œå»ºè®®å…³æ³¨å¸¸è§„å¤©æ°”é¢„æŠ¥"

        summary_parts = []
        for decision in decisions:
            weather_type = decision['weather_type']
            priority = decision['priority']

            if weather_type == 'high_temperature':
                summary_parts.append(f"é«˜æ¸©å¤©æ°”ï¼Œ{priority}")
            elif weather_type == 'heavy_rain':
                summary_parts.append(f"æš´é›¨å¤©æ°”ï¼Œ{priority}")
            elif weather_type == 'typhoon':
                summary_parts.append(f"å°é£å¤©æ°”ï¼Œ{priority}")
            elif weather_type == 'drought':
                summary_parts.append(f"å¹²æ—±å¤©æ°”ï¼Œ{priority}")
            elif weather_type == 'cold_wave':
                summary_parts.append(f"å¯’æ½®å¤©æ°”ï¼Œ{priority}")

        return "ï¼›".join(summary_parts) if summary_parts else "å¤©æ°”æ¡ä»¶å¤æ‚ï¼Œè¯·å…³æ³¨è¯¦ç»†å»ºè®®"


class CoordinatorAgent(BaseAgent):
    """åè°ƒæ™ºèƒ½ä½“ - è´Ÿè´£åè°ƒå…¶ä»–æ™ºèƒ½ä½“å·¥ä½œ"""

    def __init__(self, agents: Dict[str, BaseAgent]):
        super().__init__(
            name="CoordinatorAgent",
            description="åè°ƒå’Œç®¡ç†å„æ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼Œæ•´åˆæœ€ç»ˆç»“æœ"
        )

        self.agents = agents
        self.workflow_status = {}
        self.results_cache = {}

        self.log(f"åˆå§‹åŒ–å®Œæˆï¼Œç®¡ç† {len(agents)} ä¸ªæ™ºèƒ½ä½“")

    async def execute_workflow(self, query: str) -> Dict:
        """æ‰§è¡Œå·¥ä½œæµç¨‹"""
        self.log(f"å¼€å§‹æ‰§è¡Œå·¥ä½œæµç¨‹ï¼ŒæŸ¥è¯¢: {query}")

        workflow_id = f"wf_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.workflow_status[workflow_id] = {
            'query': query,
            'start_time': datetime.now().isoformat(),
            'status': 'running',
            'agents_status': {}
        }

        try:
            # 1. æ£€ç´¢é˜¶æ®µ
            self.log("é˜¶æ®µ1: çŸ¥è¯†æ£€ç´¢")
            retrieval_agent = self.agents.get('retrieval')
            if retrieval_agent:
                retrieval_result = await self._execute_agent_task(
                    retrieval_agent, 'retrieval', workflow_id, query=query
                )
                self.results_cache['retrieval'] = retrieval_result
            else:
                retrieval_result = {'success': False, 'error': 'æ£€ç´¢æ™ºèƒ½ä½“æœªé…ç½®'}

            # 2. åˆ†æé˜¶æ®µ
            self.log("é˜¶æ®µ2: ç‰¹å¾åˆ†æä¸é£é™©è¯„ä¼°")
            analysis_agent = self.agents.get('analysis')
            if analysis_agent:
                analysis_result = await self._execute_agent_task(
                    analysis_agent, 'analysis', workflow_id,
                    query=query, retrieval_results=retrieval_result
                )
                self.results_cache['analysis'] = analysis_result
            else:
                analysis_result = {'success': False, 'error': 'åˆ†ææ™ºèƒ½ä½“æœªé…ç½®'}

            # 3. å†³ç­–é˜¶æ®µ
            self.log("é˜¶æ®µ3: å†³ç­–å»ºè®®ç”Ÿæˆ")
            decision_agent = self.agents.get('decision')
            if decision_agent:
                decision_result = await self._execute_agent_task(
                    decision_agent, 'decision', workflow_id,
                    analysis_report=analysis_result,
                    retrieval_results=retrieval_result
                )
                self.results_cache['decision'] = decision_result
            else:
                decision_result = {'success': False, 'error': 'å†³ç­–æ™ºèƒ½ä½“æœªé…ç½®'}

            # 4. æ•´åˆç»“æœ
            self.log("é˜¶æ®µ4: ç»“æœæ•´åˆ")
            final_result = self._integrate_results(
                query, retrieval_result, analysis_result, decision_result
            )

            # æ›´æ–°å·¥ä½œæµçŠ¶æ€
            self.workflow_status[workflow_id].update({
                'end_time': datetime.now().isoformat(),
                'status': 'completed',
                'final_result': final_result.get('success', False)
            })

            self.log(f"å·¥ä½œæµç¨‹å®Œæˆ: {workflow_id}")
            return final_result

        except Exception as e:
            self.log(f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}", "error")

            self.workflow_status[workflow_id].update({
                'end_time': datetime.now().isoformat(),
                'status': 'failed',
                'error': str(e)
            })

            return {
                'success': False,
                'error': f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}",
                'query': query,
                'workflow_id': workflow_id
            }

    async def _execute_agent_task(self, agent: BaseAgent, agent_name: str,
                                  workflow_id: str, **kwargs) -> Dict:
        """æ‰§è¡Œæ™ºèƒ½ä½“ä»»åŠ¡"""
        try:
            start_time = datetime.now()

            # è®°å½•å¼€å§‹çŠ¶æ€
            self.workflow_status[workflow_id]['agents_status'][agent_name] = {
                'status': 'running',
                'start_time': start_time.isoformat()
            }

            # æ‰§è¡Œä»»åŠ¡
            result = agent.process(**kwargs)

            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            self.workflow_status[workflow_id]['agents_status'][agent_name].update({
                'status': 'completed' if result.get('success') else 'failed',
                'end_time': end_time.isoformat(),
                'duration': duration,
                'success': result.get('success', False)
            })

            agent.log(f"ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return result

        except Exception as e:
            agent.log(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", "error")

            self.workflow_status[workflow_id]['agents_status'][agent_name] = {
                'status': 'failed',
                'error': str(e),
                'end_time': datetime.now().isoformat()
            }

            return {
                'success': False,
                'error': f"{agent_name}æ‰§è¡Œå¤±è´¥: {str(e)}"
            }

    def _integrate_results(self, query: str, retrieval_result: Dict,
                           analysis_result: Dict, decision_result: Dict) -> Dict:
        """æ•´åˆæ‰€æœ‰ç»“æœ"""
        final_result = {
            'success': all([
                retrieval_result.get('success', False),
                analysis_result.get('success', False),
                decision_result.get('success', False)
            ]),
            'query': query,
            'timestamp': datetime.now().isoformat(),
            'components': {
                'retrieval': retrieval_result.get('success', False),
                'analysis': analysis_result.get('success', False),
                'decision': decision_result.get('success', False)
            }
        }

        if final_result['success']:
            # æ•´åˆæˆåŠŸç»“æœ
            final_result.update({
                'knowledge_retrieval': {
                    'total_docs': retrieval_result.get('total_docs', 0),
                    'relevant_docs': len(retrieval_result.get('results', [])),
                    'top_categories': retrieval_result.get('top_categories', [])
                },
                'risk_assessment': analysis_result.get('risk_assessment', {}),
                'decisions': decision_result.get('decisions', []),
                'action_plan': decision_result.get('action_plan', {}),
                'summary': decision_result.get('summary', '')
            })

            # ç”Ÿæˆæœ€ç»ˆå“åº”
            final_result['response'] = self._generate_final_response(
                query, analysis_result, decision_result
            )

            # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
            confidence_sources = []
            if retrieval_result.get('results'):
                conf = retrieval_result['results'][0].get('confidence', 0) if retrieval_result['results'] else 0
                confidence_sources.append(conf)

            if 'confidence' in analysis_result:
                confidence_sources.append(analysis_result['confidence'])

            final_result['confidence'] = sum(confidence_sources) / len(
                confidence_sources) if confidence_sources else 0.7

        else:
            # å¤„ç†å¤±è´¥æƒ…å†µ
            errors = []
            if not retrieval_result.get('success'):
                errors.append(f"æ£€ç´¢å¤±è´¥: {retrieval_result.get('error')}")
            if not analysis_result.get('success'):
                errors.append(f"åˆ†æå¤±è´¥: {analysis_result.get('error')}")
            if not decision_result.get('success'):
                errors.append(f"å†³ç­–å¤±è´¥: {decision_result.get('error')}")

            final_result['errors'] = errors
            final_result['response'] = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {'; '.join(errors)}"

        return final_result

    def _generate_final_response(self, query: str, analysis_result: Dict,
                                 decision_result: Dict) -> str:
        """ç”Ÿæˆæœ€ç»ˆå“åº”æ–‡æœ¬"""
        risk_assessment = analysis_result.get('risk_assessment', {})
        decisions = decision_result.get('decisions', [])
        action_plan = decision_result.get('action_plan', {})

        lines = []

        # æ ‡é¢˜
        lines.append("=" * 60)
        lines.append(f"ğŸŒ¤ï¸ æ°”è±¡æ™ºèƒ½åˆ†ææŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append("=" * 60)
        lines.append(f"ğŸ“ æŸ¥è¯¢: {query}")
        lines.append("")

        # é£é™©è¯„ä¼°
        risk_level = risk_assessment.get('risk_level', 'low')
        level_info = risk_assessment.get('level_info', {})

        lines.append("âš ï¸ **é£é™©è¯„ä¼°**")
        lines.append(f"- é£é™©ç­‰çº§: {level_info.get('description', 'æœªçŸ¥')} {level_info.get('color', '')}")
        lines.append(f"- é£é™©è¯„åˆ†: {risk_assessment.get('risk_score', 0)}/12")

        risk_factors = risk_assessment.get('risk_factors', [])
        if risk_factors:
            lines.append(f"- ä¸»è¦é£é™©å› ç´ :")
            for factor in risk_factors[:3]:
                lines.append(f"  â€¢ {factor}")

        lines.append("")

        # å†³ç­–å»ºè®®
        lines.append("ğŸ’¡ **å†³ç­–å»ºè®®**")
        for i, decision in enumerate(decisions[:3], 1):
            weather_type = decision['weather_type']
            priority = decision['priority']
            lines.append(f"{i}. {weather_type} - {priority}")

            for rec in decision.get('recommendations', [])[:2]:
                lines.append(f"   âœ“ {rec}")

        lines.append("")

        # è¡ŒåŠ¨æ–¹æ¡ˆ
        lines.append("ğŸš€ **è¡ŒåŠ¨æ–¹æ¡ˆ**")

        if action_plan.get('immediate_actions'):
            lines.append("ç«‹å³è¡ŒåŠ¨:")
            for action in action_plan['immediate_actions'][:3]:
                lines.append(f"â€¢ {action}")

        if action_plan.get('short_term_actions'):
            lines.append("çŸ­æœŸè¡ŒåŠ¨:")
            for action in action_plan['short_term_actions'][:3]:
                lines.append(f"â€¢ {action}")

        lines.append("")
        lines.append("=" * 60)
        lines.append("ğŸ”¬ åˆ†æç³»ç»Ÿï¼šå››æ™ºèƒ½ä½“åä½œæ¡†æ¶")
        lines.append("  1. æ£€ç´¢æ™ºèƒ½ä½“ - çŸ¥è¯†åº“æ£€ç´¢")
        lines.append("  2. åˆ†ææ™ºèƒ½ä½“ - ç‰¹å¾åˆ†æä¸é£é™©è¯„ä¼°")
        lines.append("  3. å†³ç­–æ™ºèƒ½ä½“ - å»ºè®®ä¸æ–¹æ¡ˆç”Ÿæˆ")
        lines.append("  4. åè°ƒæ™ºèƒ½ä½“ - å·¥ä½œæµç¨‹ç®¡ç†")
        lines.append("=" * 60)

        return "\n".join(lines)

    def get_workflow_status(self, workflow_id: str = None) -> Dict:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        if workflow_id:
            return self.workflow_status.get(workflow_id, {})
        return self.workflow_status

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.results_cache.clear()
        self.log("ç¼“å­˜å·²æ¸…ç©º")


class MultiAgentSystem:
    """å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ"""

    def __init__(self, output_dir: str = None):
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            base_dir = config.paths.get("finetune_output", "/home/Liyang/agent/finetune_output")
            self.output_dir = Path(base_dir) / "multi_agent_system"
        else:
            self.output_dir = Path(output_dir)

        self.output_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self.logger.info("åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ...")
        self.agents = self._initialize_agents()
        self.coordinator = CoordinatorAgent(self.agents)

        self.logger.info("âœ… å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.output_dir / "multi_agent_system.log"

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

        self.logger = logging.getLogger("MultiAgentSystem")

    def _initialize_agents(self) -> Dict[str, BaseAgent]:
        """åˆå§‹åŒ–æ‰€æœ‰æ™ºèƒ½ä½“"""
        agents = {}

        # 1. æ£€ç´¢æ™ºèƒ½ä½“
        try:
            retrieval_agent = RetrievalAgent()
            agents['retrieval'] = retrieval_agent
            self.logger.info(f"âœ… åˆå§‹åŒ–æ£€ç´¢æ™ºèƒ½ä½“: {retrieval_agent.name}")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–æ£€ç´¢æ™ºèƒ½ä½“å¤±è´¥: {e}")

        # 2. åˆ†ææ™ºèƒ½ä½“
        try:
            analysis_agent = AnalysisAgent()
            agents['analysis'] = analysis_agent
            self.logger.info(f"âœ… åˆå§‹åŒ–åˆ†ææ™ºèƒ½ä½“: {analysis_agent.name}")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–åˆ†ææ™ºèƒ½ä½“å¤±è´¥: {e}")

        # 3. å†³ç­–æ™ºèƒ½ä½“
        try:
            decision_agent = DecisionAgent()
            agents['decision'] = decision_agent
            self.logger.info(f"âœ… åˆå§‹åŒ–å†³ç­–æ™ºèƒ½ä½“: {decision_agent.name}")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å†³ç­–æ™ºèƒ½ä½“å¤±è´¥: {e}")

        return agents

    async def process_query_async(self, query: str) -> Dict:
        """å¼‚æ­¥å¤„ç†æŸ¥è¯¢"""
        return await self.coordinator.execute_workflow(query)

    def process_query(self, query: str) -> Dict:
        """åŒæ­¥å¤„ç†æŸ¥è¯¢"""
        try:
            # åˆ›å»ºäº‹ä»¶å¾ªç¯
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            # æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
            result = loop.run_until_complete(self.process_query_async(query))
            loop.close()

            return result
        except Exception as e:
            self.logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'query': query
            }

    def save_result(self, result: Dict):
        """ä¿å­˜ç»“æœ"""
        if not result.get('success'):
            self.logger.warning(f"ç»“æœä¸æˆåŠŸï¼Œä¸ä¿å­˜: {result.get('error')}")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = self.output_dir / f"result_{timestamp}.json"

        # ä¿å­˜å®Œæ•´ç»“æœ
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        # ä¿å­˜å“åº”æ–‡æœ¬
        response_file = self.output_dir / f"response_{timestamp}.txt"
        with open(response_file, 'w', encoding='utf-8') as f:
            f.write(result.get('response', ''))

        self.logger.info(f"ç»“æœä¿å­˜åˆ°: {result_file}")
        self.logger.info(f"å“åº”ä¿å­˜åˆ°: {response_file}")

    def batch_process(self, queries: List[str]) -> List[Dict]:
        """æ‰¹é‡å¤„ç†æŸ¥è¯¢"""
        self.logger.info(f"æ‰¹é‡å¤„ç† {len(queries)} ä¸ªæŸ¥è¯¢")

        results = []

        for i, query in enumerate(queries, 1):
            self.logger.info(f"å¤„ç†æŸ¥è¯¢ {i}/{len(queries)}: {query}")

            try:
                result = self.process_query(query)
                results.append(result)

                # ä¿å­˜æ¯ä¸ªç»“æœ
                if result.get('success'):
                    self.save_result(result)

                # è¿›åº¦æŠ¥å‘Š
                if i % 5 == 0:
                    success_count = sum(1 for r in results if r.get('success'))
                    self.logger.info(f"è¿›åº¦: {i}/{len(queries)}, æˆåŠŸ: {success_count}")

            except Exception as e:
                self.logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥: {query}, é”™è¯¯: {e}")
                results.append({
                    'query': query,
                    'success': False,
                    'error': str(e)
                })

        # ä¿å­˜æ‰¹é‡ç»“æœæ‘˜è¦
        batch_summary = {
            'total_queries': len(queries),
            'successful': sum(1 for r in results if r.get('success')),
            'failed': sum(1 for r in results if not r.get('success')),
            'avg_confidence': np.mean([
                r.get('confidence', 0) for r in results if r.get('success')
            ]) if any(r.get('success') for r in results) else 0,
            'timestamp': datetime.now().isoformat()
        }

        batch_file = self.output_dir / f"batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(batch_summary, f, indent=2, ensure_ascii=False)

        self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ‘˜è¦ä¿å­˜åˆ°: {batch_file}")
        self.logger.info(f"æˆåŠŸ: {batch_summary['successful']}, å¤±è´¥: {batch_summary['failed']}")

        return results

    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("=" * 60)
        print("ğŸ¤– å››æ™ºèƒ½ä½“æ°”è±¡åˆ†æç³»ç»Ÿ")
        print("=" * 60)
        print("ç³»ç»Ÿæ¶æ„:")
        print("  1. æ£€ç´¢æ™ºèƒ½ä½“ - ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯")
        print("  2. åˆ†ææ™ºèƒ½ä½“ - æå–ç‰¹å¾å¹¶è¿›è¡Œé£é™©è¯„ä¼°")
        print("  3. å†³ç­–æ™ºèƒ½ä½“ - ç”Ÿæˆåº”å¯¹å»ºè®®å’Œè¡ŒåŠ¨æ–¹æ¡ˆ")
        print("  4. åè°ƒæ™ºèƒ½ä½“ - ç®¡ç†æ•´ä¸ªå·¥ä½œæµç¨‹")
        print("=" * 60)
        print("è¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ï¼Œè¾“å…¥ 'status' æŸ¥çœ‹çŠ¶æ€")
        print()

        while True:
            try:
                user_input = input("è¯·è¾“å…¥æ°”è±¡æŸ¥è¯¢: ").strip()

                if user_input.lower() == 'quit':
                    print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break

                if user_input.lower() == 'help':
                    self._show_help()
                    continue

                if user_input.lower() == 'status':
                    self._show_system_status()
                    continue

                if not user_input:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„æŸ¥è¯¢")
                    continue

                # å¤„ç†æŸ¥è¯¢
                print("\n" + "=" * 40)
                print("ğŸš€ å¯åŠ¨å››æ™ºèƒ½ä½“åä½œæµç¨‹...")

                result = self.process_query(user_input)

                if result.get('success'):
                    # æ˜¾ç¤ºå“åº”
                    print("\n" + "=" * 60)
                    print(result.get('response', 'æ— å“åº”'))
                    print("=" * 60)

                    # ä¿å­˜ç»“æœ
                    self.save_result(result)
                else:
                    print(f"\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                print()

            except KeyboardInterrupt:
                print("\n\nç¨‹åºè¢«ä¸­æ–­ï¼Œé€€å‡º...")
                break
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {e}")
                continue

    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©"""
        help_text = """
        å››æ™ºèƒ½ä½“æ°”è±¡åˆ†æç³»ç»Ÿ - å¸®åŠ©

        ç³»ç»Ÿæ¶æ„:
          æ£€ç´¢æ™ºèƒ½ä½“: ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ°”è±¡çŸ¥è¯†
          åˆ†ææ™ºèƒ½ä½“: åˆ†ææ°”è±¡ç‰¹å¾ï¼Œè¿›è¡Œé£é™©è¯„ä¼°
          å†³ç­–æ™ºèƒ½ä½“: ç”Ÿæˆåº”å¯¹å»ºè®®å’Œè¡ŒåŠ¨æ–¹æ¡ˆ
          åè°ƒæ™ºèƒ½ä½“: ç®¡ç†æ•´ä¸ªå·¥ä½œæµç¨‹

        æŸ¥è¯¢ç¤ºä¾‹:
          1. æ¸©åº¦æŸ¥è¯¢:
            - "ä»Šå¤©æ¸©åº¦35â„ƒä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ"
            - "é«˜æ¸©40åº¦éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"

          2. é™æ°´æŸ¥è¯¢:
            - "æ˜å¤©é™é›¨50mmå¦‚ä½•åº”å¯¹ï¼Ÿ"
            - "æš´é›¨å¤©æ°”å®‰å…¨æŒ‡å—"

          3. ç‰¹æ®Šå¤©æ°”:
            - "å°é£æ¥äº†æ€ä¹ˆåŠï¼Ÿ"
            - "å¹²æ—±å¤©æ°”åº”å¯¹æªæ–½"
            - "å¯’æ½®æ¥è¢­å¦‚ä½•é˜²æŠ¤ï¼Ÿ"

          4. ç»¼åˆæŸ¥è¯¢:
            - "æ¸©åº¦30â„ƒæ¹¿åº¦80%é£é€Ÿ10m/s"
            - "æœªæ¥ä¸‰å¤©é«˜æ¸©æš´é›¨é¢„è­¦"

        å‘½ä»¤:
          - help: æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
          - status: æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
          - quit: é€€å‡ºç³»ç»Ÿ
        """
        print(help_text)

    def _show_system_status(self):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        status = {
            'agents_count': len(self.agents),
            'agents': list(self.agents.keys()),
            'coordinator': self.coordinator.name if hasattr(self, 'coordinator') else 'æœªåˆå§‹åŒ–',
            'workflows_count': len(self.coordinator.get_workflow_status()) if hasattr(self, 'coordinator') else 0,
            'output_dir': str(self.output_dir)
        }

        print("\nç³»ç»ŸçŠ¶æ€:")
        for key, value in status.items():
            print(f"  {key}: {value}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="å››æ™ºèƒ½ä½“æ°”è±¡åˆ†æç³»ç»Ÿ")
    parser.add_argument('--query', type=str, help='ç›´æ¥å¤„ç†å•ä¸ªæŸ¥è¯¢')
    parser.add_argument('--batch', type=str, help='æ‰¹é‡å¤„ç†æŸ¥è¯¢æ–‡ä»¶')
    parser.add_argument('--output', type=str, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’æ¨¡å¼')

    args = parser.parse_args()

    # åˆ›å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
    system = MultiAgentSystem(output_dir=args.output)

    if args.query:
        # å¤„ç†å•ä¸ªæŸ¥è¯¢
        print(f"å¤„ç†æŸ¥è¯¢: {args.query}")
        result = system.process_query(args.query)

        if result.get('success'):
            print("\n" + "=" * 60)
            print(result.get('response', 'æ— å“åº”'))
            print("=" * 60)
        else:
            print(f"å¤„ç†å¤±è´¥: {result.get('error')}")

    elif args.batch:
        # æ‰¹é‡å¤„ç†
        with open(args.batch, 'r', encoding='utf-8') as f:
            queries = [line.strip() for line in f if line.strip()]

        system.batch_process(queries)

    elif args.interactive or (not args.query and not args.batch):
        # äº¤äº’æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        system.interactive_mode()


if __name__ == "__main__":
    main()