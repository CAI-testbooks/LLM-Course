import json
import torch
import os
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

# ===================== å…¨å±€é…ç½®å‚æ•° =====================
# è¾“å…¥æµ‹è¯•é›†è·¯å¾„ï¼ˆæ‰€æœ‰æ¨¡å‹å…±ç”¨ï¼‰
INPUT_JSON_PATH = "/root/autodl-tmp/devdata1k.json"
# ç”Ÿæˆé…ç½®ï¼ˆä½temperatureï¼Œå¯æ§éšæœºæ€§ï¼‰
GENERATION_CONFIG = GenerationConfig(
    temperature=0.3,          # ä½éšæœºæ€§ï¼ˆ0.3-0.5ä¸ºæœ€ä¼˜åŒºé—´ï¼‰ï¼Œæ—¢ä¿è¯å°å¹…å·®å¼‚åˆä¸åç¦»æ ¸å¿ƒç»“æœ
    top_p=0.9,                # æ ¸é‡‡æ ·ï¼Œä¿ç•™90%æ¦‚ç‡çš„tokenï¼ˆé…åˆä½tempå¢å¼ºç¨³å®šæ€§ï¼‰
    max_new_tokens=512,       # æœ€å¤§ç”Ÿæˆtokenæ•°
    do_sample=True,           # å¼€å¯é‡‡æ ·ï¼ˆå…³é”®ï¼šå¼•å…¥å¯æ§éšæœºæ€§ï¼‰
    eos_token_id=None,        # è‡ªåŠ¨è¯†åˆ«ç»“æŸç¬¦
    pad_token_id=None,        # è‡ªåŠ¨è¯†åˆ«paddingç¬¦
    repetition_penalty=1.05,  # é‡å¤æƒ©ç½š
)
# ä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ========== å•æ¬¡éªŒè¯æ¨¡å¼å¼€å…³ ==========
RUN_ONLY_FIRST_EXPERIMENT = False  # å…ˆéªŒè¯1æ¬¡ï¼Œæ— è¯¯åæ”¹ä¸ºFalseè·‘å…¨é‡

# ===================== å¾…è¿è¡Œçš„æ¨¡å‹åˆ—è¡¨ =====================
MODELS = [
    ("/root/autodl-tmp/llama-200", "llama-200"),
    ("/root/autodl-tmp/llama-400", "llama-400"),
    ("/root/autodl-tmp/llama-600", "llama-600"),
    ("/root/autodl-tmp/llama-800", "llama-800"),
]
# å®éªŒæ¬¡æ•°
EXPERIMENT_TIMES = 3

# ===================== åŠ è½½æ¨¡å‹å’ŒTokenizer =====================
def load_model_and_tokenizer(model_path):
    """åŠ è½½æŒ‡å®šè·¯å¾„çš„æ¨¡å‹å’Œtokenizer"""
    print(f"\n=== æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{model_path} ===")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="right"
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto"
        ).eval()
        
        # è®¾ç½®pad_tokenï¼ˆLlamaé»˜è®¤æ— pad_tokenï¼‰
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        if model.config.pad_token_id is None:
            model.config.pad_token_id = model.config.eos_token_id
        
        print(f"æ¨¡å‹ {model_path} åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡ï¼š{DEVICE}")
        return tokenizer, model
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹ {model_path} å¤±è´¥ï¼š{str(e)}")
        return None, None

# ===================== ç”Ÿæˆå‡½æ•°ï¼ˆLlamaæ ¼å¼ï¼‰ =====================
def generate_response(tokenizer, model, instruction, input_text):
    """é€‚é…Llama 3å®˜æ–¹å¯¹è¯æ ¼å¼çš„ç”Ÿæˆå‡½æ•°"""
    prompt = f"""<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>

ä½ éœ€è¦ä¸¥æ ¼æŒ‰ç…§æŒ‡ä»¤è¦æ±‚å›ç­”é—®é¢˜ã€‚
<|eot_id|>
<|start_header_id|>user<|end_header_id|>

{instruction}

{input_text}
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

"""
    
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=2048
    ).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            generation_config=GENERATION_CONFIG
        )
    
    generated_text = tokenizer.decode(
        outputs[0][len(inputs.input_ids[0]):],
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True
    ).strip()
    
    return generated_text

# ===================== å•è½®å®éªŒå¤„ç†å‡½æ•°ï¼ˆæ–°å¢éšæœºç§å­ï¼‰ =====================
def run_single_experiment(model_path, model_name, exp_num):
    """è¿è¡Œå•ä¸ªæ¨¡å‹çš„å•æ¬¡å®éªŒï¼Œä¸ºæ¯æ¬¡å®éªŒè®¾ç½®å”¯ä¸€éšæœºç§å­"""
    # ========== æ ¸å¿ƒï¼šä¸ºæ¯æ¬¡å®éªŒè®¾ç½®ä¸åŒçš„éšæœºç§å­ ==========
    # ç§å­å€¼ = æ¨¡å‹åç§°å“ˆå¸Œ + å®éªŒåºå·ï¼ˆç¡®ä¿ä¸åŒæ¨¡å‹/å®éªŒçš„ç§å­å”¯ä¸€ï¼‰
    seed = hash(model_name) % 10000 + exp_num
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    print(f"æœ¬æ¬¡å®éªŒéšæœºç§å­ï¼š{seed}")
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = f"/root/autodl-tmp/test_{model_name}_exp{exp_num}.json"
    
    # é˜²è¦†ç›–
    if os.path.exists(output_file):
        print(f"âš ï¸  è¾“å‡ºæ–‡ä»¶ {output_file} å·²å­˜åœ¨ï¼Œè·³è¿‡æœ¬æ¬¡å®éªŒ")
        return
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_model_and_tokenizer(model_path)
    if tokenizer is None or model is None:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡ {model_name} ç¬¬{exp_num}æ¬¡å®éªŒ")
        return
    
    # è¯»å–æ•°æ®é›†
    try:
        print(f"\næ­£åœ¨è¯»å–è¾“å…¥æ–‡ä»¶ï¼š{INPUT_JSON_PATH}")
        with open(INPUT_JSON_PATH, "r", encoding="utf-8") as f:
            dataset = json.load(f)
        if not isinstance(dataset, list):
            raise ValueError("è¾“å…¥JSONå¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼")
    except Exception as e:
        print(f"âŒ è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return
    
    # å¤„ç†æ•°æ®
    generated_dataset = []
    for idx, sample in enumerate(tqdm(dataset, desc=f"{model_name} ç¬¬{exp_num}æ¬¡å®éªŒ")):
        try:
            instruction = sample.get("instruction", "")
            input_text = sample.get("input", "")
            ground_truth = sample.get("output", "")
            
            # ç”Ÿæˆå›å¤ï¼ˆå¸¦éšæœºæ€§ï¼‰
            generated_output = generate_response(tokenizer, model, instruction, input_text)
            
            new_sample = {
                "instruction": instruction,
                "input": input_text,
                "ground_truth": ground_truth,
                "generated_output": generated_output,
                "experiment_num": exp_num,
                "model_name": model_name,
                "seed": seed  # è®°å½•ç§å­ï¼Œä¾¿äºå¤ç°
            }
            generated_dataset.append(new_sample)
        except Exception as e:
            print(f"\nâš ï¸  å¤„ç†ç¬¬{idx}æ¡æ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}")
            continue
    
    # ä¿å­˜ç»“æœ
    try:
        print(f"\næ­£åœ¨ä¿å­˜ç”Ÿæˆç»“æœåˆ°ï¼š{output_file}")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(generated_dataset, f, ensure_ascii=False, indent=2)
        print(f"âœ… {model_name} ç¬¬{exp_num}æ¬¡å®éªŒå®Œæˆï¼Œç”Ÿæˆ {len(generated_dataset)} æ¡æ•°æ®")
        print(f"ğŸ“‚ ç»“æœæ–‡ä»¶è·¯å¾„ï¼š{output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥ï¼š{str(e)}")
    
    # æ¸…ç†æ˜¾å­˜
    del model, tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ===================== æ‰¹é‡è¿è¡Œä¸»å‡½æ•° =====================
def run_batch_experiments():
    print("=== å¼€å§‹è¿è¡Œå®éªŒ ===")
    
    # å•æ¬¡éªŒè¯æ¨¡å¼
    if RUN_ONLY_FIRST_EXPERIMENT:
        print(f"ğŸ” å•æ¬¡éªŒè¯æ¨¡å¼å¼€å¯ï¼Œä»…è¿è¡Œç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç¬¬ä¸€æ¬¡å®éªŒï¼ˆ1/15ï¼‰")
        first_model_path, first_model_name = MODELS[0]
        first_exp_num = 1
        print(f"\n==================================================")
        print(f"å¼€å§‹è¿è¡Œï¼š{first_model_name} - ç¬¬{first_exp_num}æ¬¡å®éªŒ")
        print(f"==================================================")
        run_single_experiment(first_model_path, first_model_name, first_exp_num)
        print("\nğŸ‰ å•æ¬¡éªŒè¯å®éªŒå®Œæˆï¼")
        print(f"ğŸ“Œ ç»“æœæ–‡ä»¶ï¼š/root/autodl-tmp/test_{first_model_name}_exp{first_exp_num}.json")
        print(f"ğŸ’¡ éªŒè¯åå°† RUN_ONLY_FIRST_EXPERIMENT æ”¹ä¸º False è¿è¡Œå…¨é‡")
        return
    
    # å…¨é‡è¿è¡Œæ¨¡å¼
    print(f"ğŸ“¦ å…¨é‡è¿è¡Œæ¨¡å¼å¼€å¯")
    print(f"æ¨¡å‹æ•°é‡ï¼š{len(MODELS)} | å®éªŒæ¬¡æ•°ï¼š{EXPERIMENT_TIMES} | æ€»è®¡ï¼š{len(MODELS)*EXPERIMENT_TIMES}")
    
    for model_path, model_name in MODELS:
        for exp_num in range(1, EXPERIMENT_TIMES + 1):
            print(f"\n==================================================")
            print(f"å¼€å§‹è¿è¡Œï¼š{model_name} - ç¬¬{exp_num}æ¬¡å®éªŒ")
            print(f"==================================================")
            run_single_experiment(model_path, model_name, exp_num)
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒè¿è¡Œå®Œæˆï¼")

if __name__ == "__main__":
    run_batch_experiments()