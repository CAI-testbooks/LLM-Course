import json
import openai
import time
import re
import os
from datetime import datetime

def query_gpt4o_mini(question):
    """
    ä½¿ç”¨GPT-4o-miniæ¨¡å‹è¿›è¡ŒæŸ¥è¯¢
    """
    openai.api_key = "sk-tV4cZ8IDjmMTz3DgjKQKQHa1WP35TM2HhD0Dpdw0pC2m1Ko7"
    openai.base_url = 'https://4.0.wokaai.com/v1/'

    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",  # ä½¿ç”¨GPT-4o-miniæ¨¡å‹
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬è¯„ä¼°ä¸“å®¶ï¼Œæ“…é•¿è¯„ä¼°æ”¯æŒäº‹å®çš„è´¨é‡ã€‚è¯·åªè¿”å›åˆ†æ•°ï¼Œä¸éœ€è¦ä»»ä½•è§£é‡Šã€‚"},
                {"role": "user", "content": question}
            ],
            temperature=0.1  # é™ä½éšæœºæ€§ï¼Œç¡®ä¿è¯„åˆ†ä¸€è‡´æ€§
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"APIè°ƒç”¨é”™è¯¯: {e}")
        return None

def parse_input_content(input_str):
    """
    ä»inputå­—æ®µä¸­æ‹†åˆ†Contextå’ŒQuestion
    è¾“å…¥æ ¼å¼ç¤ºä¾‹ï¼šContext: xxx Question: xxx
    """
    # æ‹†åˆ†Contextå’ŒQuestionï¼ˆå…¼å®¹æ¢è¡Œ/ç©ºæ ¼ç­‰æ ¼å¼ï¼‰
    context_match = re.search(r'Context:\s*(.*?)\s*Question:', input_str, re.DOTALL)
    question_match = re.search(r'Question:\s*(.*)', input_str, re.DOTALL)
    
    context = context_match.group(1).strip() if context_match else ""
    question = question_match.group(1).strip() if question_match else ""
    
    return context, question

def evaluate_supporting_facts(context, query, ground_truth_answer, generated_supporting_facts):
    """
    è¯„ä¼°generated_supporting_factsçš„è´¨é‡ï¼ˆ0-100åˆ†ï¼‰
    """
    # å°†supporting_factsè½¬æ¢ä¸ºå¯è¯»çš„å­—ç¬¦ä¸²
    sf_text = ""
    for i, fact in enumerate(generated_supporting_facts):
        if isinstance(fact, list) and len(fact) > 0:
            sf_text += f"{i+1}. {fact[0]}\n"
    
    # æ„å»ºè¯„ä¼°é—®é¢˜ - è°ƒæ•´ä¸º0-100åˆ†ï¼Œä¸”å†—ä½™åº¦åˆ†æ•°è¶Šé«˜=è¶Šä¸å†—ä½™
    question = f"""
è¯·å¯¹ä»¥ä¸‹ç”Ÿæˆçš„æ”¯æŒäº‹å®(supporting_facts)è¿›è¡Œè¯„åˆ†ï¼ˆè¯„åˆ†èŒƒå›´0-100åˆ†ï¼‰ï¼š

ä¸Šä¸‹æ–‡(context): {context}
é—®é¢˜(query): {query}
æ ‡å‡†ç­”æ¡ˆ(ground_truth_answer): {ground_truth_answer}
ç”Ÿæˆçš„æ”¯æŒäº‹å®(generated_supporting_facts):
{sf_text}

è¯·ä»ä¸¤ä¸ªæ–¹é¢è¿›è¡Œè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ï¼š
1. æ”¯æŒåº¦è¯„åˆ†ï¼šåˆ†æ•°è¶Šé«˜ï¼Œè¡¨ç¤ºè¿™äº›æ”¯æŒäº‹å®è¶Šèƒ½å……åˆ†æ”¯æŒé—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆï¼›åˆ†æ•°è¶Šä½ï¼Œè¡¨ç¤ºæ”¯æŒæ€§è¶Šå·®ã€‚
   è¯„åˆ†ç»´åº¦åŒ…æ‹¬ï¼šé—®é¢˜ä¸­çš„æ ¸å¿ƒä¸»ä½“æ˜¯å¦åœ¨æ”¯æŒäº‹å®ä¸­ä½“ç°ã€æ”¯æŒäº‹å®æ˜¯å¦èƒ½æœ‰æ•ˆä½è¯ç­”æ¡ˆã€ä¿¡æ¯æ˜¯å¦å®Œæ•´ã€‚
2. å†—ä½™åº¦è¯„åˆ†ï¼šåˆ†æ•°è¶Šé«˜ï¼Œè¡¨ç¤ºè¿™äº›æ”¯æŒäº‹å®è¶Šç®€æ´ã€æ— å†—ä½™ï¼›åˆ†æ•°è¶Šä½ï¼Œè¡¨ç¤ºæ”¯æŒäº‹å®ä¸­åŒ…å«è¶Šå¤šä¸é—®é¢˜æ— å…³çš„å†—ä½™å†…å®¹ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›è¯„åˆ†ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
æ”¯æŒåº¦è¯„åˆ†: X
å†—ä½™åº¦è¯„åˆ†: Y
"""
    
    response = query_gpt4o_mini(question)
    return response

def parse_scores(response):
    """
    ä»GPTå“åº”ä¸­è§£æ0-100åˆ†çš„è¯„åˆ†
    """
    if not response:
        return None, None
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è¯„åˆ†ï¼ˆé€‚é…ä¸­æ–‡å†’å·/è‹±æ–‡å†’å·ã€ç©ºæ ¼ï¼‰
    support_match = re.search(r'æ”¯æŒåº¦è¯„åˆ†:\s*(\d{1,3})', response)
    redundancy_match = re.search(r'å†—ä½™åº¦è¯„åˆ†:\s*(\d{1,3})', response)
    
    # å…¼å®¹å…¶ä»–æ ¼å¼
    if not support_match:
        support_match = re.search(r'æ”¯æŒåº¦[ï¼š:]\s*(\d{1,3})', response)
    if not redundancy_match:
        redundancy_match = re.search(r'å†—ä½™åº¦[ï¼š:]\s*(\d{1,3})', response)
    
    # æœ€åå°è¯•æå–å‰ä¸¤ä¸ªæ•°å­—
    if not support_match or not redundancy_match:
        numbers = re.findall(r'\d+', response)
        if len(numbers) >= 2:
            # ç¡®ä¿åˆ†æ•°åœ¨0-100èŒƒå›´å†…
            support_score = min(max(int(numbers[0]), 0), 100)
            redundancy_score = min(max(int(numbers[1]), 0), 100)
            return support_score, redundancy_score
    
    # æå–å¹¶æ ¡éªŒåˆ†æ•°èŒƒå›´
    support_score = int(support_match.group(1)) if support_match else None
    redundancy_score = int(redundancy_match.group(1)) if redundancy_match else None
    
    # ç¡®ä¿åˆ†æ•°åœ¨0-100ä¹‹é—´
    if support_score is not None:
        support_score = min(max(support_score, 0), 100)
    if redundancy_score is not None:
        redundancy_score = min(max(redundancy_score, 0), 100)
    
    return support_score, redundancy_score

def process_single_dataset(input_file, output_file, sample_size=None):
    """
    å¤„ç†å•ä¸ªæ•°æ®é›†ï¼Œè¿”å›å¤„ç†ç»“æœï¼ˆæˆåŠŸæ•°/å¤±è´¥æ•°ï¼‰
    """
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return 0, 0, 0  # æ€»æ¡æ•°ã€æˆåŠŸæ•°ã€å¤±è´¥æ•°
    
    # è¯»å–æ•°æ®é›†
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_count = len(data)
    print(f"\nğŸ“‚ å¼€å§‹å¤„ç†æ•°æ®é›†: {input_file}")
    print(f"ğŸ“Š æ€»æ¡ç›®æ•°: {total_count}")
    
    # å¦‚æœæŒ‡å®šäº†æ ·æœ¬å¤§å°ï¼Œåªå¤„ç†éƒ¨åˆ†æ•°æ®
    if sample_size and sample_size < total_count:
        data = data[:sample_size]
        print(f"ğŸ” åªå¤„ç†å‰ {sample_size} æ¡è®°å½•")
        total_count = sample_size
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åŠ è½½å·²å¤„ç†çš„æ•°æ®
    processed_data = []
    start_index = 0
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            processed_data = json.load(f)
            start_index = len(processed_data)
            print(f"ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰å¤„ç†æ•°æ®ï¼Œä»ç¬¬ {start_index+1} æ¡å¼€å§‹ç»§ç»­å¤„ç†")
    
    # å¤„ç†æ¯ä¸ªæ¡ç›®
    success_count = 0
    error_count = 0
    
    for i in range(start_index, len(data)):
        item = data[i]
        print(f"â³ å¤„ç†è¿›åº¦: {i+1}/{len(data)}", end=' ')
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å­—æ®µ
        required_fields = ['instruction', 'input', 'ground_truth', 'generated_output']
        if not all(key in item for key in required_fields):
            missing_fields = [k for k in required_fields if k not in item]
            print(f"- âŒ è·³è¿‡: ç¼ºå°‘å­—æ®µ {missing_fields}")
            error_count += 1
            processed_data.append(item)
            continue
        
        try:
            # 1. è§£æinputä¸­çš„Contextå’ŒQuestion
            context, query = parse_input_content(item['input'])
            if not context or not query:
                print(f"- âŒ è·³è¿‡: æ— æ³•è§£æContext/Question")
                error_count += 1
                processed_data.append(item)
                continue
            
            # 2. è§£æground_truthï¼ˆJSONå­—ç¬¦ä¸²ï¼‰æå–æ ‡å‡†ç­”æ¡ˆ
            ground_truth = json.loads(item['ground_truth'])
            ground_truth_answer = ground_truth.get('answer', '')
            
            # 3. è§£ægenerated_outputï¼ˆJSONå­—ç¬¦ä¸²ï¼‰æå–éœ€è¦è¯„åˆ†çš„supporting_facts
            generated_output = json.loads(item['generated_output'])
            generated_sf = generated_output.get('supporting_facts', [])
            
            # 4. è¯„ä¼°supporting_facts
            evaluation_response = evaluate_supporting_facts(context, query, ground_truth_answer, generated_sf)
            
            if evaluation_response:
                support_score, redundancy_score = parse_scores(evaluation_response)
                
                # æ·»åŠ è¯„åˆ†å­—æ®µåˆ°åŸæ•°æ®
                item['support_score'] = support_score
                item['redundancy_score'] = redundancy_score
                
                print(f"- âœ… æ”¯æŒåº¦: {support_score}/100, å†—ä½™åº¦: {redundancy_score}/100")
                success_count += 1
            else:
                print(f"- âŒ è¯„ä¼°å¤±è´¥: APIè¿”å›ç©º")
                item['support_score'] = None
                item['redundancy_score'] = None
                error_count += 1
        
        except json.JSONDecodeError as e:
            print(f"- âŒ è¯„ä¼°å¤±è´¥: JSONè§£æé”™è¯¯: {str(e)[:50]}...")
            item['support_score'] = None
            item['redundancy_score'] = None
            error_count += 1
        except Exception as e:
            print(f"- âŒ è¯„ä¼°å¤±è´¥: æœªçŸ¥é”™è¯¯: {str(e)[:50]}...")
            item['support_score'] = None
            item['redundancy_score'] = None
            error_count += 1
        
        # æ·»åŠ åˆ°å¤„ç†åçš„æ•°æ®å¹¶ä¿å­˜ï¼ˆå¢é‡å†™å…¥ï¼‰
        processed_data.append(item)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, indent=2, ensure_ascii=False)
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
        time.sleep(1)
    
    print(f"âœ… æ•°æ®é›† {input_file} å¤„ç†å®Œæˆ!")
    print(f"   æˆåŠŸè¯„ä¼°: {success_count} æ¡ | è¯„ä¼°å¤±è´¥: {error_count} æ¡")
    return total_count, success_count, error_count

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªæ•°æ®é›†
    ä½ åªéœ€ä¿®æ”¹ä¸‹é¢çš„ DATASET_LIST åˆ—è¡¨å³å¯ï¼
    """
    # ====================== æ•°æ®é›†é…ç½®åˆ—è¡¨ï¼ˆæ ¸å¿ƒä¿®æ”¹åŒºï¼‰======================
    # æ ¼å¼è¯´æ˜ï¼š
    # {
    #   "input_file": è¾“å…¥æ•°æ®é›†è·¯å¾„,
    #   "output_file": è¾“å‡ºå¸¦è¯„åˆ†çš„æ•°æ®é›†è·¯å¾„,
    #   "sample_size": å¯é€‰ï¼Œæµ‹è¯•æ—¶ç”¨ï¼Œæ¯”å¦‚åªå¤„ç†å‰10æ¡ï¼Œæ­£å¼è¿è¡Œè®¾ä¸ºNone
    # }
    DATASET_LIST = [
        # ç¤ºä¾‹1ï¼šç¬¬ä¸€ä¸ªæ•°æ®é›†
        #{
            #"input_file": "/root/autodl-tmp/test_Qwen2.5-7B-Instruct_exp3_evaluation.json",
            #"output_file": "/root/autodl-tmp/test_qwen_with_score.json",
            #"sample_size": None
        #},
        {
            "input_file": "/root/autodl-tmp/test_qwen-0.5K_exp3.json",
            "output_file": "/root/autodl-tmp/test_qwen_0.5K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_qwen-1K_exp3.json",
            "output_file": "/root/autodl-tmp/test_qwen_1K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_qwen-10K_exp3.json",
            "output_file": "/root/autodl-tmp/test_qwen_10K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_qwen-10K0.5K_exp3.json",
            "output_file": "/root/autodl-tmp/test_qwen_10K0.5K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_qwen-10K1K_exp3.json",
            "output_file": "/root/autodl-tmp/test_qwen_10K1K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_glm_exp2.json",
            "output_file": "/root/autodl-tmp/test_glm_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_glm-0.5K_exp2.json",
            "output_file": "/root/autodl-tmp/test_glm-0.5K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_glm-1K_exp2.json",
            "output_file": "/root/autodl-tmp/test_glm-1K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_glm-10K_exp2.json",
            "output_file": "/root/autodl-tmp/test_glm-10K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_glm-10K0.5K_exp2.json",
            "output_file": "/root/autodl-tmp/test_glm-10K0.5K_with_score.json",
            "sample_size": None
        },
        {
            "input_file": "/root/autodl-tmp/test_glm-10K1K_exp2.json",
            "output_file": "/root/autodl-tmp/test_glm-10K1K_with_score.json",
            "sample_size": None
        },
        
        
    ]
    # ====================== é…ç½®ç»“æŸ =======================

    # æ‰¹é‡å¤„ç†æ±‡æ€»ç»Ÿè®¡
    summary = {
        "total_datasets": len(DATASET_LIST),
        "processed_datasets": 0,
        "total_records": 0,
        "success_records": 0,
        "failed_records": 0,
        "failed_datasets": []
    }

    print("="*80)
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ•°æ®é›† | å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‹ å¾…å¤„ç†æ•°æ®é›†æ€»æ•°: {summary['total_datasets']}")
    print("="*80)

    # éå†å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for idx, dataset in enumerate(DATASET_LIST, 1):
        print(f"\n{'='*20} å¤„ç†ç¬¬ {idx}/{summary['total_datasets']} ä¸ªæ•°æ®é›† {'='*20}")
        input_file = dataset["input_file"]
        output_file = dataset["output_file"]
        sample_size = dataset.get("sample_size", None)

        # å¤„ç†å•ä¸ªæ•°æ®é›†
        total, success, failed = process_single_dataset(input_file, output_file, sample_size)
        
        # æ›´æ–°æ±‡æ€»ç»Ÿè®¡
        summary["total_records"] += total
        summary["success_records"] += success
        summary["failed_records"] += failed
        summary["processed_datasets"] += 1

        if failed == total and total > 0:  # è¯¥æ•°æ®é›†å…¨éƒ¨å¤±è´¥
            summary["failed_datasets"].append(input_file)

    # è¾“å‡ºæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š æ‰¹é‡å¤„ç†æ±‡æ€»æŠ¥å‘Š")
    print("="*80)
    print(f"æ€»æ•°æ®é›†æ•°: {summary['total_datasets']}")
    print(f"å·²å¤„ç†æ•°æ®é›†æ•°: {summary['processed_datasets']}")
    print(f"æ€»è®°å½•æ•°: {summary['total_records']}")
    print(f"æˆåŠŸè¯„åˆ†è®°å½•æ•°: {summary['success_records']}")
    print(f"å¤±è´¥è¯„åˆ†è®°å½•æ•°: {summary['failed_records']}")
    if summary["total_records"] > 0:
        success_rate = (summary["success_records"] / summary["total_records"]) * 100
        print(f"æ•´ä½“æˆåŠŸç‡: {success_rate:.2f}%")
    if summary["failed_datasets"]:
        print(f"å®Œå…¨å¤„ç†å¤±è´¥çš„æ•°æ®é›†: {summary['failed_datasets']}")
    print(f"ğŸ•’ ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

if __name__ == "__main__":
    main()