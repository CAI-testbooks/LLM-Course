"""
RAG è¯„æµ‹è„šæœ¬
æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼Œä½¿ç”¨ LLM åˆ¤æ–­ç­”æ¡ˆæ­£ç¡®æ€§
è¯„æµ‹æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€å¼•ç”¨F1ã€å¹»è§‰ç‡ã€Hit@Kã€MRR
"""

import os
import sys
import json
import re
import csv
import time
import random
from pathlib import Path
from typing import List, Dict, Tuple, Set, Optional
from dataclasses import dataclass, field, asdict
from collections import defaultdict
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from rag import RAG


# ==================== æ•°æ®ç»“æ„ ====================

@dataclass
class Question:
    """é—®é¢˜"""
    question_id: str
    content: str


@dataclass
class Answer:
    """ç­”æ¡ˆ"""
    ans_id: str
    question_id: str
    content: str


@dataclass
class EvalSample:
    """è¯„æµ‹æ ·æœ¬"""
    question_id: str
    question_content: str
    correct_ans_ids: List[str]  # æ­£ç¡®ç­”æ¡ˆIDåˆ—è¡¨
    correct_answers: List[str]   # æ­£ç¡®ç­”æ¡ˆå†…å®¹åˆ—è¡¨


@dataclass
class EvalResult:
    """å•æ¡è¯„æµ‹ç»“æœ"""
    question_id: str
    question: str
    ground_truths: List[str]      # æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨
    prediction: str               # æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
    retrieved_docs: List[dict]    # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    retrieved_ans_ids: List[str]  # æ£€ç´¢åˆ°çš„ç­”æ¡ˆID
    
    # æ£€ç´¢æŒ‡æ ‡
    hit: bool = False             # æ˜¯å¦å‘½ä¸­æ­£ç¡®ç­”æ¡ˆ
    hit_rank: int = -1            # å‘½ä¸­æ’åï¼ˆ-1è¡¨ç¤ºæœªå‘½ä¸­ï¼‰
    
    # ç”ŸæˆæŒ‡æ ‡ï¼ˆç”±LLMè¯„åˆ¤ï¼‰
    is_correct: bool = False      # ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
    citation_precision: float = 0.0  # å¼•ç”¨ç²¾ç¡®ç‡
    citation_recall: float = 0.0     # å¼•ç”¨å¬å›ç‡
    hallucination_rate: float = 0.0  # å¹»è§‰ç‡
    
    # LLM è¯„åˆ¤åŸå§‹ç»“æœ
    llm_judgment: dict = field(default_factory=dict)


@dataclass
class EvalMetrics:
    """è¯„æµ‹æŒ‡æ ‡æ±‡æ€»"""
    total_samples: int = 0
    
    # æ£€ç´¢æŒ‡æ ‡
    hit_at_1: float = 0.0
    hit_at_3: float = 0.0
    hit_at_5: float = 0.0
    hit_at_10: float = 0.0
    mrr: float = 0.0
    
    # ç”ŸæˆæŒ‡æ ‡
    accuracy: float = 0.0
    citation_precision: float = 0.0
    citation_recall: float = 0.0
    citation_f1: float = 0.0
    hallucination_rate: float = 0.0
    
    # è€—æ—¶
    elapsed_seconds: float = 0.0


# ==================== æ•°æ®åŠ è½½ ====================

class DataLoader:
    """æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.questions: Dict[str, Question] = {}
        self.answers: Dict[str, Answer] = {}
    
    def _detect_delimiter(self, file_path: Path) -> str:
        """æ£€æµ‹åˆ†éš”ç¬¦"""
        with open(file_path, 'r', encoding='utf-8') as f:
            first_line = f.readline()
            if '\t' in first_line:
                return '\t'
            return ','
    
    def load_questions(self, filename: str = "question.csv") -> Dict[str, Question]:
        """åŠ è½½é—®é¢˜"""
        file_path = self.data_dir / filename
        delimiter = self._detect_delimiter(file_path)
        
        print(f"Loading questions from {file_path} (delimiter: {'tab' if delimiter == '\t' else 'comma'})")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter=delimiter)
            for row in reader:
                qid = str(row['question_id']).strip()
                content = row['content'].strip()
                self.questions[qid] = Question(question_id=qid, content=content)
        
        print(f"  Loaded {len(self.questions)} questions")
        return self.questions
    
    def load_answers(self, filename: str = "answer.csv") -> Dict[str, Answer]:
        """åŠ è½½ç­”æ¡ˆ"""
        file_path = self.data_dir / filename
        delimiter = self._detect_delimiter(file_path)
        
        print(f"Loading answers from {file_path} (delimiter: {'tab' if delimiter == '\t' else 'comma'})")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter=delimiter)
            for row in reader:
                ans_id = str(row['ans_id']).strip()
                qid = str(row['question_id']).strip()
                content = row['content'].strip()
                self.answers[ans_id] = Answer(ans_id=ans_id, question_id=qid, content=content)
        
        print(f"  Loaded {len(self.answers)} answers")
        return self.answers
    
    def load_train_candidates(self, filename: str = "train_candidates.txt") -> List[Tuple[str, str]]:
        """
        åŠ è½½è®­ç»ƒé›†å€™é€‰ï¼ˆç”¨äºæ„å»ºçŸ¥è¯†åº“ï¼‰
        è¿”å›ï¼š[(question_id, pos_ans_id), ...]
        """
        file_path = self.data_dir / filename
        
        print(f"Loading train candidates from {file_path}")
        
        pairs = set()  # å»é‡
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                qid = str(row['question_id']).strip()
                pos_ans_id = str(row['pos_ans_id']).strip()
                pairs.add((qid, pos_ans_id))
        
        result = list(pairs)
        print(f"  Loaded {len(result)} unique (question, answer) pairs")
        return result
    
    def load_eval_candidates(self, filename: str) -> List[EvalSample]:
        """
        åŠ è½½è¯„æµ‹é›†å€™é€‰ï¼ˆdev æˆ– testï¼‰
        è¿”å›ï¼šEvalSample åˆ—è¡¨
        """
        file_path = self.data_dir / filename
        
        print(f"Loading eval candidates from {file_path}")
        
        # æŒ‰é—®é¢˜IDåˆ†ç»„ï¼Œæ”¶é›†æ­£ç¡®ç­”æ¡ˆ
        question_answers: Dict[str, List[str]] = defaultdict(list)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                qid = str(row['question_id']).strip()
                ans_id = str(row['ans_id']).strip()
                label = int(row['label'])
                
                if label == 1:
                    question_answers[qid].append(ans_id)
        
        # æ„å»ºè¯„æµ‹æ ·æœ¬
        samples = []
        for qid, ans_ids in question_answers.items():
            if qid not in self.questions:
                print(f"  Warning: question {qid} not found, skipping")
                continue
            
            question = self.questions[qid]
            correct_answers = []
            for ans_id in ans_ids:
                if ans_id in self.answers:
                    correct_answers.append(self.answers[ans_id].content)
                else:
                    print(f"  Warning: answer {ans_id} not found")
            
            if correct_answers:
                samples.append(EvalSample(
                    question_id=qid,
                    question_content=question.content,
                    correct_ans_ids=ans_ids,
                    correct_answers=correct_answers
                ))
        
        print(f"  Loaded {len(samples)} eval samples")
        return samples


# ==================== LLM è¯„åˆ¤å™¨ ====================

class LLMJudge:
    """ä½¿ç”¨ LLM è¯„åˆ¤ç­”æ¡ˆè´¨é‡"""
    
    JUDGE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦é—®ç­”è¯„æµ‹ä¸“å®¶ã€‚è¯·è¯„ä¼°AIåŠ©æ‰‹çš„å›ç­”è´¨é‡ã€‚

## è¯„æµ‹ä»»åŠ¡

ç»™å®šï¼š
1. ç”¨æˆ·é—®é¢˜
2. æ ‡å‡†ç­”æ¡ˆï¼ˆå¯èƒ½æœ‰å¤šä¸ªï¼‰
3. AIåŠ©æ‰‹çš„å›ç­”
4. AIå¼•ç”¨çš„å‚è€ƒæ¥æº

è¯·è¯„ä¼°ä»¥ä¸‹æŒ‡æ ‡ï¼š

### 1. æ­£ç¡®æ€§ (is_correct)
AIçš„å›ç­”æ˜¯å¦æ­£ç¡®å›ç­”äº†ç”¨æˆ·é—®é¢˜ï¼Ÿä¸æ ‡å‡†ç­”æ¡ˆçš„æ ¸å¿ƒä¿¡æ¯æ˜¯å¦ä¸€è‡´ï¼Ÿ
- true: å›ç­”æ­£ç¡®ï¼Œæ ¸å¿ƒä¿¡æ¯ä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´
- false: å›ç­”é”™è¯¯æˆ–åç¦»ä¸»é¢˜

### 2. å¼•ç”¨ç›¸å…³æ€§ (citation_relevance)
AIå¼•ç”¨çš„æ¥æºä¸­ï¼Œæœ‰å¤šå°‘æ˜¯çœŸæ­£æ”¯æŒå…¶å›ç­”çš„ï¼Ÿ
- è¿”å›ä¸€ä¸ª 0-1 ä¹‹é—´çš„åˆ†æ•°
- 1.0 = æ‰€æœ‰å¼•ç”¨éƒ½ç›¸å…³
- 0.0 = æ²¡æœ‰å¼•ç”¨æˆ–æ‰€æœ‰å¼•ç”¨éƒ½ä¸ç›¸å…³

### 3. å¼•ç”¨è¦†ç›–åº¦ (citation_coverage)  
AIçš„å›ç­”ä¸­ï¼Œæœ‰å¤šå°‘å…³é”®ä¿¡æ¯æœ‰å¼•ç”¨æ”¯æŒï¼Ÿ
- è¿”å›ä¸€ä¸ª 0-1 ä¹‹é—´çš„åˆ†æ•°
- 1.0 = æ‰€æœ‰å…³é”®ä¿¡æ¯éƒ½æœ‰å¼•ç”¨
- 0.0 = æ²¡æœ‰ä»»ä½•å¼•ç”¨æ”¯æŒ

### 4. å¹»è§‰ç‡ (hallucination_rate)
AIçš„å›ç­”ä¸­ï¼Œæœ‰å¤šå°‘ä¿¡æ¯æ˜¯ç¼–é€ çš„ï¼ˆä¸åœ¨å¼•ç”¨æ¥æºä¸­ï¼Œä¹Ÿä¸æ˜¯å¸¸è¯†ï¼‰ï¼Ÿ
- è¿”å›ä¸€ä¸ª 0-1 ä¹‹é—´çš„åˆ†æ•°
- 0.0 = æ²¡æœ‰å¹»è§‰
- 1.0 = å®Œå…¨æ˜¯å¹»è§‰

## è¾“å…¥ä¿¡æ¯

**ç”¨æˆ·é—®é¢˜ï¼š**
{question}

**æ ‡å‡†ç­”æ¡ˆï¼š**
{ground_truths}

**AIå›ç­”ï¼š**
{prediction}

**AIå¼•ç”¨çš„æ¥æºï¼š**
{sources}

## è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼š

```json
{{
    "is_correct": true/false,
    "citation_relevance": 0.0-1.0,
    "citation_coverage": 0.0-1.0,
    "hallucination_rate": 0.0-1.0,
    "reasoning": "ç®€è¦è¯´æ˜è¯„åˆ¤ç†ç”±"
}}
```"""
    
    def __init__(self, llm_client):
        self.llm = llm_client
    
    def judge(
        self,
        question: str,
        ground_truths: List[str],
        prediction: str,
        sources: List[dict]
    ) -> dict:
        """è¯„åˆ¤å•æ¡ç»“æœ"""
        
        # æ ¼å¼åŒ–æ ‡å‡†ç­”æ¡ˆ
        gt_text = "\n".join([f"ç­”æ¡ˆ{i+1}: {gt}" for i, gt in enumerate(ground_truths)])
        
        # æ ¼å¼åŒ–å¼•ç”¨æ¥æº
        if sources:
            sources_text = "\n".join([
                f"[æ¥æº{s.get('index', i+1)}] {s.get('content', '')[:500]}"
                for i, s in enumerate(sources)
            ])
        else:
            sources_text = "ï¼ˆæ— å¼•ç”¨æ¥æºï¼‰"
        
        prompt = self.JUDGE_PROMPT.format(
            question=question,
            ground_truths=gt_text,
            prediction=prediction,
            sources=sources_text
        )
        
        try:
            response = self.llm.generate(
                prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„æµ‹ä¸“å®¶ï¼Œè¯·æŒ‰ç…§è¦æ±‚è¾“å‡º JSON æ ¼å¼çš„è¯„æµ‹ç»“æœã€‚",
                temperature=0.1,
                max_tokens=512
            )
            
            # æå– JSON
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    "is_correct": bool(result.get("is_correct", False)),
                    "citation_relevance": float(result.get("citation_relevance", 0.0)),
                    "citation_coverage": float(result.get("citation_coverage", 0.0)),
                    "hallucination_rate": float(result.get("hallucination_rate", 0.0)),
                    "reasoning": result.get("reasoning", "")
                }
        except Exception as e:
            print(f"  LLM judge error: {e}")
        
        # é»˜è®¤è¿”å›
        return {
            "is_correct": False,
            "citation_relevance": 0.0,
            "citation_coverage": 0.0,
            "hallucination_rate": 1.0,
            "reasoning": "è¯„åˆ¤å¤±è´¥"
        }


# ==================== è¯„æµ‹å™¨ ====================

class RAGEvaluator:
    """RAG è¯„æµ‹å™¨"""
    
    def __init__(
        self,
        rag: RAG,
        data_dir: str,
        use_llm_judge: bool = True,
        seed: int = 42
    ):
        self.rag = rag
        self.data_loader = DataLoader(data_dir)
        self.use_llm_judge = use_llm_judge
        self.judge = LLMJudge(rag.llm) if use_llm_judge else None
        
        # ç”¨äºæ£€æŸ¥æ•°æ®é‡å 
        self.train_question_ids: set = set()
        self.train_ans_ids: set = set()
        
        random.seed(seed)
        np.random.seed(seed)
    
    def _check_data_overlap(self, eval_samples: List[EvalSample]):
        """æ£€æŸ¥è®­ç»ƒæ•°æ®å’Œè¯„æµ‹æ•°æ®çš„é‡å æƒ…å†µ"""
        print(f"\n[DEBUG] Data Overlap Check:")
        print(f"  Train question_ids: {len(self.train_question_ids)}")
        print(f"  Train ans_ids: {len(self.train_ans_ids)}")
        
        eval_question_ids = set(s.question_id for s in eval_samples)
        eval_ans_ids = set()
        for s in eval_samples:
            eval_ans_ids.update(s.correct_ans_ids)
        
        print(f"  Eval question_ids: {len(eval_question_ids)}")
        print(f"  Eval ans_ids: {len(eval_ans_ids)}")
        
        # æ£€æŸ¥é‡å 
        qid_overlap = self.train_question_ids & eval_question_ids
        ans_overlap = self.train_ans_ids & eval_ans_ids
        
        print(f"\n  Question ID overlap: {len(qid_overlap)} ({len(qid_overlap)/len(eval_question_ids)*100:.1f}%)")
        print(f"  Answer ID overlap: {len(ans_overlap)} ({len(ans_overlap)/len(eval_ans_ids)*100:.1f}%)")
        
        if len(ans_overlap) == 0:
            print(f"\n  âš ï¸ WARNING: No answer ID overlap between train and eval data!")
            print(f"     This means retrieval Hit@K metrics will always be 0.")
            print(f"     The evaluation will fallback to question_id matching.")
        
        # æ‰“å°ä¸€äº›æ ·ä¾‹
        print(f"\n  Sample train ans_ids: {list(self.train_ans_ids)[:5]}")
        print(f"  Sample eval ans_ids: {list(eval_ans_ids)[:5]}")
    
    def build_knowledge_base(self, max_docs: int = None, batch_size: int = 64) -> int:
        """
        æ„å»ºçŸ¥è¯†åº“
        
        Args:
            max_docs: æœ€å¤§æ–‡æ¡£æ•°
            batch_size: æ‰¹é‡ embedding å¤§å°
        """
        print(f"\n{'='*50}")
        print("Building Knowledge Base")
        print(f"{'='*50}")
        
        # åŠ è½½æ•°æ®
        self.data_loader.load_questions()
        self.data_loader.load_answers()
        train_pairs = self.data_loader.load_train_candidates()
        
        # å­˜å‚¨è®­ç»ƒæ•°æ®çš„ IDsï¼ˆç”¨äºåç»­æ£€æŸ¥é‡å ï¼‰
        for qid, ans_id in train_pairs:
            self.train_question_ids.add(qid)
            self.train_ans_ids.add(ans_id)
        
        if max_docs and len(train_pairs) > max_docs:
            print(f"  Limiting to {max_docs} documents")
            random.shuffle(train_pairs)
            train_pairs = train_pairs[:max_docs]
        
        # æ¸…ç©ºå·²æœ‰çŸ¥è¯†åº“
        self.rag.vector_store.clear()
        
        total_chunks = 0
        skipped = 0
        batch_count = 0
        
        # ä½¿ç”¨æ‰¹é‡å¤„ç†
        chunk_buffer = []
        
        for qid, ans_id in tqdm(train_pairs, desc="Indexing"):
            # è·å–é—®é¢˜å’Œç­”æ¡ˆ
            if qid not in self.data_loader.questions:
                skipped += 1
                continue
            if ans_id not in self.data_loader.answers:
                skipped += 1
                continue
            
            question = self.data_loader.questions[qid]
            answer = self.data_loader.answers[ans_id]
            
            # æ„å»ºæ–‡æ¡£ï¼šé—®é¢˜ + ç­”æ¡ˆ
            doc_content = f"é—®é¢˜ï¼š{question.content}\nç­”æ¡ˆï¼š{answer.content}"
            doc_name = f"qa_{qid}_{ans_id}"
            
            # åˆ†å—
            chunks = self.rag.chunker.chunk_text(doc_content, doc_name)
            if not chunks:
                skipped += 1
                continue
            
            # å­˜å‚¨å…ƒæ•°æ®
            for chunk in chunks:
                chunk.metadata = {
                    "question_id": qid,
                    "ans_id": ans_id,
                    "question": question.content,
                    "answer": answer.content
                }
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            chunk_buffer.extend(chunks)
            total_chunks += len(chunks)
            
            # è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶è¿›è¡Œ embedding
            if len(chunk_buffer) >= batch_size:
                embeddings = self.rag.embedder.embed([c.content for c in chunk_buffer])
                self.rag.vector_store.add(chunk_buffer, embeddings)
                batch_count += 1
                chunk_buffer = []
        
        # å¤„ç†å‰©ä½™çš„ chunks
        if chunk_buffer:
            embeddings = self.rag.embedder.embed([c.content for c in chunk_buffer])
            self.rag.vector_store.add(chunk_buffer, embeddings)
            batch_count += 1
        
        # ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜ï¼ˆåªä¿å­˜ä¸€æ¬¡ï¼ï¼‰
        self.rag.vector_store.save()
        
        print(f"\nKnowledge base built:")
        print(f"  Total chunks: {total_chunks}")
        print(f"  Batch count: {batch_count} (batch_size={batch_size})")
        print(f"  Skipped: {skipped}")
        
        return total_chunks
    
    def _evaluate_single(
        self,
        sample: EvalSample,
        top_k: int = 5,
        debug: bool = False
    ) -> EvalResult:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬ï¼ˆä¾›å¹¶å‘è°ƒç”¨ï¼‰"""
        # 1. æ£€ç´¢
        retrieval_results = self.rag.retrieve(sample.question_content, top_k=max(top_k, 10))
        
        # æå–æ£€ç´¢åˆ°çš„ç­”æ¡ˆID
        retrieved_ans_ids = []
        retrieved_question_ids = []
        retrieved_docs = []
        for i, r in enumerate(retrieval_results):
            ans_id = r.chunk.metadata.get("ans_id", "")
            question_id = r.chunk.metadata.get("question_id", "")
            retrieved_ans_ids.append(ans_id)
            retrieved_question_ids.append(question_id)
            retrieved_docs.append({
                "index": i + 1,
                "doc_name": r.chunk.doc_name,
                "score": round(r.score, 4),
                "content": r.chunk.content,
                "ans_id": ans_id,
                "question_id": question_id
            })
        
        # 2. è®¡ç®—æ£€ç´¢æŒ‡æ ‡ - æ”¯æŒæŒ‰ ans_id æˆ– question_id åŒ¹é…
        hit = False
        hit_rank = -1
        
        # æ–¹å¼1: æŒ‰ ans_id åŒ¹é…
        for i, ans_id in enumerate(retrieved_ans_ids):
            if ans_id and ans_id in sample.correct_ans_ids:
                hit = True
                hit_rank = i + 1
                break
        
        # æ–¹å¼2: å¦‚æœ ans_id æ²¡å‘½ä¸­ï¼Œå°è¯•æŒ‰ question_id åŒ¹é…
        if not hit:
            for i, qid in enumerate(retrieved_question_ids):
                if qid and qid == sample.question_id:
                    hit = True
                    hit_rank = i + 1
                    break
        
        # è°ƒè¯•ä¿¡æ¯
        if debug and not hit:
            print(f"\n[DEBUG] Question {sample.question_id}:")
            print(f"  Correct ans_ids: {sample.correct_ans_ids}")
            print(f"  Retrieved ans_ids: {retrieved_ans_ids[:5]}")
            print(f"  Retrieved question_ids: {retrieved_question_ids[:5]}")
        
        # 3. ç”Ÿæˆå›ç­”
        answer = ""
        sources = []
        try:
            answer, sources = self.rag.query(sample.question_content, top_k=top_k)
        except Exception as e:
            pass  # é™é»˜å¤„ç†é”™è¯¯
        
        # 4. LLM è¯„åˆ¤
        llm_judgment = {}
        is_correct = False
        citation_precision = 0.0
        citation_recall = 0.0
        hallucination_rate = 0.0
        
        if self.use_llm_judge and self.judge and answer:
            try:
                llm_judgment = self.judge.judge(
                    question=sample.question_content,
                    ground_truths=sample.correct_answers,
                    prediction=answer,
                    sources=sources
                )
                
                is_correct = llm_judgment.get("is_correct", False)
                citation_precision = llm_judgment.get("citation_relevance", 0.0)
                citation_recall = llm_judgment.get("citation_coverage", 0.0)
                hallucination_rate = llm_judgment.get("hallucination_rate", 0.0)
            except Exception as e:
                pass  # é™é»˜å¤„ç†é”™è¯¯
        
        # 5. è¿”å›ç»“æœ
        return EvalResult(
            question_id=sample.question_id,
            question=sample.question_content,
            ground_truths=sample.correct_answers,
            prediction=answer,
            retrieved_docs=retrieved_docs,
            retrieved_ans_ids=retrieved_ans_ids,
            hit=hit,
            hit_rank=hit_rank,
            is_correct=is_correct,
            citation_precision=citation_precision,
            citation_recall=citation_recall,
            hallucination_rate=hallucination_rate,
            llm_judgment=llm_judgment
        )
    
    def evaluate(
        self,
        eval_file: str = "dev_candidates.txt",
        max_samples: int = None,
        top_k: int = 5,
        workers: int = 1,
        debug: bool = False
    ) -> Tuple[EvalMetrics, List[EvalResult]]:
        """
        æ‰§è¡Œè¯„æµ‹
        
        Args:
            eval_file: è¯„æµ‹æ–‡ä»¶
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            top_k: æ£€ç´¢ Top-K
            workers: å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 1ï¼Œå•çº¿ç¨‹ï¼‰
            debug: æ‰“å°è°ƒè¯•ä¿¡æ¯
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        print(f"\n{'='*50}")
        print(f"Running Evaluation on {eval_file}")
        print(f"  Workers: {workers}")
        print(f"{'='*50}")
        
        # åŠ è½½è¯„æµ‹æ•°æ®
        samples = self.data_loader.load_eval_candidates(eval_file)
        
        if max_samples and len(samples) > max_samples:
            print(f"  Limiting to {max_samples} samples")
            random.shuffle(samples)
            samples = samples[:max_samples]
        
        # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®é‡å 
        if debug:
            self._check_data_overlap(samples)
        
        results: List[EvalResult] = []
        
        if workers <= 1:
            # å•çº¿ç¨‹æ¨¡å¼
            for sample in tqdm(samples, desc="Evaluating"):
                result = self._evaluate_single(sample, top_k, debug=debug)
                results.append(result)
        else:
            # å¤šçº¿ç¨‹å¹¶å‘æ¨¡å¼ï¼ˆdebug æ¨¡å¼ä¸‹åªæ‰“å°å‰å‡ ä¸ªï¼‰
            debug_count = 5 if debug else 0
            with ThreadPoolExecutor(max_workers=workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_sample = {
                    executor.submit(self._evaluate_single, sample, top_k, debug=(debug and i < debug_count)): sample
                    for i, sample in enumerate(samples)
                }
                
                # æ”¶é›†ç»“æœ
                for future in tqdm(as_completed(future_to_sample), total=len(samples), desc=f"Evaluating (workers={workers})"):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        sample = future_to_sample[future]
                        print(f"  Error evaluating {sample.question_id}: {e}")
        
        # 6. è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        n = len(results)
        
        hits_at_k = {1: 0, 3: 0, 5: 0, 10: 0}
        mrr_sum = 0.0
        correct_count = 0
        total_citation_precision = 0.0
        total_citation_recall = 0.0
        total_hallucination = 0.0
        
        for r in results:
            # æ£€ç´¢æŒ‡æ ‡
            for k in hits_at_k.keys():
                if 0 < r.hit_rank <= k:
                    hits_at_k[k] += 1
            if r.hit_rank > 0:
                mrr_sum += 1.0 / r.hit_rank
            
            # ç”ŸæˆæŒ‡æ ‡
            if r.is_correct:
                correct_count += 1
            total_citation_precision += r.citation_precision
            total_citation_recall += r.citation_recall
            total_hallucination += r.hallucination_rate
        
        avg_precision = total_citation_precision / n if n > 0 else 0
        avg_recall = total_citation_recall / n if n > 0 else 0
        citation_f1 = (2 * avg_precision * avg_recall / (avg_precision + avg_recall)) if (avg_precision + avg_recall) > 0 else 0
        
        metrics = EvalMetrics(
            total_samples=n,
            hit_at_1=hits_at_k[1] / n if n > 0 else 0,
            hit_at_3=hits_at_k[3] / n if n > 0 else 0,
            hit_at_5=hits_at_k[5] / n if n > 0 else 0,
            hit_at_10=hits_at_k[10] / n if n > 0 else 0,
            mrr=mrr_sum / n if n > 0 else 0,
            accuracy=correct_count / n if n > 0 else 0,
            citation_precision=avg_precision,
            citation_recall=avg_recall,
            citation_f1=citation_f1,
            hallucination_rate=total_hallucination / n if n > 0 else 0
        )
        
        return metrics, results
    
    def evaluate_retrieval_only(
        self,
        eval_file: str = "dev_candidates.txt",
        max_samples: int = None,
        top_k: int = 10,
        debug: bool = False
    ) -> EvalMetrics:
        """ä»…è¯„æµ‹æ£€ç´¢ï¼ˆä¸è°ƒç”¨LLMç”Ÿæˆï¼Œé€Ÿåº¦æ›´å¿«ï¼‰"""
        print(f"\n{'='*50}")
        print(f"Running Retrieval-Only Evaluation on {eval_file}")
        print(f"{'='*50}")
        
        samples = self.data_loader.load_eval_candidates(eval_file)
        
        if max_samples and len(samples) > max_samples:
            random.shuffle(samples)
            samples = samples[:max_samples]
        
        # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®é‡å 
        if debug:
            self._check_data_overlap(samples)
        
        hits_at_k = {1: 0, 3: 0, 5: 0, 10: 0}
        mrr_sum = 0.0
        debug_count = 0
        
        for sample in tqdm(samples, desc="Evaluating Retrieval"):
            results = self.rag.retrieve(sample.question_content, top_k=top_k)
            
            hit_rank = -1
            
            # æ–¹å¼1: æŒ‰ ans_id åŒ¹é…
            for i, r in enumerate(results):
                ans_id = r.chunk.metadata.get("ans_id", "")
                if ans_id and ans_id in sample.correct_ans_ids:
                    hit_rank = i + 1
                    break
            
            # æ–¹å¼2: å¦‚æœ ans_id æ²¡å‘½ä¸­ï¼Œå°è¯•æŒ‰ question_id åŒ¹é…
            if hit_rank < 0:
                for i, r in enumerate(results):
                    qid = r.chunk.metadata.get("question_id", "")
                    if qid and qid == sample.question_id:
                        hit_rank = i + 1
                        break
            
            # è°ƒè¯•è¾“å‡º
            if debug and hit_rank < 0 and debug_count < 5:
                retrieved_ans_ids = [r.chunk.metadata.get("ans_id", "") for r in results[:5]]
                retrieved_qids = [r.chunk.metadata.get("question_id", "") for r in results[:5]]
                print(f"\n[DEBUG] No hit for question {sample.question_id}:")
                print(f"  Correct ans_ids: {sample.correct_ans_ids}")
                print(f"  Retrieved ans_ids: {retrieved_ans_ids}")
                print(f"  Retrieved question_ids: {retrieved_qids}")
                debug_count += 1
            
            for k in hits_at_k.keys():
                if 0 < hit_rank <= k:
                    hits_at_k[k] += 1
            
            if hit_rank > 0:
                mrr_sum += 1.0 / hit_rank
        
        n = len(samples)
        return EvalMetrics(
            total_samples=n,
            hit_at_1=hits_at_k[1] / n if n > 0 else 0,
            hit_at_3=hits_at_k[3] / n if n > 0 else 0,
            hit_at_5=hits_at_k[5] / n if n > 0 else 0,
            hit_at_10=hits_at_k[10] / n if n > 0 else 0,
            mrr=mrr_sum / n if n > 0 else 0
        )


# ==================== å·¥å…·å‡½æ•° ====================

def print_metrics(metrics: EvalMetrics):
    """æ‰“å°è¯„æµ‹æŒ‡æ ‡"""
    print(f"\n{'='*50}")
    print("Evaluation Results")
    print(f"{'='*50}")
    
    print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"  æ ·æœ¬æ•°: {metrics.total_samples}")
    if metrics.elapsed_seconds > 0:
        print(f"  è€—æ—¶: {metrics.elapsed_seconds:.2f}s")
    
    print(f"\nğŸ” æ£€ç´¢æŒ‡æ ‡:")
    print(f"  Hit@1:  {metrics.hit_at_1:.4f} ({metrics.hit_at_1*100:.2f}%)")
    print(f"  Hit@3:  {metrics.hit_at_3:.4f} ({metrics.hit_at_3*100:.2f}%)")
    print(f"  Hit@5:  {metrics.hit_at_5:.4f} ({metrics.hit_at_5*100:.2f}%)")
    print(f"  Hit@10: {metrics.hit_at_10:.4f} ({metrics.hit_at_10*100:.2f}%)")
    print(f"  MRR:    {metrics.mrr:.4f}")
    
    if metrics.accuracy > 0 or metrics.citation_f1 > 0:
        print(f"\nğŸ“ ç”ŸæˆæŒ‡æ ‡:")
        print(f"  å‡†ç¡®ç‡ (Accuracy):     {metrics.accuracy:.4f} ({metrics.accuracy*100:.2f}%)")
        print(f"  å¼•ç”¨ç²¾ç¡®ç‡ (Precision): {metrics.citation_precision:.4f}")
        print(f"  å¼•ç”¨å¬å›ç‡ (Recall):    {metrics.citation_recall:.4f}")
        print(f"  å¼•ç”¨ F1:               {metrics.citation_f1:.4f}")
        print(f"  å¹»è§‰ç‡ (Hallucination): {metrics.hallucination_rate:.4f} ({metrics.hallucination_rate*100:.2f}%)")


def save_results(
    metrics: EvalMetrics,
    results: List[EvalResult],
    output_path: str
):
    """ä¿å­˜è¯„æµ‹ç»“æœ"""
    output = {
        "metrics": asdict(metrics),
        "samples": [
            {
                "question_id": r.question_id,
                "question": r.question,
                "ground_truths": r.ground_truths,
                "prediction": r.prediction,
                "hit": r.hit,
                "hit_rank": r.hit_rank,
                "is_correct": r.is_correct,
                "citation_precision": r.citation_precision,
                "citation_recall": r.citation_recall,
                "hallucination_rate": r.hallucination_rate,
                "retrieved_ans_ids": r.retrieved_ans_ids[:5],
                "llm_judgment": r.llm_judgment
            }
            for r in results
        ]
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ Results saved to: {output_path}")


# ==================== ä¸»å‡½æ•° ====================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG Evaluation Script")
    
    # æ•°æ®è·¯å¾„
    parser.add_argument("--data-dir", default="./data", help="æ•°æ®ç›®å½•è·¯å¾„")
    
    # LLM é…ç½®
    parser.add_argument("--base-url", default=None, help="LLM API åœ°å€")
    parser.add_argument("--api-key", default=None, help="LLM API å¯†é’¥")
    parser.add_argument("--provider", default=None, 
                        choices=["openai", "deepseek", "zhipu", "moonshot", "qwen", "ollama", "vllm"],
                        help="LLM æä¾›å•†")
    parser.add_argument("--model", default="gpt-3.5-turbo", help="æ¨¡å‹åç§°")
    
    # å…¼å®¹æ—§å‚æ•°
    parser.add_argument("--vllm-url", default=None, help="[å…¼å®¹] ç­‰åŒäº --base-url")
    
    # å…¶ä»– RAG é…ç½®
    parser.add_argument("--embedding", default="BAAI/bge-base-zh-v1.5", help="åµŒå…¥æ¨¡å‹")
    parser.add_argument("--db-dir", default="./eval_faiss_db", help="FAISS ç´¢å¼•ç›®å½•")
    
    # è¯„æµ‹é…ç½®
    parser.add_argument("--max-knowledge", type=int, default=None, help="æœ€å¤§çŸ¥è¯†åº“æ–‡æ¡£æ•°")
    parser.add_argument("--max-samples", type=int, default=None, help="æœ€å¤§è¯„æµ‹æ ·æœ¬æ•°")
    parser.add_argument("--top-k", type=int, default=5, help="æ£€ç´¢ Top-K")
    parser.add_argument("--batch-size", type=int, default=64, help="æ‰¹é‡ embedding å¤§å°")
    parser.add_argument("--workers", type=int, default=1, help="å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 1ï¼‰")
    parser.add_argument("--eval-file", default="dev_candidates.txt", help="è¯„æµ‹æ–‡ä»¶")
    parser.add_argument("--retrieval-only", action="store_true", help="ä»…è¯„æµ‹æ£€ç´¢")
    parser.add_argument("--no-llm-judge", action="store_true", help="ä¸ä½¿ç”¨LLMè¯„åˆ¤")
    parser.add_argument("--skip-build", action="store_true", help="è·³è¿‡çŸ¥è¯†åº“æ„å»ºï¼Œä½¿ç”¨å·²æœ‰ç´¢å¼•")
    parser.add_argument("--debug", action="store_true", help="æ‰“å°è°ƒè¯•ä¿¡æ¯")
    parser.add_argument("--output", default="eval_results.json", help="ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ– RAG
    print("Initializing RAG...")
    print(f"  Provider: {args.provider or 'custom'}")
    print(f"  Base URL: {args.base_url or args.vllm_url or '(from provider)'}")
    print(f"  API Key: {'***' + args.api_key[-4:] if args.api_key else '(not set / from env)'}")
    print(f"  Model: {args.model}")
    
    rag = RAG(
        base_url=args.base_url or args.vllm_url,
        model_name=args.model,
        api_key=args.api_key,
        provider=args.provider,
        embedding_model=args.embedding,
        persist_dir=args.db_dir,
        top_k=args.top_k
    )
    
    # åˆå§‹åŒ–è¯„æµ‹å™¨
    evaluator = RAGEvaluator(
        rag=rag,
        data_dir=args.data_dir,
        use_llm_judge=not args.no_llm_judge,
        seed=args.seed
    )
    
    # æ„å»ºçŸ¥è¯†åº“ï¼ˆæˆ–è·³è¿‡ï¼‰
    if args.skip_build:
        # è·³è¿‡æ„å»ºï¼Œä½¿ç”¨å·²æœ‰ç´¢å¼•
        print(f"\n{'='*50}")
        print("Skipping Knowledge Base Build (using existing index)")
        print(f"{'='*50}")
        print(f"  Index path: {args.db_dir}")
        print(f"  Total chunks: {rag.vector_store.count()}")
        
        if rag.vector_store.count() == 0:
            print("  âš ï¸ WARNING: Index is empty! Did you forget to build it?")
        
        # ä»éœ€åŠ è½½é—®é¢˜å’Œç­”æ¡ˆæ•°æ®ï¼ˆç”¨äºè¯„æµ‹ï¼‰
        evaluator.data_loader.load_questions()
        evaluator.data_loader.load_answers()
    else:
        evaluator.build_knowledge_base(max_docs=args.max_knowledge, batch_size=args.batch_size)
    
    # æ‰§è¡Œè¯„æµ‹
    start_time = time.time()
    
    if args.retrieval_only:
        metrics = evaluator.evaluate_retrieval_only(
            eval_file=args.eval_file,
            max_samples=args.max_samples,
            top_k=args.top_k,
            debug=args.debug
        )
        results = []
    else:
        metrics, results = evaluator.evaluate(
            eval_file=args.eval_file,
            max_samples=args.max_samples,
            top_k=args.top_k,
            workers=args.workers,
            debug=args.debug
        )
    
    metrics.elapsed_seconds = round(time.time() - start_time, 2)
    
    # æ‰“å°ç»“æœ
    print_metrics(metrics)
    
    # ä¿å­˜ç»“æœ
    save_results(metrics, results, args.output)


if __name__ == "__main__":
    main()
